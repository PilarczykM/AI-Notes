# Embeddings: Co to jest i jak dziaÅ‚a?

![PrzykÅ‚ad-Modelu-Embeddingu](./images/embedding_proc.webp)


## ğŸ“Œ Czym sÄ… Embeddings?

Embeddings to gÄ™ste, niskowymiarowe reprezentacje danych wysokowymiarowych. PozwalajÄ… na przeksztaÅ‚cenie zÅ‚oÅ¼onych informacji w formÄ™ zrozumiaÅ‚Ä… dla algorytmÃ³w uczenia maszynowego.

- UÅ‚atwiajÄ… analizÄ™ semantycznÄ… w NLP (przetwarzanie jÄ™zyka naturalnego).
- Wykorzystywane w systemach rekomendacji, analizie obrazÃ³w i dÅºwiÄ™kÃ³w.

DziÄ™ki embeddingom algorytmy mogÄ… lepiej rozumieÄ‡ kontekst i relacje miÄ™dzy danymi.

![image clasification](./images/blog-king-queen-embeddings.webp)

---

## ğŸ—‚ï¸ PrzykÅ‚ad Zastosowania

RozwaÅ¼my sÅ‚owa: **"krÃ³lowa", "krÃ³l", "ksiÄ™Å¼niczka", "ksiÄ…Å¼Ä™"**.

### Kodowanie One-hot:

```plaintext
| Word       | One-hot Vector  |
| ---------- | --------------- |
| queen      | [1, 0, 0, 0]    |
| king       | [0, 1, 0, 0]    |
| princess   | [0, 0, 1, 0]    |
| prince     | [0, 0, 0, 1]    |
```

Takie przedstawienie nie pokazuje relacji miÄ™dzy sÅ‚owami.

### Reprezentacja Embedding:

```plaintext
| Word       | [Gender, Age]  |
| ---------- | -------------- |
| queen      | [1, 0]         |
| king       | [0, 0]         |
| princess   | [1, 1]         |
| prince     | [0, 1]         |
```

- **Gender:** 1 = female, 0 = male
- **Age:** 1 = youth, 0 = adulthood

DziÄ™ki temu Å‚atwo dostrzec podobieÅ„stwa i rÃ³Å¼nice.

---

## ğŸš€ Zastosowania EmbeddingÃ³w

- **Systemy rekomendacji:** Przewidywanie preferencji uÅ¼ytkownikÃ³w.
- **Przetwarzanie jÄ™zyka naturalnego (NLP):** Analiza semantyczna tekstu.
- **Analiza obrazÃ³w i dÅºwiÄ™ku:** Klasyfikacja i rozpoznawanie wzorcÃ³w.

Embeddings sÄ… fundamentem modeli AI, takich jak transformery.

## ğŸŒ“ Rodzaje EmbeddingÃ³w

1. **Embeddingi SÅ‚Ã³w (Word Embeddings)**  
   Te embeddingi reprezentujÄ… pojedyncze sÅ‚owa jako wektory. PrzykÅ‚ady obejmujÄ… Word2Vec, GloVe i FastText. Embeddingi sÅ‚Ã³w sÄ… podstawÄ… w zadaniach NLP, umoÅ¼liwiajÄ…c modelom zrozumienie semantycznych relacji miÄ™dzy sÅ‚owami.

   **PrzykÅ‚ad:** W Word2Vec, sÅ‚owo takie jak "krÃ³l" ("king") moÅ¼e byÄ‡ reprezentowane przez wektor taki jak `[0.5, 0.8, -0.1, 0.3, 0.9]`. SÅ‚owo "krÃ³lowa" ("queen") miaÅ‚oby podobny wektor, co pokazuje ich semantyczne podobieÅ„stwo, podczas gdy sÅ‚owo "jabÅ‚ko" ("apple") byÅ‚oby znacznie bardziej odlegÅ‚e w przestrzeni wektorowej.

2. **Embeddingi ZdaÅ„ (Sentence Embeddings)**  
   Te embeddingi wychwytujÄ… znaczenie caÅ‚ych zdaÅ„ lub akapitÃ³w. Modele takie jak Universal Sentence Encoder i Sentence-BERT (SBERT) generujÄ… te embeddingi poprzez uÅ›rednianie lub agregacjÄ™ embeddingÃ³w sÅ‚Ã³w w celu stworzenia pojedynczego wektora reprezentujÄ…cego caÅ‚e zdanie.

   **PrzykÅ‚ad:** W zdaniu "Kot usiadÅ‚ na macie" ("The cat sat on the mat"), embedding zdaÅ„ moÅ¼e przypisaÄ‡ temu zdaniu wektor, ktÃ³ry odzwierciedla ogÃ³lne znaczenie zdania, a nie tylko poszczegÃ³lne sÅ‚owa.

3. **Embeddingi ObrazÃ³w (Image Embeddings)**  
   Te embeddingi sÄ… wykorzystywane w wizji komputerowej do reprezentowania obrazÃ³w jako wektorÃ³w. Konwolucyjne sieci neuronowe (CNN) czÄ™sto sÅ‚uÅ¼Ä… jako podstawa do generowania tych embeddingÃ³w, ktÃ³re mogÄ… byÄ‡ nastÄ™pnie wykorzystywane w zadaniach takich jak wyszukiwanie obrazÃ³w czy klasyfikacja.

   **PrzykÅ‚ad:** Obraz kota moÅ¼e zostaÄ‡ odwzorowany w postaci wektora bliskiego innym obrazom kotÃ³w w przestrzeni embeddingÃ³w, co pomaga modelowi rozpoznawaÄ‡ podobne obiekty na rÃ³Å¼nych obrazach.

4. **Embeddingi GrafÃ³w (Graph Embeddings)**  
   W embeddingach grafÃ³w wÄ™zÅ‚y lub caÅ‚e podgrafy sÄ… odwzorowywane na wektory, ktÃ³re zachowujÄ… strukturalne relacje w obrÄ™bie grafu. Modele takie jak DeepWalk i GraphSAGE sÄ… wykorzystywane do generowania tych embeddingÃ³w.

   **PrzykÅ‚ad:** W grafie sieci spoÅ‚ecznoÅ›ciowej embeddingi mogÄ… byÄ‡ uÅ¼ywane do reprezentowania uÅ¼ytkownikÃ³w jako wektorÃ³w, gdzie bliskoÅ›Ä‡ wektorÃ³w wskazuje na siÅ‚Ä™ relacji miÄ™dzy uÅ¼ytkownikami.

![different vector embeddings](./images/type_of_embedding.webp)

## Kluczowe Modele EmbeddingÃ³w

### Word2Vec
Word2Vec, opracowany przez Google, byÅ‚ jednym z pionierskich modeli embeddingÃ³w sÅ‚Ã³w. Wykorzystuje pÅ‚ytkie sieci neuronowe do nauki powiÄ…zaÅ„ miÄ™dzy sÅ‚owami na podstawie duÅ¼ego korpusu tekstu. Word2Vec ma dwa gÅ‚Ã³wne podejÅ›cia: Continuous Bag of Words (CBOW) i Skip-Gram.

- **CBOW:** W CBOW model przewiduje docelowe sÅ‚owo na podstawie otaczajÄ…cych je sÅ‚Ã³w kontekstowych. Na przykÅ‚ad w zdaniu "Kot usiadÅ‚ na macie" model moÅ¼e przewidzieÄ‡ sÅ‚owo "usiadÅ‚" na podstawie sÅ‚Ã³w kontekstowych "kot", "na" i "macie".
  ![CBOW](./images/cbow.webp)

- **Skip-Gram:** Model Skip-Gram dziaÅ‚a odwrotnie, przewidujÄ…c sÅ‚owa kontekstowe na podstawie sÅ‚owa docelowego. Na przykÅ‚ad, majÄ…c sÅ‚owo "usiadÅ‚", model przewidziaÅ‚by "kot", "na" i "macie".
  ![skup-gram](./images/skip-gram.webp)

**Architektura Skip-Gram:**  
Word2Vec uczy siÄ™ generowania embeddingÃ³w, w ktÃ³rych semantycznie podobne sÅ‚owa majÄ… podobne reprezentacje wektorowe.

**PrzykÅ‚ad:** Znane porÃ³wnanie: "krÃ³l" âˆ’ "mÄ™Å¼czyzna" + "kobieta" â‰ˆ "krÃ³lowa". Pokazuje to, Å¼e Word2Vec wychwytuje zarÃ³wno znaczenie semantyczne, jak i relacje miÄ™dzy sÅ‚owami.

---

### GloVe (Global Vectors for Word Representation)
GloVe, opracowany przez Uniwersytet Stanforda, to kolejny popularny model embeddingÃ³w sÅ‚Ã³w. W przeciwieÅ„stwie do Word2Vec, ktÃ³ry opiera siÄ™ na predykcji, GloVe to model oparty na zliczaniu, wykorzystujÄ…cy macierz wspÃ³Å‚wystÄ™powania sÅ‚Ã³w w caÅ‚ym korpusie tekstu. Celem GloVe jest tworzenie wektorÃ³w sÅ‚Ã³w, ktÃ³re oddajÄ… globalne informacje statystyczne o wspÃ³Å‚wystÄ™powaniu sÅ‚Ã³w.

- **Zaleta:** GloVe dobrze radzi sobie z wychwytywaniem zarÃ³wno lokalnego, jak i globalnego kontekstu, co czyni go odpowiednim do rÃ³Å¼nych zadaÅ„ NLP.

- **PrzykÅ‚ad:** JeÅ›li sÅ‚owa takie jak "lÃ³d" i "Å›nieg" czÄ™sto wspÃ³Å‚wystÄ™pujÄ… w korpusie, GloVe utworzy wektory, w ktÃ³rych "lÃ³d" i "Å›nieg" bÄ™dÄ… blisko siebie w przestrzeni wektorowej, odzwierciedlajÄ…c ich semantyczny zwiÄ…zek.
  ![glove](./images/glove.webp)

---

### BERT (Bidirectional Encoder Representations from Transformers)
BERT to znaczÄ…cy krok naprzÃ³d w modelach embeddingÃ³w. W przeciwieÅ„stwie do tradycyjnych modeli traktujÄ…cych sÅ‚owa niezaleÅ¼nie, BERT generuje kontekstualizowane embeddingi, co oznacza, Å¼e to samo sÅ‚owo moÅ¼e mieÄ‡ rÃ³Å¼ne wektory w zaleÅ¼noÅ›ci od kontekstu. BERT opiera siÄ™ na architekturze Transformer, wykorzystujÄ…c mechanizmy uwagi do modelowania relacji miÄ™dzy sÅ‚owami w zdaniu.

- **Kluczowa cecha:** BERT jest dwukierunkowy, co oznacza, Å¼e podczas treningu bierze pod uwagÄ™ kontekst z lewej i prawej strony sÅ‚owa. Pozwala to na peÅ‚ne uchwycenie znaczenia sÅ‚owa w zdaniu.

- **Architektura:** BERT korzysta z wielu warstw TransformerÃ³w do tworzenia embeddingÃ³w. KaÅ¼da warstwa Transformer stosuje mechanizm samo-uwagi, aby wychwyciÄ‡ relacje miÄ™dzy sÅ‚owami, niezaleÅ¼nie od ich odlegÅ‚oÅ›ci w zdaniu.

- **Trening:** BERT jest trenowany z wykorzystaniem dwÃ³ch zadaÅ„: Masked Language Modeling (MLM) i Next Sentence Prediction (NSP). W MLM niektÃ³re sÅ‚owa w zdaniu wejÅ›ciowym sÄ… maskowane, a model uczy siÄ™ przewidywaÄ‡ brakujÄ…ce sÅ‚owa. W NSP model uczy siÄ™ przewidywaÄ‡, czy dwa zdania sÄ… ze sobÄ… powiÄ…zane.

- **Zastosowanie:** BERT doskonale sprawdza siÄ™ w zadaniach takich jak odpowiadanie na pytania i klasyfikacja tekstu. Na przykÅ‚ad w systemie obsÅ‚ugi klienta BERT moÅ¼e byÄ‡ wykorzystywany do zrozumienia i precyzyjnego odpowiadania na zapytania klientÃ³w.

- **PrzykÅ‚ad:** W zdaniu "Bank moÅ¼e zagwarantowaÄ‡, Å¼e depozyt dotrze jutro" BERT wygeneruje rÃ³Å¼ne embeddingi dla sÅ‚owa "bank" w zaleÅ¼noÅ›ci od kontekstu. Zrozumie, czy "bank" odnosi siÄ™ do instytucji finansowej, czy do brzegu rzeki.
![BERT](./images/bert.webp)
---

### GPT (Generative Pre-trained Transformer)
GPT, opracowany przez OpenAI, to kolejny model oparty na Transformerach, skupiajÄ…cy siÄ™ na zadaniach generatywnych. W przeciwieÅ„stwie do BERT, ktÃ³ry jest przeznaczony gÅ‚Ã³wnie do zrozumienia tekstu, GPT doskonale radzi sobie z generowaniem spÃ³jnego tekstu na podstawie podanego promptu. GPT wykorzystuje jednokierunkowy Transformer, co oznacza, Å¼e podczas generowania tekstu bierze pod uwagÄ™ tylko kontekst z lewej strony.

- **Kluczowa cecha:** GPT jest wstÄ™pnie trenowany na duÅ¼ych zbiorach danych tekstowych i dostrajany do konkretnych zadaÅ„, co czyni go bardzo wszechstronnym. Jego zdolnoÅ›Ä‡ do generowania tekstu przypominajÄ…cego ludzkie wypowiedzi sprawia, Å¼e jest popularny w aplikacjach takich jak chatboty, generowanie treÅ›ci i twÃ³rcze pisanie.

- **Architektura:** GPT wykorzystuje stos warstw dekoderÃ³w Transformer. KaÅ¼da warstwa stosuje mechanizm samo-uwagi do sekwencji wejÅ›ciowej i generuje przewidywanie dla nastÄ™pnego tokena (sÅ‚owa lub pod-sÅ‚owa) w sekwencji.

- **Trening:** GPT jest trenowany w trybie uczenia nienadzorowanego, gdzie model uczy siÄ™ przewidywaÄ‡ kolejne sÅ‚owo w sekwencji. NastÄ™pnie przeprowadza siÄ™ dostrajanie do specyficznych zadaÅ„ z wykorzystaniem uczenia nadzorowanego.

- **Zastosowanie:** GPT znalazÅ‚ zastosowanie w rÃ³Å¼nych kreatywnych aplikacjach, takich jak generowanie artykuÅ‚Ã³w, wierszy, a nawet kodu. Na przykÅ‚ad GPT-3 moÅ¼e wygenerowaÄ‡ caÅ‚y esej na podstawie krÃ³tkiego promptu, a jego tekst czÄ™sto jest trudny do odrÃ³Å¼nienia od pisma ludzkiego.

- **PrzykÅ‚ad:** MajÄ…c prompt taki jak "Dawno, dawno temu..." ("Once upon a time"), GPT moÅ¼e wygenerowaÄ‡ caÅ‚Ä… historiÄ™, wykorzystujÄ…c swoje zrozumienie jÄ™zyka i kontekstu do stworzenia spÃ³jnej i kreatywnej narracji.
