{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG application from scratch\n",
    "Here is a high-level overview of the system we want to build:\n",
    "\n",
    "![System1](../images/system1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "Let's define the LLM model that we'll use as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "try:\n",
    "    model = OllamaLLM(model=\"llama3.2\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Please install ollama first.\")\n",
    "    print(e.msg)\n",
    "    exit()\n",
    "\n",
    "model.invoke(\"What is LLM, answer in one sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the model is an AIMessage instance containing the answer.\n",
    "\n",
    "We can extract this answer by chaining the model with an [output parser](https://python.langchain.com/docs/how_to/#output-parsers).\n",
    "\n",
    "Here is what chaining the model with an `output parser` looks like:\n",
    "\n",
    "![Chain 1](../images/chain1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for above example, you will use simple StrOutputParser to extract the message from object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "str_output_chain = model | parser\n",
    "str_output_chain.invoke(\"What is LLM, answer in one sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing prompt templates\n",
    "We want to provide the model with some context and the question. [Prompt templates](\"https://python.langchain.com/docs/how_to/#prompt-templates\") are a simple way to define and reuse prompts.\n",
    "\n",
    "![Chain 2](../images/chain2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "print(\n",
    "    prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain prompt with model and output parser\n",
    "\n",
    "* Input: Context + Question.\n",
    "* Template Injection: Combines inputs into a structured prompt.\n",
    "* LLM Processing: The LLM generates an answer based on the injected prompt.\n",
    "* Output: The answer, which can either be the final result or used in further processing.\n",
    "\n",
    "![Chain 2](../images/chain2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke(\n",
    "    input={\"context\": \"Mary's sister is Susana\", \"question\": \"Who is Mary's sister?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining chains\n",
    "\n",
    "We can now create a new translation chain that combines the result from the first chain with the translation prompt.\n",
    "\n",
    "Here is what the new workflow looks like:\n",
    "![Chain 3](../images/chain3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_template = \"\"\"\n",
    "    Translate {answer} to {language}.\n",
    "    Return only translation, without any additional explanation.\"\"\"\n",
    "translation_prompt = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"language\"], template=translation_template\n",
    ")\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")}\n",
    "    | translation_prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Polish\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
