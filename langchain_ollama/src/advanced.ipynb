{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "The context we want to send the model comes from a YouTube video.\n",
    "\n",
    "Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/index/whisper/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n",
    "TEMP_DIR = Path(\"..\") / Path(\"temp\")\n",
    "TRANSCRIPT_FILE = (Path(\"..\") / Path(\"transcription.txt\"))\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TRANSCRIPT_FILE.exists():\n",
    "    import tempfile\n",
    "    import whisper\n",
    "    from yt_dlp import YoutubeDL\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory(dir=TEMP_DIR) as temp_dir:\n",
    "        # Define the URL of the YouTube video\n",
    "\n",
    "        # Set up yt-dlp options\n",
    "        ydl_opts = {\n",
    "            \"verbose\": True,\n",
    "            \"format\": \"m4a/bestaudio/best\",\n",
    "            \"outtmpl\": str(Path(temp_dir) / Path(\"%(title)s.%(ext)s\" )),\n",
    "        }\n",
    "\n",
    "        # Download the audio\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            error_code = ydl.download([YOUTUBE_VIDEO])\n",
    "            if error_code != 0:\n",
    "                raise Exception(\"Error downloading the video\")\n",
    "\n",
    "        # Load whisper model\n",
    "        whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "        # Foreach audio file in the temporary directory\n",
    "        for file in Path(temp_dir).iterdir():\n",
    "            if file.suffix == \".m4a\":\n",
    "                # Transcribe the audio\n",
    "                transcription = whisper_model.transcribe(str(file), fp16=False, verbose=True)[\"text\"].strip()\n",
    "            \n",
    "            # Write the transcription to a file\n",
    "            with TRANSCRIPT_FILE.open(\"w\") as file:\n",
    "                    file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the transcription and display the first few characters to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TRANSCRIPT_FILE.open(\"r\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "print(transcription[:100])\n",
    "print(f\"Length: {len(transcription)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the entire transcription as a context\n",
    "\n",
    "If you try to invoke the chain using the transcription as context, the model probably will return an **error** indicating the context is too long.\n",
    "\n",
    "> *LLMs support limited context size*.\n",
    "\n",
    "E.g. most books are far beyond context size so we need to find the solution to somehow truncate context off to pick essential parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from httpx import ConnectError\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from ollama import ResponseError\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "try:\n",
    "    print(model.invoke(\"What is 2 + 2?\"))\n",
    "except ConnectError as e:\n",
    "    print(f\"Error connecting to the model: {e}\")\n",
    "    print(\"Please make sure the model is running and try again.\")\n",
    "    exit(1)\n",
    "except ResponseError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Looks like this model handles that big context..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"context\": transcription, \"question\": \"What is the video about?\"}))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How token works\n",
    "\n",
    "[Tiktokenizer](https://tiktokenizer.vercel.app/)\n",
    "\n",
    "https://tiktokenizer.vercel.app/?model=cl100k_base - Basically we can assume that one word is one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiting the transcription\n",
    "\n",
    "Since we **shouldn't use** the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks.\n",
    "\n",
    "We can then invoke the model using only the relevant chunks to answer a particular question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text document\n",
    "\n",
    "[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html#textloader) - using langchain_community there is a simple TextLoader which can help us to load file as Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(TRANSCRIPT_FILE)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting document\n",
    "There are many different ways to split document. In this example we will use simple splitter that splits document into **chunks** of fixed size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(document)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How embed works\n",
    "\n",
    "[Link to playground](https://dashboard.cohere.com/playground/embed)\n",
    "\n",
    "An embedding is a **mathematical representation** of the semantic meaning of a word, sentence, or document.\n",
    "\n",
    "It's a projection of a concept in a high-dimensional space.\n",
    "\n",
    "Embeddings have a simple characteristic:\n",
    "* The projection of related concepts will be **close to each other**\n",
    "* While concepts with different meanings will **lie far away**\n",
    "\n",
    "![embed](../images/embed_subreddit_titles.png)\n",
    "\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
    "\n",
    "![embed](../images/system3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate embeddings for an arbitrary query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "try:\n",
    "    embedded_query = embeddings.embed_query(\"My mother has two sisters and one brother.\")\n",
    "except ConnectError as e:\n",
    "    print(f\"Error connecting to the model: {e}\")\n",
    "    print(\"Please make sure the model is running and try again.\")\n",
    "    exit(1)\n",
    "except ResponseError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Embedded length {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"How much sister does Biden have?\")\n",
    "sentence2 = embeddings.embed_query(\"Joanna has one twin brother and two older sisters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "query_sentence1_similarity , query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Vector Store\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches.\n",
    "\n",
    "![System 4](../images/system4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the vector store to find the most similar embeddings to a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the vector store to the chain\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model.\n",
    "\n",
    "Here is how we can connect the vector store to the chain:\n",
    "\n",
    "![System](../images/chain4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure a [Retriever](https://python.langchain.com/docs/how_to/#retrievers). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "retriever.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
    "\n",
    "We can create a map with the two inputs by using the `RunnableParallel` and `RunnablePassthrough` classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain using another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading transcription into the vector store\n",
    "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store2 = DocArrayInMemorySearch.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new chain using the correct vector store. This time we are using a different equivalent syntax to specify the `RunnableParallel` portion of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup2 = RunnableParallel(context=vector_store2.as_retriever(), question=RunnablePassthrough())\n",
    "\n",
    "print(setup2.invoke(\"What is synthetic intelligence?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    setup2\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "print(chain2.invoke(\"What is synthetic intelligence?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
