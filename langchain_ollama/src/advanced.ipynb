{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/index/whisper/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n",
      "[debug] yt-dlp version stable@2025.01.15 from yt-dlp/yt-dlp [c8541f8b1] (pip) API\n",
      "[debug] params: {'verbose': True, 'format': 'm4a/bestaudio/best', 'outtmpl': '../temp/tmpqr17x7v7/%(title)s.%(ext)s', 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.17 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n",
      "[debug] Python 3.9.21 (CPython arm64 64bit) - macOS-15.2-arm64-arm-64bit (OpenSSL 3.0.15 3 Sep 2024)\n",
      "[debug] exe versions: ffmpeg 7.1 (setts), ffprobe 7.1\n",
      "[debug] Optional libraries: certifi-2024.12.14, requests-2.32.3, sqlite3-3.47.1, urllib3-2.3.0\n",
      "[debug] Proxy map: {}\n",
      "[debug] Request Handlers: urllib, requests\n",
      "[debug] Loaded 1837 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=cdiD-9MMpb0\n",
      "[youtube] cdiD-9MMpb0: Downloading webpage\n",
      "[youtube] cdiD-9MMpb0: Downloading tv player API JSON\n",
      "[youtube] cdiD-9MMpb0: Downloading ios player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Loading youtube-nsig.6e1dd460 from cache\n",
      "[debug] [youtube] Decrypted nsig CRX72ukJ1CSY2NmNe => l9_M4VCUdQZsfg\n",
      "[debug] Loading youtube-nsig.6e1dd460 from cache\n",
      "[debug] [youtube] Decrypted nsig SxlXwwi9ORrG0vUBF => sMRwwA7a5OogNQ\n",
      "[debug] [youtube] cdiD-9MMpb0: ios client https formats require a PO Token which was not provided. They will be skipped as they may yield HTTP Error 403. You can manually pass a PO Token for this client with --extractor-args \"youtube:po_token=ios+XXX\". For more information, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#po-token-guide . To enable these broken formats anyway, pass --extractor-args \"youtube:formats=missing_pot\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] cdiD-9MMpb0: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec, channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] cdiD-9MMpb0: Downloading 1 format(s): 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr2---sn-8xjug5oxu-f5fs.googlevideo.com/videoplayback?expire=1737161678&ei=bqeKZ4nsH6fti9oP88WTuQw&ip=109.241.119.217&id=o-AO1MJ8sb_hJ6MIC83zuy6DB8kT14-2YjGe9LebxY1Mdh&itag=140&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1737140078%2C&mh=B_&mm=31%2C29&mn=sn-8xjug5oxu-f5fs%2Csn-f5f7lnld&ms=au%2Crdu&mv=m&mvi=2&pl=22&rms=au%2Cau&initcwndbps=3617500&bui=AY2Et-MIPC1qgPuTlosdHBdH7PLn0vNbbcRtm6MPOJNELpUVVJ3DAXW6AAae6PIXzzdmLZhEgF-XHOlB&vprv=1&svpuc=1&mime=audio%2Fmp4&ns=_tCmG5anWqEpJQF8ppm3meoQ&rqh=1&gir=yes&clen=202744240&dur=12527.489&lmt=1733476171765855&mt=1737139755&fvip=1&keepalive=yes&lmw=1&fexp=51326932%2C51335594%2C51353498%2C51371294%2C51384461&c=TVHTML5&sefc=1&txp=5532434&n=sMRwwA7a5OogNQ&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRAIgPQWWHHNv4xj47mX2rFGmwJa7GWDPpAX3Z7r0ctSS9MMCIACPIJR5FwBqPbl9WbK30YAgg1A21A5YEvx13txFJOSa&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRgIhAKw6zzmgPZuP82Iqe8Eur2IhQDde6IIBd5WZaUWhDfUyAiEA3LYQuJ1HuULEqO8B9X4llYlXLsSfWC0MTp0dL8LUiSc%3D\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: ../temp/tmpqr17x7v7/Andrej Karpathy： Tesla AI, Self-Driving, Optimus, Aliens, and AGI ｜ Lex Fridman Podcast #333.m4a\n",
      "[download] 100% of  193.35MiB in 00:00:13 at 14.03MiB/s    \n",
      "[FixupM4a] Correcting container of \"../temp/tmpqr17x7v7/Andrej Karpathy： Tesla AI, Self-Driving, Optimus, Aliens, and AGI ｜ Lex Fridman Podcast #333.m4a\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i 'file:../temp/tmpqr17x7v7/Andrej Karpathy： Tesla AI, Self-Driving, Optimus, Aliens, and AGI ｜ Lex Fridman Podcast #333.m4a' -map 0 -dn -ignore_unknown -c copy -f mp4 -movflags +faststart 'file:../temp/tmpqr17x7v7/Andrej Karpathy： Tesla AI, Self-Driving, Optimus, Aliens, and AGI ｜ Lex Fridman Podcast #333.temp.m4a'\n",
      "/Users/marcinpilarczyk/projects/python/RAGs/langchain_ollama/.venv/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:03.280]  I think it's possible that physics has exploits and we should be trying to find them.\n",
      "[00:03.280 --> 00:08.160]  arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,\n",
      "[00:08.160 --> 00:10.720]  somehow gives you a rounding error in the floating point.\n",
      "[00:10.720 --> 00:15.280]  Synthetic intelligences are kind of like the next stage of development.\n",
      "[00:15.280 --> 00:17.120]  And I don't know where it leads to.\n",
      "[00:17.120 --> 00:22.080]  Like at some point, I suspect the universe is some kind of a puzzle.\n",
      "[00:23.040 --> 00:27.440]  These synthetic AIs will uncover that puzzle and solve it.\n",
      "[00:30.000 --> 00:35.760]  The following is a conversation with Andre Kappathi, previously the director of AI at Tesla.\n",
      "[00:35.760 --> 00:43.840]  And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and\n",
      "[00:43.840 --> 00:47.040]  educators in the history of artificial intelligence.\n",
      "[00:47.760 --> 00:50.800]  This is the Lex Friedman podcast to support it.\n",
      "[00:50.800 --> 00:54.400]  Please check out our sponsors and now to your friends.\n",
      "[00:54.960 --> 00:56.880]  Here's Andre Kappathi.\n",
      "[00:57.840 --> 01:04.400]  What is a neural network and what does it seem to do such a surprisingly good job of learning?\n",
      "[01:04.400 --> 01:05.360]  What is a neural network?\n",
      "[01:05.360 --> 01:09.840]  It's a mathematical abstraction of the brain.\n",
      "[01:09.840 --> 01:11.600]  I would say that's how it was originally developed.\n",
      "[01:12.480 --> 01:14.400]  At the end of the day, it's a mathematical expression.\n",
      "[01:14.400 --> 01:17.280]  And it's a fairly simple mathematical expression when you get down to it.\n",
      "[01:17.280 --> 01:23.360]  It's basically a sequence of matrix-multiplyse, whichever link dot products mathematically.\n",
      "[01:23.360 --> 01:25.280]  And some null linearities thrown in.\n",
      "[01:25.360 --> 01:27.040]  And so it's a very simple mathematical expression.\n",
      "[01:27.600 --> 01:29.200]  And it's got knobs in it.\n",
      "[01:29.200 --> 01:30.000]  Many knobs.\n",
      "[01:30.000 --> 01:30.560]  Many knobs.\n",
      "[01:30.560 --> 01:34.080]  And these knobs are loosely related to the synapses in your brain.\n",
      "[01:34.080 --> 01:34.800]  They're trainable.\n",
      "[01:34.800 --> 01:35.680]  They're modifiable.\n",
      "[01:35.680 --> 01:40.640]  And so the idea is we need to find the setting of the knobs that makes the neural net do whatever\n",
      "[01:40.640 --> 01:42.720]  you wanted to do, like classify images and so on.\n",
      "[01:43.440 --> 01:45.600]  And so there's not too much mystery I would say in it.\n",
      "[01:45.600 --> 01:50.640]  Like you might think that basically don't want to end out with too much meaning with respect to\n",
      "[01:50.640 --> 01:52.640]  the brain and how it works.\n",
      "[01:52.720 --> 01:55.440]  It's really just a complicated mathematical expression with knobs.\n",
      "[01:55.440 --> 01:59.120]  And those knobs need a proper setting for it to do something desirable.\n",
      "[01:59.120 --> 02:03.200]  Yeah, but poetry is just the collection of letters with spaces.\n",
      "[02:03.200 --> 02:05.200]  But it can make us feel a certain way.\n",
      "[02:05.200 --> 02:08.160]  And in that same way, when you get a large number of knobs together,\n",
      "[02:08.160 --> 02:11.920]  whether it's inside the brain or inside a computer,\n",
      "[02:11.920 --> 02:15.840]  they seem to surprise us with their power.\n",
      "[02:15.840 --> 02:16.640]  Yeah.\n",
      "[02:16.640 --> 02:17.360]  I think that's fair.\n",
      "[02:17.360 --> 02:20.640]  So basically, I'm underselling it by a lot because\n",
      "[02:20.640 --> 02:25.440]  you definitely do get very surprising emergent behaviors out of these neural nets when they're\n",
      "[02:25.440 --> 02:28.640]  large enough and trained on complicated enough problems.\n",
      "[02:28.640 --> 02:32.880]  Like say, for example, the next word prediction in a massive data set from the internet.\n",
      "[02:33.520 --> 02:37.120]  And then these neural nets take on pretty surprising magical properties.\n",
      "[02:37.760 --> 02:41.520]  Yeah, I think it's kind of interesting how much you can get out of even very simple mathematical\n",
      "[02:41.520 --> 02:42.240]  formalism.\n",
      "[02:42.240 --> 02:46.080]  When your brain right now is talking, is it doing next word prediction?\n",
      "[02:47.040 --> 02:48.960]  Or is it doing something more interesting?\n",
      "[02:48.960 --> 02:53.280]  Well, it's definitely some kind of a generative model that's a GPT-like and prompted by you.\n",
      "[02:54.320 --> 02:58.560]  Yeah, so you're giving me a prompt and I'm kind of like responding to it in a generative way.\n",
      "[02:58.560 --> 03:04.640]  And by yourself, perhaps a little bit like are you adding extra prompts from your own memory\n",
      "[03:04.640 --> 03:05.520]  inside your head?\n",
      "[03:05.520 --> 03:06.400]  Hmm.\n",
      "[03:06.400 --> 03:06.800]  Or no.\n",
      "[03:06.800 --> 03:11.520]  Well, definitely feels like you're referencing some kind of a declarative structure of like memory and\n",
      "[03:11.520 --> 03:12.240]  so on.\n",
      "[03:12.240 --> 03:17.040]  And then you're putting that together with your prompt and giving away some extra.\n",
      "[03:17.120 --> 03:20.880]  How much of what you just said has been said by you before?\n",
      "[03:22.000 --> 03:23.520]  Nothing, basically, right?\n",
      "[03:23.520 --> 03:28.560]  No, but if you actually look at all the words you've ever said in your life and you do a search,\n",
      "[03:29.360 --> 03:34.080]  you'll probably said a lot of the same words in the same order before.\n",
      "[03:34.080 --> 03:34.640]  Yeah, could be.\n",
      "[03:35.200 --> 03:37.360]  I mean, I'm using phrases that are common, etc.\n",
      "[03:37.360 --> 03:41.920]  But I'm remixing it into a pretty sort of unique sentence at the end of the day.\n",
      "[03:41.920 --> 03:44.240]  But you're right, definitely, there's like a ton of remixing.\n",
      "[03:44.240 --> 03:52.320]  Why you didn't, you just like Magnus Carlson said, I'm rated 2,900, whatever, which is pretty decent.\n",
      "[03:52.320 --> 03:57.280]  I think you're talking very, you're not giving enough credit to neural nets here.\n",
      "[03:58.080 --> 04:05.200]  Why do they seem to, what's your best intuition about this emergent behavior?\n",
      "[04:05.200 --> 04:08.800]  I mean, it's kind of interesting because I'm simultaneously underselling them.\n",
      "[04:08.800 --> 04:12.720]  But I also feel like there's an element to which I'm over like, it's actually kind of incredible\n",
      "[04:12.720 --> 04:17.440]  that you can get so much emergent magical behavior out of them, despite them being so simple mathematically.\n",
      "[04:17.440 --> 04:21.760]  So I think those are kind of like two surprising statements that are kind of just juxtaposed together.\n",
      "[04:22.560 --> 04:26.480]  And I think basically what it is is we are actually fairly good at optimizing these neural nets.\n",
      "[04:27.120 --> 04:32.640]  And when you give them a hard enough problem, they are forced to learn very interesting solutions\n",
      "[04:32.640 --> 04:34.000]  in the optimization.\n",
      "[04:34.000 --> 04:38.240]  And those solutions basically have these emergent properties that are very interesting.\n",
      "[04:38.720 --> 04:42.320]  There's wisdom and knowledge in the knobs.\n",
      "[04:43.760 --> 04:49.280]  And so this representation that's in the knobs doesn't make sense to you intuitively,\n",
      "[04:49.280 --> 04:55.760]  the large number of knobs can hold a representation that captures some deep wisdom about the data.\n",
      "[04:55.760 --> 04:58.400]  It has looked at a lot of knobs.\n",
      "[04:58.400 --> 04:59.440]  It's a lot of knobs.\n",
      "[05:00.000 --> 05:05.600]  And somehow, so speaking concretely, one of the neural nets that people are very excited about right now\n",
      "[05:05.600 --> 05:10.160]  are our GPs, which are basically just next word prediction networks.\n",
      "[05:10.160 --> 05:14.800]  So you consume a sequence of words from the internet and you try to predict the next word.\n",
      "[05:15.440 --> 05:19.440]  And once you train these on a large enough data set,\n",
      "[05:21.120 --> 05:25.440]  they you can basically prompt these neural nets in arbitrary ways and you can ask them to solve\n",
      "[05:25.440 --> 05:30.240]  problems and they will. So you can just tell them you can make it look like you're trying to\n",
      "[05:31.840 --> 05:35.280]  solve some kind of a mathematical problem and they will continue what they think is the solution.\n",
      "[05:35.280 --> 05:36.800]  Based on what they've seen on the internet.\n",
      "[05:36.800 --> 05:40.640]  And very often, those solutions look very remarkably consistent.\n",
      "[05:40.640 --> 05:41.840]  Look correct potentially.\n",
      "[05:43.040 --> 05:45.280]  Do you still think about the brain side of it?\n",
      "[05:45.280 --> 05:49.440]  So as neural nets as an abstraction or mathematical abstraction of the brain,\n",
      "[05:49.440 --> 05:57.600]  you still draw wisdom from the biological neural networks or even the bigger question.\n",
      "[05:57.600 --> 06:00.640]  So your big fan of biology and biological computation,\n",
      "[06:01.280 --> 06:08.400]  what impressive thing is biology doing to you the computer, not yet, the gap?\n",
      "[06:08.400 --> 06:13.680]  I would say I'm definitely on a much more hesitant with the analogies to the brain than I think\n",
      "[06:13.680 --> 06:20.160]  you would see potentially in the field. And I kind of feel like certainly the way neural networks\n",
      "[06:20.160 --> 06:25.520]  started is everything stemmed from inspiration by the brain. But at the end of the day, artifacts\n",
      "[06:25.520 --> 06:30.080]  that you get after training, they are arrived at by a very different optimization process than the\n",
      "[06:30.080 --> 06:36.480]  optimization process that gave rise to the brain. And so I think I kind of think of it as a very\n",
      "[06:36.480 --> 06:41.840]  complicated alien artifact. It's something different. I'm sorry, the neural nets that were training.\n",
      "[06:42.480 --> 06:47.920]  They are complicated alien artifact. I do not make analogies to the brain because I think the\n",
      "[06:47.920 --> 06:53.040]  optimization process that gave rise to it is very different from the brain. So there was no multi-agent\n",
      "[06:53.040 --> 07:00.080]  self-play kind of setup and evolution. It was an optimization that is basically a, what amounts to\n",
      "[07:00.080 --> 07:05.680]  a compression objective on a mass and amount of data. Okay, so artificial neural networks are doing\n",
      "[07:05.680 --> 07:12.480]  compression and biological neural networks are now to survive. And they're not really doing\n",
      "[07:12.480 --> 07:18.560]  anything. They're an agent in a multi-agent self-play system that's been running for very, very\n",
      "[07:18.560 --> 07:25.120]  long time. That said, evolution has found that it is very useful to predict and have a predictive\n",
      "[07:25.120 --> 07:30.320]  model in the brain. And so I think our brain utilizes something that looks like that as a part of it.\n",
      "[07:30.880 --> 07:37.360]  But it has a lot more, you know, gadgets and gizmos and value functions and ancient nuclei that\n",
      "[07:37.360 --> 07:41.920]  are all trying to like make a survivor reproduce and everything else. And the whole thing through\n",
      "[07:41.920 --> 07:47.360]  embryogenesis is built from a single cell. I mean, it's just the code is inside the DNA.\n",
      "[07:48.080 --> 07:51.760]  And it just builds it up like the entire organism with the sounds.\n",
      "[07:51.760 --> 07:52.160]  It's crazy.\n",
      "[07:53.520 --> 07:56.640]  And legs. Yes. And like it does it pretty well.\n",
      "[07:57.520 --> 08:02.480]  It should not be possible. So there's some learning going on. There's some, there's some kind of\n",
      "[08:02.480 --> 08:08.880]  computation going through that building process. I mean, I don't know where, if you were just to\n",
      "[08:08.880 --> 08:14.640]  look at the entirety of history of life on earth, where do you think is the most interesting\n",
      "[08:14.640 --> 08:22.080]  invention? Is it the origin of life itself? Is it just jumping to eukaryotes? Is it mammals?\n",
      "[08:22.080 --> 08:29.120]  Is it humans themselves, homo sapiens? The origin of intelligence or highly complex intelligence?\n",
      "[08:31.600 --> 08:34.240]  Or is it all just a continuation of the same kind of process?\n",
      "[08:35.840 --> 08:40.240]  Certainly, I would say it's an extremely remarkable story that I'm only like briefly learning\n",
      "[08:40.320 --> 08:46.240]  about recently, all the way from actually like you almost have to start at the formation of earth\n",
      "[08:46.240 --> 08:49.600]  and all of its conditions and the entire solar system and how everything is arranged with\n",
      "[08:49.600 --> 08:54.640]  Jupiter and Moon and the habitable zone and everything. And then you have an active earth\n",
      "[08:55.440 --> 09:02.080]  that's turning over material. And then you start with a biogenesis and everything. And so it's\n",
      "[09:02.080 --> 09:08.560]  all like a pretty remarkable story. I'm not sure that I can pick like a single unique piece of it\n",
      "[09:09.200 --> 09:14.240]  that I find most interesting. I guess for me as an artificial intelligence researcher,\n",
      "[09:14.240 --> 09:20.320]  it's probably the last piece. We have lots of animals that are not building technological\n",
      "[09:20.320 --> 09:26.080]  society, but we do. And it seems to have happened very quickly. It seems to have happened very recently.\n",
      "[09:26.880 --> 09:31.760]  And something very interesting happened there that I don't fully understand. I almost understand\n",
      "[09:31.760 --> 09:37.920]  everything else, I think intuitively, but I don't understand exactly that part and how quick it was.\n",
      "[09:37.920 --> 09:42.560]  Both explanations would be interesting. One is that this is just a continuation of the same kind\n",
      "[09:42.560 --> 09:47.840]  of process. There's nothing special about humans. That would be deeply understanding that would be\n",
      "[09:47.840 --> 09:53.840]  very interesting. That we think of ourselves as special, but it was obvious. It was already written\n",
      "[09:54.400 --> 10:02.960]  in the code that you would have greater and greater intelligence emerging. And then the other\n",
      "[10:03.040 --> 10:08.400]  explanation, which is something truly special happened, something like a rare event, whether it's\n",
      "[10:08.400 --> 10:14.240]  like crazy rare event, like space Odyssey, what would it be? See if you say like the invention of\n",
      "[10:14.240 --> 10:24.640]  fire or the, as Richard and Rangham says, the beta males deciding a clever way to kill the alpha\n",
      "[10:24.640 --> 10:31.600]  males by collaborating. So just optimizing the collaboration, the multi agent aspect of the multi agent.\n",
      "[10:31.600 --> 10:38.160]  And that really being constrained on resources and trying to survive the collaboration aspect\n",
      "[10:38.160 --> 10:43.360]  is what created the complex intelligence. But it seems like it's a natural algorithm, the evolution\n",
      "[10:43.360 --> 10:49.760]  process. What could possibly be a magical thing that happened? Like a rare thing that would say\n",
      "[10:49.760 --> 10:55.200]  that humans are actually human level intelligence is actually a really rare thing in the universe.\n",
      "[10:56.640 --> 11:00.080]  Yeah, I'm hesitant to say that it is rare, by the way, but it definitely seems like\n",
      "[11:00.880 --> 11:05.360]  it's kind of like a punctuated equilibrium where you have lots of exploration and then you have\n",
      "[11:05.360 --> 11:12.160]  certain leaps, sparse leaps in between. So of course, like origin of life would be one, DNA, sex,\n",
      "[11:12.720 --> 11:19.680]  eukaryotic, eukaryotic life, the endosymbiosis event where the archaeon ate the old bacteria,\n",
      "[11:19.680 --> 11:24.160]  you know, just the whole thing. And then of course, emergence of consciousness and so on. So it seems\n",
      "[11:24.160 --> 11:27.760]  like definitely there are sparse events where mass amount of progress was made, but yeah, it's kind\n",
      "[11:27.760 --> 11:34.000]  of hard to pick one. So you don't think humans are unique. I got to ask you how many intelligent\n",
      "[11:34.000 --> 11:42.640]  alien civilizations do you think are out there? And is there intelligence different or similar to ours?\n",
      "[11:44.480 --> 11:49.680]  Yeah, I've been preoccupied with this question quite a bit recently, basically the Fermi\n",
      "[11:49.680 --> 11:55.040]  paradox and just thinking through. And the reason actually that I am very interested in the\n",
      "[11:55.040 --> 11:58.800]  origin of life is fundamentally trying to understand how common it is that there are technological\n",
      "[11:58.800 --> 12:08.240]  societies out there in space. And the more I study it, the more I think that there should be\n",
      "[12:08.240 --> 12:14.720]  quite a lot. Why haven't we heard from them? Because I agree with you. It feels like I just don't see\n",
      "[12:15.920 --> 12:21.360]  why what we did here in Earth is so difficult to do. Yeah, and especially when you get into the\n",
      "[12:21.360 --> 12:27.360]  details of it, I used to think origin of life was very, it was this magical rare event, but then\n",
      "[12:27.360 --> 12:34.640]  you read books like, for example, in the claim, the vital question, life ascending, etc. And he\n",
      "[12:34.640 --> 12:40.000]  really gets in and he really makes you believe that this is not that rare basic chemistry. You have\n",
      "[12:40.000 --> 12:44.320]  an active Earth and you have your alkaline vents and you have lots of alkaline waters mixing\n",
      "[12:44.320 --> 12:49.280]  with a devotion and you have your proton gradients and you have little porous pockets of these alkaline\n",
      "[12:49.360 --> 12:54.880]  vents that concentrate chemistry. And basically as he steps through all of these little pieces,\n",
      "[12:54.880 --> 12:59.920]  you start to understand that actually this is not that crazy. You could see this happen on other\n",
      "[12:59.920 --> 13:07.760]  systems. And he really takes you from just a geology to primitive life and he makes it feel like\n",
      "[13:07.760 --> 13:14.640]  it's actually pretty plausible. And also like the origin of life didn't, was actually fairly fast\n",
      "[13:14.720 --> 13:19.120]  after formation of Earth. If I'm or correctly just a few hundred million years for something like\n",
      "[13:19.120 --> 13:23.600]  that after basically when it was possible, life actually arose. And so that makes me feel that\n",
      "[13:23.600 --> 13:27.200]  that is not the constraint, that is not the limiting variable and that life should actually be\n",
      "[13:27.200 --> 13:36.000]  fairly common. And then where the drop offs are is very interesting to think about. I currently\n",
      "[13:36.000 --> 13:40.480]  think that there's no major drop offs basically and so there should be quite a lot of life. And\n",
      "[13:40.560 --> 13:44.560]  basically what it where that brings me to then is the only way to reconcile the fact that we\n",
      "[13:44.560 --> 13:50.480]  haven't found anyone and so on is that we just can't we can't see them. We can't observe them.\n",
      "[13:50.480 --> 13:55.680]  Just a quick brief comment. Nick Lane and a lot of biologists I talked to, they really seem to\n",
      "[13:55.680 --> 14:01.760]  think that the jump from bacteria to more complex organisms is the hardest jump. The you carry\n",
      "[14:01.760 --> 14:09.120]  the glycology. Yeah. Which I don't I get it. They're much more knowledgeable than me about like\n",
      "[14:09.120 --> 14:15.200]  the intricacies of biology. But that seems like crazy. Because how much how many single cell\n",
      "[14:15.200 --> 14:21.840]  organisms are there? Like and how much time you have surely it's not that difficult. Like an\n",
      "[14:21.840 --> 14:28.320]  a billion years is not even that long of a time really. Just all these bacteria under constrained\n",
      "[14:28.320 --> 14:33.600]  resources battling it out. I'm sure they can invent more complex. Like I don't understand. It's like\n",
      "[14:33.840 --> 14:39.280]  how to move from a hello world program to like invent a function or something like that. I don't\n",
      "[14:39.280 --> 14:46.640]  yeah. So I don't yeah so I'm with you. I just feel like I don't see any if the origin of life\n",
      "[14:46.640 --> 14:50.160]  that would be my intuition. That's the hardest thing. But if that's not the hardest thing because\n",
      "[14:50.160 --> 14:55.200]  it happens so quickly then it's got to be everywhere. And yeah maybe we're just too dumb to see it.\n",
      "[14:55.200 --> 14:59.200]  Well it's just we don't have really good mechanisms for seeing this life. I mean by what\n",
      "[15:00.160 --> 15:06.720]  how so I'm not an expert just to preface this but just from what I mean. I want to meet an expert\n",
      "[15:06.720 --> 15:11.040]  on alien intelligence and how to communicate. I'm very suspicious of our ability to to find\n",
      "[15:11.040 --> 15:16.240]  these intelligence is out there and to find these Earth like radio waves for example are terrible.\n",
      "[15:16.240 --> 15:21.440]  Their power drops off as basically one over our square. So I remember reading that our current\n",
      "[15:21.440 --> 15:28.080]  radio waves would not be the ones that we are broadcasting would not be measurable by our devices\n",
      "[15:28.080 --> 15:32.560]  today. Only like was it like one tenth of a light year away like not even basically tiny\n",
      "[15:32.560 --> 15:38.480]  distance because you really need like a targeted transmission of massive power directed somewhere\n",
      "[15:38.480 --> 15:43.040]  for this to be picked up on long long distances. And so I just think that our ability to measure\n",
      "[15:43.040 --> 15:47.520]  is is not amazing. I think there's probably other civilizations out there. And then the big\n",
      "[15:47.520 --> 15:51.120]  question is why don't they build one element probes and why don't they interstellar travel across\n",
      "[15:51.120 --> 15:55.600]  the entire galaxy. And my current answer is it's probably interstellar travel is like really hard.\n",
      "[15:56.400 --> 15:59.680]  You have the interstellar medium if you want to move at close to speed of light you're going to\n",
      "[15:59.680 --> 16:05.600]  be encountering bullets along the way because even like tiny hydrogen atoms and little particles of\n",
      "[16:05.600 --> 16:10.160]  dust are basically have like massive kinetic energy at those speeds. And so basically you need\n",
      "[16:10.160 --> 16:14.720]  some kind of shielding you need that you have all the cosmic radiation. It's just like brutal out\n",
      "[16:14.720 --> 16:19.040]  there. It's really hard. And so my thinking is maybe interstellar travel is just extremely hard.\n",
      "[16:19.680 --> 16:28.400]  And you have to do it very slow. It feels like we're not a billion years away from doing that.\n",
      "[16:28.400 --> 16:33.360]  It just might be that it's very you have to go very slowly potentially as an example through space.\n",
      "[16:34.160 --> 16:38.240]  Right. As opposed to close to the speed of light. So I'm suspicious basically of our ability to\n",
      "[16:38.240 --> 16:43.280]  measure life and I'm suspicious of the ability to just permeate all of space in the galaxy or\n",
      "[16:43.280 --> 16:47.920]  across galaxies. And that's the only way that I can certainly I can currently see away around it.\n",
      "[16:47.920 --> 16:53.120]  Yeah, it's kind of mind blowing to think that there's trillions of intelligent alien\n",
      "[16:53.120 --> 16:59.280]  civilizations out there kind of slowly traveling through space. Made to meet each other and some of\n",
      "[16:59.280 --> 17:06.400]  them meet some of them go to war some of them collaborate. Or they're all just independent. They\n",
      "[17:06.400 --> 17:13.040]  are all just like little pockets. I know. Was statistically if there's like if it's this trillions of\n",
      "[17:13.040 --> 17:17.120]  them surely some of them some of the pockets are close enough to get some of them happen to be close.\n",
      "[17:17.600 --> 17:22.880]  In the close enough to see each other and then once you see once you see something that is\n",
      "[17:23.600 --> 17:30.080]  definitely complex life like if we see something yeah we're probably going to be severe like\n",
      "[17:30.080 --> 17:34.880]  intensely aggressively motivated to figure out what the hell that is and try to meet them.\n",
      "[17:34.880 --> 17:43.680]  But will be your first instinct to try to like at a generational level meet them or defend against them\n",
      "[17:44.320 --> 17:52.800]  or will be your instinct as a president of the United States. And the scientist I don't know\n",
      "[17:52.800 --> 17:57.840]  which hat you prefer in this question. Yeah, I think the question it's really hard.\n",
      "[17:59.760 --> 18:05.840]  I will say like for example for us we have lots of primitive life forms on earth next to us.\n",
      "[18:05.840 --> 18:10.800]  We have all kinds of ants and everything else in a shared space with them. And we are hesitant to\n",
      "[18:10.800 --> 18:16.480]  impact on them and to we are trying to protect them by default because they are amazing interesting\n",
      "[18:16.480 --> 18:20.720]  dynamical systems that took a long time to evolve and they are interesting and special. And\n",
      "[18:21.360 --> 18:29.440]  I don't know that you want to destroy that by default. And so I like complex dynamical systems\n",
      "[18:29.440 --> 18:36.720]  that took a lot of time to evolve. I think I'd like to I like to preserve it if I can afford to.\n",
      "[18:36.720 --> 18:40.640]  And I'd like to think that the same would be true about the galactic resources and that\n",
      "[18:41.360 --> 18:46.000]  they would think that we're kind of incredible interesting story that took time. It took a few\n",
      "[18:46.000 --> 18:50.560]  billion years to unravel and you don't want to just destroy it. I could see two aliens talking\n",
      "[18:50.560 --> 18:57.440]  about earth right now and saying I'm a big fan of complex dynamical systems. So I think it's\n",
      "[18:57.440 --> 19:02.720]  with a value to preserve these and we basically are a video game they watch or show a TV show that\n",
      "[19:02.720 --> 19:08.880]  they watch. Yeah I think you would need like a very good reason I think to to destroy it. Like why\n",
      "[19:08.880 --> 19:12.320]  don't we destroy these ant farms and so on. It's because we're not actually like really in direct\n",
      "[19:12.320 --> 19:19.360]  competition with them right now. We do it accidentally and so on but there's plenty of resources.\n",
      "[19:19.360 --> 19:22.160]  And so why would you destroy something that is so interesting and precious?\n",
      "[19:22.160 --> 19:26.960]  Well from a scientific perspective you might probe it. You might interact with it later.\n",
      "[19:26.960 --> 19:31.680]  Exactly. You might want to learn something from it. So I wonder there could be certain physical\n",
      "[19:31.760 --> 19:36.480]  phenomena that we think is a physical phenomena but it's actually interacting with us to like\n",
      "[19:36.480 --> 19:40.000]  poke the finger and see what happens. I think it should be very interesting to scientists.\n",
      "[19:40.000 --> 19:45.600]  Other alien scientists what happened here and you know what we're seeing today as a snapshot\n",
      "[19:45.600 --> 19:52.160]  basically it's a result of a huge amount of computation of like billion years or something like that.\n",
      "[19:52.160 --> 19:57.840]  So it could have been initiated by aliens. This could be a computer running a program. Like wouldn't\n",
      "[19:57.920 --> 20:03.760]  okay if you had the power to do this when you okay for sure at least I would I would pick\n",
      "[20:04.880 --> 20:09.200]  a earth-like planet that has the conditions based my understanding of the chemistry prerequisites\n",
      "[20:09.200 --> 20:16.480]  for life and I would see it with life and run it right. Like yeah wouldn't you 100% do that and\n",
      "[20:16.480 --> 20:22.560]  observe it and then protect I mean that that's not just the hell of a good TV show it's it's a good\n",
      "[20:22.560 --> 20:31.200]  scientific experiment yeah and that it is it's physical simulation right what maybe maybe the\n",
      "[20:31.200 --> 20:38.800]  evolution is the most like actually running it is the most efficient way to understand\n",
      "[20:39.360 --> 20:44.800]  computation or to compute stuff for understand life or you know what life looks like and what\n",
      "[20:44.800 --> 20:49.520]  branches it can take. It does make me kind of feel weird there were part of a science experiment but\n",
      "[20:49.520 --> 20:54.160]  maybe it's everything's a science experiment. Does that change anything for us?\n",
      "[20:54.880 --> 21:00.960]  If we're a science experiment I don't know two descendants of age talking about being inside\n",
      "[21:00.960 --> 21:05.600]  a science experiment. I'm suspicious of this idea of like deliberate pens premier as you described\n",
      "[21:05.600 --> 21:11.040]  it. I don't see a divine intervention in some way in the in the historical record right now.\n",
      "[21:11.040 --> 21:16.720]  I do feel like the story in these in these books like Nikolai's books and so on sort of makes sense\n",
      "[21:17.280 --> 21:23.040]  and it makes sense how life arose on earth uniquely and yeah I don't need a I mean I don't\n",
      "[21:23.040 --> 21:27.760]  need to reach for more exotic explanations right now. Sure but NPCs inside a video game don't\n",
      "[21:28.960 --> 21:35.280]  don't observe any divine intervention either and we might just be all NPCs running a kind of code.\n",
      "[21:35.280 --> 21:38.960]  Maybe eventually they will. Currently NPCs are really dumb but once they're running GPTs\n",
      "[21:39.760 --> 21:46.240]  maybe they will be like hey this is really suspicious what the hell. So you famously tweeted it looks\n",
      "[21:46.320 --> 21:53.520]  like if you bombard earth with photons for a while you can emit a roaster. So if like in Hitchhiker's\n",
      "[21:53.520 --> 21:59.120]  guide to the galaxy we would summarize the story of earth so in that book it's mostly harmless.\n",
      "[22:00.320 --> 22:04.800]  What do you think is it all the possible stories like a paragraph long or a sentence long\n",
      "[22:05.680 --> 22:12.640]  that earth could be summarized as? Once it's done it's computation. So like all the possible\n",
      "[22:12.640 --> 22:20.640]  full if earth is a book right yeah probably there has to be an ending I mean there's going to be\n",
      "[22:20.640 --> 22:25.520]  an end to earth and you could end in all kinds of ways you can end soon you can end later. What do you\n",
      "[22:25.520 --> 22:32.240]  think are the possible stories? Well definitely there seems to be yeah you're sort of it's pretty\n",
      "[22:32.240 --> 22:37.600]  incredible that these self-replicating systems will basically arise from the dynamics and then\n",
      "[22:37.600 --> 22:41.200]  they perpetuate themselves and become more complex and eventually become conscious and build\n",
      "[22:41.760 --> 22:47.840]  a society and I kind of feel like in some sense it's kind of like a deterministic wave that\n",
      "[22:47.840 --> 22:52.640]  you know that kind of just like happens on any you know any sufficiently well-arranged system\n",
      "[22:52.640 --> 22:57.040]  like earth. And so I kind of feel like there's a certain sense of inevitability in it\n",
      "[22:58.320 --> 23:06.800]  and it's really beautiful and it ends somehow right so it's a it's a chemically a diverse environment\n",
      "[23:07.760 --> 23:16.160]  where complex dynamical systems can evolve and become more further further complex but then there's\n",
      "[23:16.160 --> 23:24.160]  a certain what is it there's certain terminating conditions. Yeah I don't know what the\n",
      "[23:24.160 --> 23:27.440]  terminating conditions are but definitely there's a trend line of something and we're part of\n",
      "[23:27.440 --> 23:32.320]  that story and like where does that where does it go? So you know we're famously described often\n",
      "[23:32.320 --> 23:36.960]  as a biological bootloader for AIs and that's because humans I mean they were an incredible\n",
      "[23:37.920 --> 23:44.960]  biological system and we're capable of computation and you know and love and so on but we're\n",
      "[23:44.960 --> 23:48.480]  extremely inefficient as well like we're talking to each other through audio it's just kind of\n",
      "[23:48.480 --> 23:55.040]  embarrassing honestly they were manipulating like seven symbols, seriously we're using vocal chords\n",
      "[23:55.040 --> 23:59.360]  it's all happening over like multiple seconds. It's just like kind of embarrassing when you step\n",
      "[23:59.360 --> 24:06.080]  down to the frequencies at which computers operate or are you able to operate on and so basically\n",
      "[24:06.080 --> 24:13.520]  it does seem like synthetic intelligences are kind of like the next stage of development and I don't\n",
      "[24:13.520 --> 24:21.680]  know where it leads to like at some point I suspect the universe is some kind of a puzzle and these\n",
      "[24:21.760 --> 24:29.200]  synthetic AIs will uncover that puzzle and solve it. And then what happens after right? Like what\n",
      "[24:29.200 --> 24:35.280]  because if you just like fast forward earth many billions of years it's like it's quiet and then\n",
      "[24:35.280 --> 24:39.680]  it's like to turmoil you see like city lights and stuff like that and then what happens at like\n",
      "[24:39.680 --> 24:46.720]  at the end like is it like a pwp or is it like calming is it explosion is it like earth like open\n",
      "[24:46.800 --> 24:53.440]  like a giant because you said emit roasters like we'll start emitting like like a giant\n",
      "[24:54.240 --> 24:59.840]  number of like satellites. Yes it's some kind of a crazy explosion and we're living we're like\n",
      "[24:59.840 --> 25:04.080]  we're stepping through a explosion and we're like living day to day and it doesn't look like it but\n",
      "[25:04.080 --> 25:08.880]  it's actually if you I saw a very cool animation of earth and life on earth and basically nothing\n",
      "[25:08.880 --> 25:13.040]  happens for a long time and then the last like two seconds like basically cities and everything and\n",
      "[25:14.000 --> 25:17.360]  and the lower orbit just gets cluttered and just the whole thing happens in the last two seconds\n",
      "[25:17.360 --> 25:23.840]  and you're like this is exploding. This is a state of explosion. So if you play yeah yeah if you play\n",
      "[25:23.840 --> 25:29.200]  at a normal speed yeah it'll just look like an explosion. It's a firecracker we're living in a\n",
      "[25:29.200 --> 25:34.000]  firecracker where it's going to start emitting all kinds of interesting things. Yeah and then\n",
      "[25:34.720 --> 25:40.240]  the so explosion doesn't it might actually look like a little explosion with lights and fire and\n",
      "[25:40.240 --> 25:45.440]  energy emitted all that kind of stuff but when you look inside the details of the explosion there's\n",
      "[25:45.440 --> 25:52.400]  actual complexity happening where there's like a human life or some kind of life. We hope it's\n",
      "[25:52.400 --> 25:58.560]  another destructive firecracker. It's kind of like a constructive firecracker. All right so given\n",
      "[25:58.560 --> 26:02.960]  that I think hilarious this guy. It is really interesting to think about like what the puzzle of\n",
      "[26:02.960 --> 26:07.680]  the universe is did the creator of the universe give us a message like for example in the book contact\n",
      "[26:08.480 --> 26:15.840]  Carl Sagan there's a message for humanity for any civilization in the digits in the expansion of\n",
      "[26:15.840 --> 26:21.680]  pie in base 11 eventually. We're just kind of interesting thought maybe we're supposed to be giving\n",
      "[26:21.680 --> 26:26.240]  a message to our creator. Maybe we're supposed to somehow create some kind of a quantum mechanical\n",
      "[26:26.240 --> 26:30.960]  system that alerts them to our intelligent presence here because if you think about it from their\n",
      "[26:30.960 --> 26:35.920]  perspective it's just say like quantum field theory massive like solar atomic bomb like thing\n",
      "[26:36.560 --> 26:40.720]  and like how do you even notice that we exist you might not even be able to pick us up in\n",
      "[26:40.720 --> 26:45.520]  that simulation and so how do you how do you prove that you exist that you're intelligent and that\n",
      "[26:45.520 --> 26:50.080]  you're part of the universe. So this is like a touring test for intelligence from Earth.\n",
      "[26:50.080 --> 26:55.360]  Yes. I got the creators I mean maybe this is like trying to complete the next war in the\n",
      "[26:55.360 --> 27:00.720]  Suns this is a complicated way of that like Earth is just is basically sending a message back.\n",
      "[27:00.720 --> 27:04.400]  Yeah the puzzle is basically like alerting the creator that we exist. Yeah.\n",
      "[27:04.400 --> 27:09.040]  Or maybe the puzzle is just to just break out of the system and just you know stick it to the\n",
      "[27:09.040 --> 27:14.800]  creator in some way. Basically like if you're playing a video game you can you can somehow find\n",
      "[27:14.800 --> 27:20.160]  an exploit and find a way to execute on the host machine in the arbitrary code. There's some\n",
      "[27:20.720 --> 27:26.720]  for example I believe someone got Mario a game of Mario to play pong just by exploiting it and then\n",
      "[27:27.840 --> 27:33.120]  creating a basically writing writing code and being able to execute arbitrary code in the game.\n",
      "[27:33.120 --> 27:36.400]  And so maybe we should be maybe that's the puzzle is that we should be\n",
      "[27:38.160 --> 27:42.160]  find a way to exploit it. So so I think like some of these synthetic ares will eventually find the\n",
      "[27:42.160 --> 27:46.080]  universe to be some kind of a puzzle and then solve it in some way and that's kind of like the end\n",
      "[27:46.080 --> 27:54.640]  game somehow. Do you often think about it as a as a simulation so as the universe being a kind\n",
      "[27:54.640 --> 28:00.560]  of computation that has might have bugs and exploits. Yes. Yeah I think so. I think that physics is\n",
      "[28:00.560 --> 28:04.720]  essentially I think it's possible that physics has exploits and we should be trying to find them\n",
      "[28:04.720 --> 28:09.520]  arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,\n",
      "[28:09.520 --> 28:16.960]  somehow gives you a rounding error in the floating point. Yeah that's right and more and more\n",
      "[28:16.960 --> 28:22.160]  sophisticated exploits that those are jokes but that could be actually. Yeah we'll find some way\n",
      "[28:22.160 --> 28:27.040]  to extract infinite energy. For example when you train reinforcement learning agents in physical\n",
      "[28:27.040 --> 28:32.160]  simulations and you ask them to say run quickly on the flat ground they'll end up doing all kinds\n",
      "[28:32.160 --> 28:36.880]  of like weird things in part of that optimization right they'll get on their back lag and they'll\n",
      "[28:36.880 --> 28:42.000]  slide across the floor and it's because the optimization the reinforcement learning optimization\n",
      "[28:42.000 --> 28:45.680]  on that agent has figured out a way to extract infinite energy from the friction forces and\n",
      "[28:46.640 --> 28:50.640]  basically their poor implementation and they found a way to generate infinite energy and just\n",
      "[28:50.640 --> 28:56.000]  slide across the surface and it's not what you expected it's just sort of like a perverse solution.\n",
      "[28:56.080 --> 28:59.360]  And so maybe we can find something like that maybe we can be that little\n",
      "[29:00.160 --> 29:07.760]  dog in this physical simulation. The cracks or escapes the intended consequences of the physics\n",
      "[29:07.760 --> 29:11.920]  that the universe came up with. Yeah we'll figure out some kind of shortcut to some weirdness\n",
      "[29:11.920 --> 29:17.520]  and then see the problem with that weirdness is the first person to discover the weirdness\n",
      "[29:17.520 --> 29:24.560]  like sliding in the back legs that's all we're going to do. Yeah it's very quickly become\n",
      "[29:24.560 --> 29:32.080]  everybody does that thing. So like the paperclip maximizer is a ridiculous idea but that very well\n",
      "[29:32.720 --> 29:38.560]  could be what then we'll just we'll just all switch that because it's so fun. Well no person\n",
      "[29:38.560 --> 29:43.600]  will discover it I think by the way I think it's going to have to be some kind of a super intelligent\n",
      "[29:43.600 --> 29:48.160]  AGI of a Thorpe generation like we're building the first generation AGI.\n",
      "[29:48.800 --> 29:57.680]  Maybe you know third generation. Yeah so the bootloader for an AI that AI will be a bootloader\n",
      "[29:57.680 --> 30:04.080]  for another AI. And then there's no way for us to interest back like what that might have been.\n",
      "[30:04.080 --> 30:07.680]  I think it's very likely that these things for example like say you have these AGIs it's very\n",
      "[30:07.680 --> 30:12.000]  likely that for example they will be completely inert. I like these kinds of sci-fi books sometimes\n",
      "[30:12.000 --> 30:15.920]  where these things are just completely inert they don't interact with anything and I find that\n",
      "[30:15.920 --> 30:20.720]  kind of beautiful because they probably they've probably figured out the meta-metagame of the\n",
      "[30:20.720 --> 30:24.800]  universe in some way potentially they're they're doing something completely beyond our imagination\n",
      "[30:25.920 --> 30:32.240]  and they don't interact with simple chemical life forms. Why would you do that? So I find those\n",
      "[30:32.240 --> 30:37.200]  kinds of ideas compelling. What's their source of fun? What are they doing? What's the source of pleasure?\n",
      "[30:37.200 --> 30:44.320]  Solving in the universe but inert so can you define what it means inert so they escape the\n",
      "[30:44.320 --> 30:53.280]  interactual physics in our reality as in they will behave in some very like strange way to us\n",
      "[30:53.280 --> 30:58.400]  because they're beyond they're playing the meta game and the meta game is probably say like\n",
      "[30:58.400 --> 31:02.560]  arranging quantum mechanical systems in some very weird ways to extract infinite energy\n",
      "[31:03.120 --> 31:08.800]  solve the digital expansion of pie to whatever amount they will build their own like little fusion\n",
      "[31:08.800 --> 31:13.600]  reactors or something crazy like they're doing something beyond comprehension and not understandable\n",
      "[31:14.320 --> 31:20.880]  and actually brilliant under the hood. What if quantum mechanics itself is the system and we're just\n",
      "[31:20.880 --> 31:28.960]  thinking it's physics but we're really parasites on not parasite we're not really hurting physics\n",
      "[31:28.960 --> 31:35.200]  we're just living on this organisms this organism and we're like trying to understand it but really\n",
      "[31:35.200 --> 31:40.640]  it is an organism and with a deep deep intelligence maybe physics itself is\n",
      "[31:40.960 --> 31:48.640]  the the the organism that's doing the super interesting thing and we're just like one little thing\n",
      "[31:48.640 --> 31:53.920]  yeah and sitting on top of it try to get energy from it we're just kind of like these particles\n",
      "[31:53.920 --> 31:58.880]  in the wave that I feel like is mostly deterministic and takes universe from some kind of a big bang\n",
      "[31:58.880 --> 32:05.120]  to some kind of a super intelligent replicator some kind of a stable point in the universe given\n",
      "[32:05.120 --> 32:11.520]  these laws of physics you don't think as Einstein said God doesn't play dice so you think it's\n",
      "[32:11.520 --> 32:15.040]  mostly deterministic there's no randomness in the thing I think as a deterministic oh there's\n",
      "[32:15.040 --> 32:21.200]  tons of well I'm I'm gonna be careful with randomness pseudo random yeah I don't like random I think\n",
      "[32:21.200 --> 32:26.000]  maybe the laws of physics are deterministic um yeah I think they're deterministic just got really\n",
      "[32:26.000 --> 32:31.600]  uncomfortable with this question I just do you have anxiety about whether the universe is random\n",
      "[32:31.600 --> 32:39.120]  or not there's no randomness it's you like goodwill hunting it's not your fault Andre\n",
      "[32:42.560 --> 32:48.800]  so you don't like randomness yeah I think it's unsettling I think it's a deterministic system I\n",
      "[32:48.800 --> 32:53.680]  think that things that look random like say the collapse of the wave function etc I think they're\n",
      "[32:53.680 --> 32:58.880]  actually deterministic just entanglement and so on and some kind of a multiverse theory something\n",
      "[32:59.520 --> 33:05.520]  okay so why does it feel like we have a free will like if I raise this hand I chose to do this\n",
      "[33:05.520 --> 33:14.560]  now um what that doesn't feel like a deterministic thing it feels like I'm making a choice it\n",
      "[33:14.560 --> 33:21.680]  feels like it okay so it's all feelings it's just feelings yeah so when RL agent is making a choice\n",
      "[33:21.680 --> 33:28.400]  is that um it's not really making a choice the choice is all already there yeah you're interpreting\n",
      "[33:28.400 --> 33:33.200]  the choice and you're creating a narrative for or haven't made it yeah and now we're talking about\n",
      "[33:33.200 --> 33:39.680]  the narrative it's very meta looking back what is the most beautiful or surprising idea in deep learning\n",
      "[33:39.680 --> 33:47.280]  or AI in general that you've come across you've seen this field explode uh and grow in interesting\n",
      "[33:47.280 --> 33:53.760]  ways just what what cool ideas like like we made you sit back and go hmm small bigger small\n",
      "[33:54.400 --> 34:01.200]  well the one that I've been thinking about recently the most probably is the the transformer\n",
      "[34:01.200 --> 34:08.160]  architecture um so basically uh neural hours have a lot of architectures that were trendy have come\n",
      "[34:08.160 --> 34:13.200]  and gone for different uh in a sensor in modalities like for vision audio text you would process them\n",
      "[34:13.200 --> 34:17.280]  with different looking neural nuts and recently we've seen these come this convergence towards one\n",
      "[34:17.280 --> 34:22.480]  architecture the transformer and uh you can feed it video or you can feed it you know images or\n",
      "[34:22.480 --> 34:27.120]  speech or text and it just gobbles it up and it's kind of like a bit of a general purpose\n",
      "[34:28.080 --> 34:33.440]  computer there is also trainable and very efficient to run on our hardware and so uh this paper\n",
      "[34:33.440 --> 34:39.520]  came out in 2016 I want to say um attention is all you need attention is all you need you could\n",
      "[34:39.520 --> 34:47.760]  assess the paper title in retrospect that it wasn't um it didn't foresee the bigness of the impact\n",
      "[34:47.760 --> 34:51.440]  yeah it was going to have yeah I'm not sure if the authors were aware of the impact that that\n",
      "[34:51.440 --> 34:56.160]  paper would go on to have probably they weren't but I think they were aware of some of the motivations\n",
      "[34:56.160 --> 35:00.640]  and design decisions beyond the transformer and they chose not to I think expand on it in that way\n",
      "[35:00.640 --> 35:06.560]  in the paper and so I think they had an idea that there was more um then just the surface of just\n",
      "[35:06.560 --> 35:09.920]  like over just doing translation and here's a better architecture you're not just doing translation\n",
      "[35:09.920 --> 35:14.240]  this is like a really cool differentiable optimizable efficient computer that you've proposed\n",
      "[35:14.720 --> 35:18.800]  and maybe they didn't have all of that foresight but I think it's really interesting isn't it funny\n",
      "[35:18.880 --> 35:25.840]  sorry to interrupt that that title is memeable that they went for such a profound idea they went with\n",
      "[35:25.840 --> 35:30.560]  the I don't think anyone used that kind of title before right attention is all you need yeah it's\n",
      "[35:30.560 --> 35:37.280]  like a meme or something yeah it's not funny that one like uh maybe if it was a more serious\n",
      "[35:37.280 --> 35:41.440]  title yeah I don't have the impact honestly I yeah there is an element of me that honestly\n",
      "[35:41.440 --> 35:48.080]  agrees with you and prefers it this way yes if it was too grand it would overpromise and then\n",
      "[35:48.080 --> 35:54.320]  underdeveloper potentially so you want to just uh meme your way to greatness that should be a t-shirt\n",
      "[35:54.320 --> 35:59.920]  so you you tweeted the transformers a magnificent neural network architecture because it is a general\n",
      "[35:59.920 --> 36:06.240]  purpose differentiable computer it is simultaneously expressive in the forward pass optimizable via\n",
      "[36:06.240 --> 36:13.040]  brap back propagation gradient descent and efficient high parallelism compute graph can you discuss\n",
      "[36:13.040 --> 36:19.520]  some of those details expressed of optimizable efficient yeah for memory or or in general whatever\n",
      "[36:19.520 --> 36:23.280]  comes to your heart you want to have a general purpose computer that you can train on arbitrary\n",
      "[36:23.280 --> 36:28.160]  problems uh like say the task of next work prediction or detecting if there's a cat in a image or\n",
      "[36:28.160 --> 36:32.800]  something like that and you want to train this computer so you want to set its its weights and I\n",
      "[36:32.800 --> 36:37.680]  think there's a number of design criteria that sort of overlap in the transformer simultaneously\n",
      "[36:37.680 --> 36:42.080]  that made it very successful and I think the authors were kind of uh deliberately trying to\n",
      "[36:42.800 --> 36:50.320]  make this really powerful architecture and um so in a basically it's very powerful in the forward\n",
      "[36:50.320 --> 36:56.720]  pass because it's able to express um very uh general computation as sort of something that looks like\n",
      "[36:56.720 --> 37:01.760]  message passing uh you have nodes and the old store vectors and uh these nodes get to basically look\n",
      "[37:01.760 --> 37:07.440]  at each other and it's uh each other's vectors and they get to communicate and basically nodes get to\n",
      "[37:08.000 --> 37:11.680]  broadcast hey I'm looking for certain things and then other nodes get to broadcast hey these are\n",
      "[37:11.680 --> 37:15.600]  different things I have those are the keys in the values so it's not just attention yeah exactly\n",
      "[37:15.600 --> 37:19.440]  transformer is much more than just the attention component that's got many pieces architectural that\n",
      "[37:19.440 --> 37:23.600]  went into it the residual connection of the weights arranged there's uh multi layer perceptron and\n",
      "[37:23.600 --> 37:28.880]  there the weights a stacked and so on um but basically there's a message passing scheme where nodes\n",
      "[37:28.880 --> 37:34.480]  get to look at each other decide what's interesting and then update each other and uh so I think the um\n",
      "[37:34.480 --> 37:38.560]  when you get to the details of it I think it's a very expressive function uh so it can express\n",
      "[37:38.560 --> 37:42.480]  lots of different types of algorithms in forward pass not only that but the weights designed\n",
      "[37:42.480 --> 37:46.320]  with the residual connections, layer normalizations, the softmax attention and everything\n",
      "[37:46.320 --> 37:50.880]  it's also optimizable this is a really big deal because there's lots of computers there are\n",
      "[37:50.880 --> 37:55.040]  powerful that you can't optimize um or they're not easy to optimize using the techniques that we have\n",
      "[37:55.040 --> 37:58.880]  which is back propagation and grading and send these are first order methods very simple optimizers\n",
      "[37:58.880 --> 38:05.920]  really and so um you also needed to be optimizable um and then lastly you wanted to run efficiently in\n",
      "[38:05.920 --> 38:12.880]  our hardware our hardware is a massive throughput machine like GPUs they prefer lots of parallelism\n",
      "[38:12.880 --> 38:16.160]  so you don't want to do lots of sequential operations you want to do a lot of operations\n",
      "[38:16.160 --> 38:20.800]  seriously and the transformer is designed with that in mind as well and so it's designed for our\n",
      "[38:20.800 --> 38:25.040]  hardware and is designed to both be very expressive in a forward pass but also very optimizable\n",
      "[38:25.040 --> 38:30.480]  in the backward pass and you said that uh the residual connection support a kind of ability to\n",
      "[38:30.960 --> 38:36.720]  learn short algorithms fast and first and then gradually extend them uh longer during training\n",
      "[38:36.720 --> 38:41.520]  yeah what's what's the idea of learning short algorithms right think of it as uh so basically\n",
      "[38:41.520 --> 38:47.280]  a transformer is a uh series of uh blocks right and these blocks have attention and a little\n",
      "[38:47.280 --> 38:52.240]  multi-layer perceptron and so you you go off into a block and you come back to this residual pathway\n",
      "[38:52.240 --> 38:55.440]  and then you go off and you come back and then you have a number of layers arranged sequentially\n",
      "[38:55.920 --> 39:00.160]  and so the way to look at it I think is uh because of the residual pathway in the backward\n",
      "[39:00.160 --> 39:06.320]  pass the gradients sort of flow allowing it uninterrupted because addition uh distributes the\n",
      "[39:06.320 --> 39:10.960]  gradient equally to all of its branches so the gradient from the supervision at the top uh just\n",
      "[39:10.960 --> 39:16.480]  floats directly to the uh first layer and the all the residual connections are arranged so that\n",
      "[39:16.480 --> 39:19.920]  in the beginning at doing initialization they contribute nothing to the residual pathway\n",
      "[39:20.640 --> 39:25.680]  um so what it kind of looks like is imagine the transformer is kind of like a uh python\n",
      "[39:26.320 --> 39:32.640]  function like a death and um you get to do various kinds of like lines of code uh say you have a\n",
      "[39:32.640 --> 39:37.760]  hundred layers deep uh transformer typically they would be much shorter say 20 so if 20 lines of\n",
      "[39:37.760 --> 39:41.440]  code then you can do something in them and so think of during the optimization basically what it\n",
      "[39:41.440 --> 39:44.960]  looks like is first you optimize the first line of code and then the second line of code can kick in\n",
      "[39:44.960 --> 39:48.880]  and the third line of code can kick in and I kind of uh feel like because of the residual pathway and\n",
      "[39:48.880 --> 39:53.280]  the dynamics of the optimization uh you can sort of learn a very short algorithm that gets the\n",
      "[39:53.280 --> 39:57.600]  approximate answer but then the other layers can sort of kick in and start to create a contribution\n",
      "[39:57.600 --> 40:01.600]  and at the end of it you're you're optimizing over an algorithm that is uh 20 lines of code\n",
      "[40:02.400 --> 40:05.680]  except these lines of code are very complex because this is an entire block of a transformer\n",
      "[40:05.680 --> 40:09.680]  you can do a lot in there well it's really interesting is that this transformer architecture actually\n",
      "[40:09.680 --> 40:14.240]  has been a remarkably resilient basically a transformer that came out in 2016 is the transformer\n",
      "[40:14.240 --> 40:18.960]  you would use today except you reshuffle some delayer norms uh the delayer normalizations have been\n",
      "[40:18.960 --> 40:24.480]  reshuffle to a pre-norm um formulation and so it's been remarkably stable but there's a lot of\n",
      "[40:24.480 --> 40:28.800]  bells and whistles that people have attached on it and try to improve it I do think that basically\n",
      "[40:28.800 --> 40:33.600]  it's a it's a big step in simultaneously optimizing for lots of properties of a desirable neural network\n",
      "[40:33.600 --> 40:37.440]  architecture and I think that people have been trying to change it but it's proven remarkably resilient\n",
      "[40:38.480 --> 40:43.200]  but I do think that there should be even better architectures potentially but it's uh you\n",
      "[40:43.200 --> 40:48.160]  you admire the resilience here yeah there's something profound about this architecture that\n",
      "[40:48.240 --> 40:53.920]  at least was it so maybe we can everything can be turned into uh into a problem that\n",
      "[40:53.920 --> 40:58.160]  transformers can solve currently definitely looks like the transformers taking over AI and you can\n",
      "[40:58.160 --> 41:02.560]  feed basically arbitrary problems into it and it's a general the franchiseable computer and it's\n",
      "[41:02.560 --> 41:08.800]  extremely powerful and uh this conversions in AI has been really interesting to watch uh from\n",
      "[41:08.800 --> 41:13.360]  me personally what else do you think could be discovered here about transformers like what's the\n",
      "[41:13.360 --> 41:19.600]  surprising thing or is it a stable um I want a stable place is there something interesting\n",
      "[41:19.600 --> 41:25.520]  we might discover about transformers like aha moments maybe has to do with memory um maybe\n",
      "[41:25.520 --> 41:31.280]  knowledge representation like that stuff um definitely the zeitgeist today is just pushing like\n",
      "[41:31.280 --> 41:35.920]  basically right now the zeitgeist is do not touch the transformer touch everything else yes so\n",
      "[41:35.920 --> 41:39.360]  people are scaling up the datasets making them much much bigger they're working on the evaluation\n",
      "[41:39.440 --> 41:45.760]  making the evaluation much much bigger and uh they're basically keeping the architecture unchanged and\n",
      "[41:45.760 --> 41:51.920]  that's how we've uh that's the last five years of progress in AI kind of what do you think about\n",
      "[41:51.920 --> 42:00.320]  one flavor of it which is language models have you been surprised uh has your sort of imagination been\n",
      "[42:00.320 --> 42:06.640]  captivated by you mentioned dpt in all the bigger and bigger and bigger language models and uh\n",
      "[42:06.640 --> 42:14.320]  what are the limits of those models do you think so just the task of natural language\n",
      "[42:15.840 --> 42:20.080]  basically the way gpt is trained right as you just download a massive amount of uh text data from\n",
      "[42:20.080 --> 42:24.720]  the internet and you try to predict the next uh word in a sequence roughly speaking uh you're\n",
      "[42:24.720 --> 42:29.920]  predicting a little work chunks uh but uh roughly speaking that's it um and what's been really\n",
      "[42:29.920 --> 42:34.400]  interesting to watch is uh basically it's a language model language models have actually existed for\n",
      "[42:34.400 --> 42:40.240]  a very long time um there's papers on language modeling from 2003 even earlier can you explain\n",
      "[42:40.240 --> 42:46.080]  that case what a language model is uh yeah so language model just uh basically the rough idea is um\n",
      "[42:46.080 --> 42:50.720]  just predicting the next uh word in a sequence roughly speaking uh so there's a paper from for\n",
      "[42:50.720 --> 42:57.200]  example uh bengio uh and the team from 2003 where for the first time they were using a neural network\n",
      "[42:57.200 --> 43:02.640]  to take say like three or five words and predict the um next word and they're doing this on much\n",
      "[43:02.640 --> 43:07.280]  smaller datasets and the neural net is not a transformer it's a multi-layer perceptron but\n",
      "[43:07.280 --> 43:10.880]  but it's the first time that a neural network has been applied in that setting but even before\n",
      "[43:10.880 --> 43:17.200]  neural networks there were um language models except they were using um n-gram models so n-gram\n",
      "[43:17.200 --> 43:23.440]  models are just uh count-based models so um if you tried if you tried to take two words and predict\n",
      "[43:23.440 --> 43:28.880]  the third one you just count up how many times you've seen any uh two word combinations and what\n",
      "[43:28.880 --> 43:33.280]  came next and what you predict as coming next is just what you've seen the most of in the training set\n",
      "[43:34.080 --> 43:37.920]  and so uh language modeling has been around for a long time neural networks have done language\n",
      "[43:37.920 --> 43:43.840]  modeling for a long time so really what's uh new or interesting or exciting is just realizing that\n",
      "[43:43.840 --> 43:49.680]  when you scale it up uh with powerful enough neural net transformer you have all these emergent\n",
      "[43:49.680 --> 43:55.600]  properties where uh basically what happens is if you have a large enough dataset of text\n",
      "[43:56.080 --> 44:02.080]  you are in the task of predicting the next uh word you are multitasking a huge amount of\n",
      "[44:02.720 --> 44:08.560]  different kinds of problems you are multitasking understanding of you know chemistry physics\n",
      "[44:08.560 --> 44:12.880]  human nature lots of things are sort of clustered in that objective it's a very simple objective\n",
      "[44:12.880 --> 44:17.120]  but actually you have to understand a lot about the world to make that prediction you just said\n",
      "[44:17.120 --> 44:23.920]  the you word understanding uh are you in terms of chemistry and physics and so on what do you\n",
      "[44:23.920 --> 44:29.760]  feel like it's doing is it's searching for the right context uh in in like what is it what is the\n",
      "[44:29.760 --> 44:35.280]  actual process happening here yeah so basically it gets a thousand words and is trying to predict a\n",
      "[44:35.280 --> 44:40.480]  thousand at first and uh in order to do that very very well over the entire dataset available in the\n",
      "[44:40.480 --> 44:46.400]  internet you actually have to basically kind of understand the context of of what's going on in there\n",
      "[44:46.400 --> 44:53.680]  yeah um and uh it's a sufficiently hard problem that you uh if you have a powerful enough computer\n",
      "[44:53.680 --> 45:00.080]  like a transformer you end up with uh interesting solutions and uh you can ask it uh to do all kinds\n",
      "[45:00.080 --> 45:06.240]  of things and um it it it shows a lot of uh emergent properties like in context learning that was the\n",
      "[45:06.240 --> 45:11.680]  big deal with GPD and the original paper when they published it is that you can just sort of uh prompt\n",
      "[45:11.680 --> 45:15.280]  it in various ways and ask it to do various things and it will just kind of complete the sentence but\n",
      "[45:15.280 --> 45:19.920]  in the process of just completing the sentence it's actually solving all kinds of really uh interesting\n",
      "[45:19.920 --> 45:25.200]  problems that we care about do you think is doing something like understanding like um and when\n",
      "[45:25.200 --> 45:32.160]  we use the word understanding for us humans i think is doing some understanding it in its weights it\n",
      "[45:32.160 --> 45:36.880]  understands i think a lot about the world and it has to in order to predict the next word in the\n",
      "[45:36.880 --> 45:43.920]  sequence so it's trained on the data from the internet uh what do you think about this this\n",
      "[45:43.920 --> 45:49.520]  approach in terms of datasets of using data from the internet do you think the internet has enough\n",
      "[45:49.520 --> 45:55.600]  structured data to teach AI about human civilization yes so i think the internet as a huge amount of\n",
      "[45:55.600 --> 46:01.440]  data i'm not sure if it's a complete enough set i don't know that uh text is enough for having a\n",
      "[46:01.440 --> 46:07.280]  sufficiently powerful aji as an outcome um of course there is audio and video and images and all\n",
      "[46:07.280 --> 46:11.120]  that kind of stuff yeah so text by itself i'm a little bit suspicious about there's a ton of\n",
      "[46:11.120 --> 46:15.200]  things we don't put in text in writing uh just because they're obvious to us about how the world\n",
      "[46:15.200 --> 46:18.960]  works and the physics of it and the things fall we don't put that stuff in text because why would you\n",
      "[46:18.960 --> 46:23.680]  we share that understanding and so text is a communication medium between humans and it's not a\n",
      "[46:24.480 --> 46:29.520]  all-encompassing medium of knowledge about the world but as you pointed out we do have video\n",
      "[46:29.520 --> 46:34.400]  and we have images and we have audio and so i think that that definitely helps a lot but we haven't\n",
      "[46:34.400 --> 46:39.920]  trained models sufficiently uh across both across all those modalities yet uh so i think that's\n",
      "[46:39.920 --> 46:44.080]  what a lot of people are interested in but i wonder what that shared understanding of like what we\n",
      "[46:44.080 --> 46:50.960]  might call common sense has to be learned inferred in order to complete the sentence correctly\n",
      "[46:51.600 --> 46:57.360]  so maybe the fact that it's implied on the internet the model's gonna have to learn that\n",
      "[46:58.000 --> 47:04.720]  not by reading about it by inferring it in the representation so like common sense just like we\n",
      "[47:04.720 --> 47:11.680]  i don't think we learned common sense like nobody says tells us explicitly we just figure it all out\n",
      "[47:11.680 --> 47:16.880]  by interacting with the world right and so here's a model reading about the way people interact\n",
      "[47:16.880 --> 47:23.600]  with the world it might have to infer that i wonder yeah uh you you briefly worked in a project called\n",
      "[47:23.600 --> 47:30.800]  the world of bits training an r rl system to take actions on the internet um versus just consuming\n",
      "[47:30.800 --> 47:34.880]  the internet like you talked about do you think there's a future for that kind of system interacting\n",
      "[47:34.880 --> 47:39.600]  with the internet to help the learning yes i think that's probably the uh the final frontier for a\n",
      "[47:39.600 --> 47:45.200]  lot of these models uh because um so as you mentioned i was at opening i i was working on this project\n",
      "[47:45.200 --> 47:48.960]  for all the bits and basically it was the idea of giving neural networks access to a keyboard and\n",
      "[47:48.960 --> 47:57.600]  an ms and the idea possibly go wrong so basically you um you perceive the input of the uh screen\n",
      "[47:57.600 --> 48:04.160]  pixels and uh basically the state of the computer is sort of visualized uh for human consumption in\n",
      "[48:04.160 --> 48:08.560]  images of the web browser and stuff like that and then you give the neural or the ability to press\n",
      "[48:08.560 --> 48:12.640]  keyboards and use the mouse and we were trying to get it to for example complete bookings and\n",
      "[48:12.640 --> 48:17.680]  you know interact with user interfaces and um would you learn from that experience like what was\n",
      "[48:17.680 --> 48:24.880]  some fun stuff this is super cool idea yeah i mean it's like uh yeah i mean the the step between\n",
      "[48:24.880 --> 48:30.720]  observer to actor yeah is a super fascinating step yeah well it's the universal interface in the\n",
      "[48:30.720 --> 48:35.520]  digital realm i would say and there's a universal interface in like the physical realm which in my\n",
      "[48:35.520 --> 48:40.320]  mind is a humanoid form factor kind of thing uh we can later talk about optimists and so on but\n",
      "[48:40.320 --> 48:45.760]  i feel like there's a uh they're kind of like uh similar philosophy in some way where the human\n",
      "[48:45.760 --> 48:50.000]  the world the physical world is designed for the human form and the digital world is designed for\n",
      "[48:50.000 --> 48:54.960]  the human form of seeing the screen and using keyword not keyboard and mouse and so it's the\n",
      "[48:54.960 --> 49:00.320]  universal universal interface that can uh basically uh command the digital infrastructure we've built up\n",
      "[49:00.320 --> 49:06.000]  for ourselves and so it feels like a very powerful interface to to command and to build on top of\n",
      "[49:06.560 --> 49:10.720]  uh now to your question is to like what i learned from that it's interesting because the world of\n",
      "[49:10.720 --> 49:17.680]  bits was basically uh too early i think at open AI at the time um this is around 2015 or so\n",
      "[49:18.320 --> 49:23.760]  and the zeitgeist at that time was very different in AI from the zeitgeist today at the time\n",
      "[49:23.760 --> 49:28.240]  everyone was super excited about reinforcement learning from scratch uh this is the time of the\n",
      "[49:28.320 --> 49:33.680]  autoreapaper uh where uh neural networks were playing autore games um and beating humans in some cases\n",
      "[49:34.560 --> 49:38.560]  alpha go and so on so everyone was very excited about training training neural networks from scratch\n",
      "[49:38.560 --> 49:44.000]  using reinforcement learning um directly it turns out that reinforcement learning is extremely\n",
      "[49:44.000 --> 49:47.760]  an efficient way of training neural networks because you're taking all these actions and all\n",
      "[49:47.760 --> 49:52.640]  these observations and you get some sparse rewards once in a while so you do all this stuff based\n",
      "[49:52.640 --> 49:57.280]  on all these inputs and once in a while you're like told you did a good thing you did a bad thing\n",
      "[49:57.280 --> 50:00.880]  and it's just an extremely hard problem you can't learn from that uh you can burn a forest\n",
      "[50:01.520 --> 50:05.760]  and you can sort of brute force through it and we saw that i think with uh you know with uh go and\n",
      "[50:05.760 --> 50:11.280]  doda and so on and does work uh but it's extremely inefficient uh and uh not how you want to approach\n",
      "[50:11.280 --> 50:16.000]  problems uh practically speaking and so that's the approach that at the time we also took to world\n",
      "[50:16.000 --> 50:21.520]  of bits uh we would uh have an agent initialize randomly so with keyboard mash and mouse mash and\n",
      "[50:21.520 --> 50:27.440]  try to make a booking and it's just like revealed the insanity of that approach very quickly where\n",
      "[50:27.440 --> 50:31.920]  you have to stumble by the correct booking in order to get a reward of you did it correctly and you're\n",
      "[50:31.920 --> 50:37.360]  never gonna stumble by it by chance at random so even with a simple web interface there's too many\n",
      "[50:37.360 --> 50:42.800]  options there's just too many options uh and uh it's two sparse reward signal and you're starting\n",
      "[50:42.800 --> 50:46.400]  from scratch at the time and so you don't know how to read you don't understand pictures images\n",
      "[50:46.400 --> 50:51.360]  buttons you don't understand what it means to like make a booking but now what's happened is uh it\n",
      "[50:51.360 --> 50:56.400]  is time to revisit that and opening eyes interested in this uh companies like adept are interested\n",
      "[50:56.400 --> 51:01.600]  in this and so on and uh the idea is coming back uh because the interface is very powerful but now\n",
      "[51:01.600 --> 51:06.320]  you're not training an agent from scratch you are taking the GPT as an initialization so GPT is\n",
      "[51:06.320 --> 51:13.120]  pre-trained on all of text and it understands what's a booking it understands what's a submit\n",
      "[51:13.200 --> 51:17.760]  it understands um quite a bit more and so it already has those representations they are very\n",
      "[51:17.760 --> 51:23.280]  powerful and that makes all the training significantly more efficient um and makes the problem tractable\n",
      "[51:23.280 --> 51:28.480]  should the interaction be with like the way humans see it with the buttons and the language or\n",
      "[51:28.480 --> 51:34.160]  should be with the HTML JavaScript and the CSS yeah what's what do you think is the better uh\n",
      "[51:34.160 --> 51:38.560]  today all of this interaction is mostly on the level of HTML CSS and so on that's done up because of\n",
      "[51:38.560 --> 51:43.840]  computational constraints uh but I think ultimately um uh everything is designed for human\n",
      "[51:43.840 --> 51:47.840]  visual consumption and so at the end of the day there's all the additional information is in\n",
      "[51:48.640 --> 51:52.560]  the layout of the web page and what's next to you and what's our red background and all this kind\n",
      "[51:52.560 --> 51:56.720]  of stuff and what it looks like visually so I think that's the final frontier as we are taking in\n",
      "[51:56.720 --> 52:01.760]  pixels and we're giving out keyboard mouse commands uh but I think it's impractical still today\n",
      "[52:01.760 --> 52:07.440]  do you worry about bots on the internet given given these ideas given how exciting they are\n",
      "[52:07.440 --> 52:12.800]  do worry about bots on twitter being not the stupid bots that we see now with the crypto bots\n",
      "[52:12.800 --> 52:17.920]  but the bots that might be out there actually that we don't see that they're interacting in interesting\n",
      "[52:17.920 --> 52:24.080]  ways so this kind of system feels like it should be able to pass the I'm not a robot click button\n",
      "[52:24.080 --> 52:30.320]  whatever um which do you actually understand how that test works I don't quite like uh there's\n",
      "[52:30.720 --> 52:35.600]  there's a check box or whatever that you click it's presumably tracking oh I see\n",
      "[52:36.400 --> 52:42.240]  like mouse movement and the timing and so on yeah so exactly this kind of system we're talking about\n",
      "[52:42.240 --> 52:50.480]  should be able to pass that so yeah what do you feel about um bots that are language models plus\n",
      "[52:50.480 --> 52:55.680]  have some interactability and are able to tweet and reply and so on do you worry about that world\n",
      "[52:55.760 --> 53:01.120]  uh yeah I think it's always been a bit of an arms race uh between sort of the attack and the\n",
      "[53:01.120 --> 53:05.840]  defense uh so the attack will get stronger but the defense will get stronger as well uh our\n",
      "[53:05.840 --> 53:10.720]  ability to detect that how do you defend how do you detect how do you know that your\n",
      "[53:10.720 --> 53:17.520]  a kapate account on twitter is as human how would you approach that like if people were claim you\n",
      "[53:17.520 --> 53:24.960]  know uh how would you defend yourself in the court of law that I'm a human um this account is\n",
      "[53:24.960 --> 53:29.840]  yeah at some point I think uh it might be I think the society uh society will evolve a little bit\n",
      "[53:29.840 --> 53:34.720]  like we might start signing digitally signing uh some of our correspondence or you know things that\n",
      "[53:34.720 --> 53:40.320]  we create uh right now it's not necessary but maybe in the future it might be I do think that we\n",
      "[53:40.320 --> 53:47.280]  are going towards a world where we share we share the digital space with uh ais synthetic beings\n",
      "[53:47.280 --> 53:52.160]  yeah and uh they will get much better and they will share our digital realm and they'll eventually\n",
      "[53:52.160 --> 53:55.920]  share our physical realm as well it's much harder uh but that's kind of like the world we're going\n",
      "[53:55.920 --> 54:00.320]  towards and most of them will be benign and awful and some of them will be malicious and it's going\n",
      "[54:00.320 --> 54:06.800]  to be an arms race trying to detect them so I mean the worst isn't the ais the worst is the ais\n",
      "[54:06.800 --> 54:13.040]  pretending to be human so I don't know if it's always malicious there's obviously a lot of malicious\n",
      "[54:13.040 --> 54:19.680]  applications but yeah it could also be you know if I was an a i I would try very hard to pretend\n",
      "[54:19.680 --> 54:24.400]  to be human because we're in a human world yeah I won't I wouldn't get any respect as an a i\n",
      "[54:24.400 --> 54:28.640]  yeah I want to get some love and respect I don't think the problem is intractable people are\n",
      "[54:28.640 --> 54:33.520]  are thinking about the proof of personhood yes and uh we might start digitally signing our stuff\n",
      "[54:33.520 --> 54:39.040]  and we might all end up having like uh yeah basically some some solution for proof of personhood\n",
      "[54:39.040 --> 54:43.120]  it doesn't seem to me intractable it's just something that we haven't had to do until now but\n",
      "[54:43.120 --> 54:48.000]  I think once the need like really starts to emerge which is soon I think people will think about\n",
      "[54:48.000 --> 54:56.720]  it much more so but that too will be a race because um obviously you can probably uh spoof or fake\n",
      "[54:56.720 --> 55:04.080]  of the the the proof of uh personhood so you have to try to figure out how to probably uh I mean\n",
      "[55:04.080 --> 55:10.240]  it's weird that we have like social security numbers and like passports and stuff it seems like\n",
      "[55:10.240 --> 55:15.360]  it's harder to fake stuff in the physical space but in the digital space it just feels like it's\n",
      "[55:15.360 --> 55:22.560]  going to be very tricky very tricky to out um because it seems to be pretty low cost of fake stuff\n",
      "[55:22.560 --> 55:30.080]  what are you gonna put an AI in jail for like trying to use the fake fake personhood proof you\n",
      "[55:30.080 --> 55:34.720]  you I mean okay fine you'll put a lot of AI in jail but there'll be more AI's arbitrary like\n",
      "[55:34.720 --> 55:41.440]  exponentially more the cost of creating bought is very low uh unless there's some kind of way\n",
      "[55:42.400 --> 55:49.680]  to track accurately like you're not allowed to create any program without showing\n",
      "[55:50.400 --> 55:56.960]  uh tying yourself to that program like you any program that runs in the internet you'll be able to\n",
      "[55:58.160 --> 56:02.800]  trace every single human program and those involved with that program right yeah maybe you have\n",
      "[56:02.800 --> 56:07.200]  to start declaring when uh you know we have to start drawing those boundaries and keeping track of\n",
      "[56:07.200 --> 56:14.400]  okay uh what are digital entities versus human entities and uh what is the ownership of human\n",
      "[56:14.400 --> 56:21.200]  entities and digital entities and uh something like that um I don't know but I think I'm optimistic that\n",
      "[56:21.200 --> 56:27.200]  this is uh this is uh possible and in some in some sense we're currently in like the worst time of\n",
      "[56:27.200 --> 56:32.400]  it because um all these bots suddenly have become very capable uh but we don't have the fences yet\n",
      "[56:32.400 --> 56:36.960]  built up the society and but I think uh there doesn't seem to me intractable it's just something that\n",
      "[56:37.200 --> 56:43.520]  we have to deal with it seems weird that the twitter bot like really crappy twitter bots are so numerous\n",
      "[56:43.520 --> 56:50.400]  like yes is it so I presume that the engineers at twitter are very good so it seems like what I\n",
      "[56:50.400 --> 56:56.640]  would infer from that uh is it seems like a hard problem it they're probably catching all right\n",
      "[56:56.640 --> 57:03.840]  if I were to sort of steal man the case it's a hard problem and there's a huge cost to uh\n",
      "[57:04.800 --> 57:13.840]  false positive to to removing a post by somebody that's not a bot that's a crazy very bad user\n",
      "[57:13.840 --> 57:20.080]  experience so they're very cautious about removing so maybe it's um and maybe the bots are really\n",
      "[57:20.080 --> 57:25.680]  good at learning what gets removed and not such that they can stay ahead of the removal process\n",
      "[57:25.680 --> 57:29.920]  very quickly my impression of it honestly is uh there's a lot of blowing for it I mean\n",
      "[57:30.560 --> 57:35.840]  yeah just that's what I it's not so I'm just my impression of it it's not so but you have\n",
      "[57:36.080 --> 57:42.720]  yeah that's my impression as well but it feels like maybe you're seeing the the tip of the iceberg\n",
      "[57:43.440 --> 57:49.280]  maybe the number of bots isn't like the trillions and you have to like just it's a constant\n",
      "[57:49.280 --> 57:55.920]  assault of bots and yeah you yeah I don't know um I you have to steal man the case because the bots\n",
      "[57:56.080 --> 58:00.560]  I'm seeing a pretty like obvious I could write a few lines of code that catch these bots\n",
      "[58:01.120 --> 58:04.640]  I mean definitely there's a lot of blowing for it but I will say I agree that if you are a\n",
      "[58:04.640 --> 58:09.600]  sophisticated actor you could probably create a pretty good bot right now um you know using tools\n",
      "[58:09.600 --> 58:14.320]  like GPTs because it's a language model you can generate faces that look quite good now\n",
      "[58:15.280 --> 58:20.480]  and you can uh do this at scale and so I think um yeah it's quite plausible and it's going to be\n",
      "[58:20.560 --> 58:27.520]  hard to defend there was a Google engineer that claimed that the lambda was sentient do you think\n",
      "[58:27.520 --> 58:36.080]  there's any inkling of truth to what he felt and more importantly to me at least do you think\n",
      "[58:36.080 --> 58:42.480]  language models will achieve sentience or the illusion of sentience soonish yes to me it's a\n",
      "[58:42.480 --> 58:48.160]  little bit of a canary in a coal mine kind of moment honestly a little bit uh because uh so\n",
      "[58:48.160 --> 58:54.320]  this engineer spoke to like a chatbot at Google and uh we can convince that uh there's\n",
      "[58:54.320 --> 58:58.480]  a bot of sentient you asked it's some existential philosophical questions and it gave like reasonable\n",
      "[58:58.480 --> 59:06.800]  answers and looked real and uh and so on uh so to me it's a uh he was he was uh he wasn't sufficiently\n",
      "[59:06.800 --> 59:15.600]  trying to stress the system I think and uh exposing the truth of it as it is today um but uh I think\n",
      "[59:15.600 --> 59:21.520]  this will be increasingly harder over time uh so uh yeah I think more and more people will basically\n",
      "[59:21.520 --> 59:28.480]  uh become um yeah I think more and more there will be more people like that over time as as this\n",
      "[59:28.480 --> 59:33.840]  gets better like form and emotional connection to to to an AI yeah perfectly plausible in my mind\n",
      "[59:33.840 --> 59:39.760]  I think these aIs are actually quite good at human human uh connection human emotion a ton of text\n",
      "[59:39.760 --> 59:44.560]  on the internet is about humans and connection and love and so on so I think they have a very good\n",
      "[59:44.560 --> 59:50.880]  understanding in some in some sense of of how people speak to each other about this and um they're\n",
      "[59:50.880 --> 59:56.560]  very capable of creating a lot of that kind of text the um there's a lot of like sci-fi from 50s\n",
      "[59:56.560 --> 01:00:01.360]  and 60s that imagined aIs in a very different way they are calculating coal Balkan like machines\n",
      "[01:00:01.360 --> 01:00:07.760]  that's not what we're getting today we're getting pretty emotional aIs that actually uh are very\n",
      "[01:00:07.760 --> 01:00:13.760]  competent and capable of generating you know possible sounding text with respect to all of these topics\n",
      "[01:00:13.760 --> 01:00:18.720]  see I'm really hopeful about AI systems that are like companions that help you grow develop as a human\n",
      "[01:00:18.720 --> 01:00:24.720]  being uh help you maximize long term happiness but I'm also very worried about AI systems that\n",
      "[01:00:24.720 --> 01:00:29.920]  figure out from the internet the humans get attracted to drama and so these would just be like\n",
      "[01:00:29.920 --> 01:00:35.520]  shit talking aIs that just constantly did you hear it like they'll do gossip they'll do uh they'll\n",
      "[01:00:35.520 --> 01:00:43.040]  try to plant seeds of suspicion to like other humans that you love and trust and uh just kind of\n",
      "[01:00:43.040 --> 01:00:47.920]  mess with people uh in the you know because because that's going to get a lot of attention to drama\n",
      "[01:00:47.920 --> 01:00:55.280]  maximize drama in on the path to maximizing uh engagement and us humans will feed into that machine\n",
      "[01:00:55.280 --> 01:01:03.520]  yeah and get it'll be a giant drama shit storm uh yeah so I'm worried about that so it's the objective\n",
      "[01:01:03.520 --> 01:01:09.520]  function really defines the way that humans are positioned progresses with AI's in it yeah\n",
      "[01:01:10.160 --> 01:01:14.400]  I think right now at least today they are not sort of it's not correct to really think of them\n",
      "[01:01:14.400 --> 01:01:20.240]  as goal seeking agents that want to do something they have no long term memory or anything they it's\n",
      "[01:01:20.240 --> 01:01:24.800]  literally a good approximation of it is you get a thousand words and you're trying to pretty get a\n",
      "[01:01:24.800 --> 01:01:29.280]  thousand at first and then you continue feeding it in and you are free to prompt it in whatever way\n",
      "[01:01:29.280 --> 01:01:35.280]  you want so in text so you say okay uh you are a psychologist and you are very good and you love\n",
      "[01:01:35.280 --> 01:01:42.080]  humans and uh here's a conversation between you and another human human colon something you something\n",
      "[01:01:42.080 --> 01:01:45.120]  and then it just continues the pattern and suddenly you're having a conversation with the fake\n",
      "[01:01:45.120 --> 01:01:50.000]  psychologist who's like trying to help you and so it's still kind of like an aroma of a tool it is a\n",
      "[01:01:50.720 --> 01:01:55.040]  people can prompt it in arbitrary ways and it can create really incredible text but it doesn't have\n",
      "[01:01:55.040 --> 01:02:00.560]  long-term goals over long periods of time it doesn't try to uh so it doesn't look that way right now\n",
      "[01:02:00.640 --> 01:02:07.520]  but you can do short-term goals that have long-term effects so if my prompting short-term goal is\n",
      "[01:02:07.520 --> 01:02:14.160]  to get on Jakapati to respond to me on Twitter whenever like I think I might that's the goal but\n",
      "[01:02:14.160 --> 01:02:19.760]  you might figure out that talking shit to you it'll be the best in a highly sophisticated interesting\n",
      "[01:02:19.760 --> 01:02:28.480]  way and then you build up a relationship when you respond once and then it like over time it gets\n",
      "[01:02:28.480 --> 01:02:39.040]  to not be sophisticated and just like just talk shit and okay maybe you won't get to Andre but it\n",
      "[01:02:39.040 --> 01:02:45.520]  might get to another celebrity and might get into other big accounts and it'll just so with just\n",
      "[01:02:45.520 --> 01:02:50.640]  that simple goal get them to respond yeah maximize the probability of actual response yeah I mean\n",
      "[01:02:50.640 --> 01:02:56.560]  you could prompt a powerful model like this with their it's opinion about how to do any possible\n",
      "[01:02:56.560 --> 01:03:00.800]  thing you're interested in so they will check us they're kind of on track to become these oracles\n",
      "[01:03:00.800 --> 01:03:05.200]  I could sort of think of it that way they are oracles uh currently is just text but they will\n",
      "[01:03:05.200 --> 01:03:10.000]  have calculators they will access to Google search they will have all kinds of gadgets and gizmos they\n",
      "[01:03:10.000 --> 01:03:16.720]  will be able to operate the internet and find different information and um yeah in some sense\n",
      "[01:03:17.840 --> 01:03:21.040]  that's kind of like currently what it looks like in terms of the development do you think it'll be\n",
      "[01:03:21.040 --> 01:03:28.560]  an improvement eventually over what Google is for access to human knowledge like it'll be a more\n",
      "[01:03:28.560 --> 01:03:32.640]  effective search engine to access human knowledge I think there's definite scope in building a better\n",
      "[01:03:32.640 --> 01:03:37.040]  search engine today and I think Google they have all the tools all the people they have everything\n",
      "[01:03:37.040 --> 01:03:40.960]  they need they have all the puzzle pieces they have people training transformers at scale they have\n",
      "[01:03:40.960 --> 01:03:46.560]  all the data uh it's just not obvious if they are capable as an organization to innovate on their\n",
      "[01:03:46.560 --> 01:03:50.720]  search engine right now and if they don't someone else will there's absolute scope for building a\n",
      "[01:03:50.720 --> 01:03:55.360]  significantly better search engine built on these tools it's so interesting a large company\n",
      "[01:03:56.080 --> 01:04:01.120]  where the search there's already an infrastructure it works as brings out a lot of money so where\n",
      "[01:04:01.760 --> 01:04:07.200]  structurally inside a company is their motivation to pivot yeah to say we're going to build a new\n",
      "[01:04:07.200 --> 01:04:14.320]  search engine yeah that's hard so it's usually going to come from a startup right that's um that would be\n",
      "[01:04:14.960 --> 01:04:21.840]  yeah or some other more competent organization um so uh I don't know so currently for example\n",
      "[01:04:21.840 --> 01:04:28.480]  maybe Bing has another shot at it you know as an exo microsoft ditch as we're talking offline um\n",
      "[01:04:28.480 --> 01:04:33.120]  I mean I definitely it's really interesting because search engines used to be about okay here's\n",
      "[01:04:33.120 --> 01:04:38.800]  some query here's here's here's web pages that look like the stuff that you have but you could\n",
      "[01:04:38.800 --> 01:04:44.880]  just directly go to answer and then have supporting evidence um and these uh these models basically\n",
      "[01:04:44.880 --> 01:04:48.880]  they've read all the texts and they've read all the web pages and so sometimes when you see yourself\n",
      "[01:04:48.880 --> 01:04:53.120]  going over to search results and sort of getting like a sense of like the average answer to whatever\n",
      "[01:04:53.120 --> 01:04:58.640]  you're interested in uh like that just directly comes out you don't have to do that work um so they're\n",
      "[01:04:58.640 --> 01:05:05.520]  kind of like uh yeah I think they have a way to this of distilling all that knowledge into like some\n",
      "[01:05:05.520 --> 01:05:12.080]  level of insight basically do you think of prompting as a kind of teaching and learning like this whole\n",
      "[01:05:12.080 --> 01:05:18.640]  process like another layer you know because maybe that's what humans are where you have that\n",
      "[01:05:18.640 --> 01:05:25.600]  background model and then the world is prompting you yeah exactly I think the way we are programming\n",
      "[01:05:25.600 --> 01:05:30.960]  these computers now like GPDs is is converging to how you program humans I mean how do I program\n",
      "[01:05:31.040 --> 01:05:36.160]  humans via prompt I go to people and I I prompt them to do things I prompt them for\n",
      "[01:05:36.160 --> 01:05:41.040]  information and so natural language prompt is how we program humans and we're starting to program\n",
      "[01:05:41.040 --> 01:05:45.520]  computers directly in that interface it's like pretty remarkable honestly so you've spoken a lot\n",
      "[01:05:45.520 --> 01:05:54.320]  about the idea of software 2.0 um all good ideas become like clichés so quickly like the terms\n",
      "[01:05:54.320 --> 01:06:01.840]  it's it's kind of hilarious um it's like I think M&M wants that like if he gets annoyed by a song\n",
      "[01:06:01.840 --> 01:06:09.680]  he's written very quickly that means it's gonna be a big hit because it's too catchy but uh can\n",
      "[01:06:09.680 --> 01:06:14.160]  you describe this idea and how you're thinking about it has evolved over the months and years since\n",
      "[01:06:14.880 --> 01:06:21.680]  since you coined it yeah yes I had a block post on software 2.0 I think several years ago now um\n",
      "[01:06:22.640 --> 01:06:28.000]  and the reason I wrote that post is because I kept I kind of saw something remarkable happening in\n",
      "[01:06:29.120 --> 01:06:34.080]  like software development and how a lot of code was being transitioned to be written not in\n",
      "[01:06:34.080 --> 01:06:38.560]  sort of like C++ and so on but it's written in the weights of a neural net basically just saying\n",
      "[01:06:38.560 --> 01:06:43.200]  that neural nets are taken over software they're almost software and uh taking more and more\n",
      "[01:06:43.200 --> 01:06:48.720]  more tasks and at the time I think not many people understood uh this uh deeply enough that this\n",
      "[01:06:48.720 --> 01:06:53.680]  is a big deal this is a big transition uh neural networks were seen as one of multiple classification\n",
      "[01:06:53.680 --> 01:06:59.840]  algorithms you might use for your dataset problem on Kaggle like this is not that this is a change in\n",
      "[01:06:59.840 --> 01:07:07.760]  how we program computers and I saw neural nets as uh this is going to take over uh the way we program\n",
      "[01:07:07.760 --> 01:07:11.920]  computers is going to change it's not going to be people writing a software in C++ or something\n",
      "[01:07:11.920 --> 01:07:16.880]  like that and directly programming the software it's going to be accumulating uh training sets and\n",
      "[01:07:16.880 --> 01:07:21.440]  datasets and crafting these objectives by which you train these neural nets and at some point there's\n",
      "[01:07:21.440 --> 01:07:26.480]  going to be a compilation process from the datasets and the objective and the architecture specification\n",
      "[01:07:26.480 --> 01:07:32.480]  into the binary which is really just uh the neural net uh you know weights and the forward pass of the\n",
      "[01:07:32.480 --> 01:07:36.960]  neural net and then you can deploy that binary and so I was talking about that sort of transition\n",
      "[01:07:37.520 --> 01:07:44.160]  and uh that's what the post is about and I saw this sort of play out in a lot of uh fields uh you know\n",
      "[01:07:44.640 --> 01:07:49.280]  out of power being one of them but also just uh simple image classification people thought\n",
      "[01:07:49.280 --> 01:07:54.400]  originally you know in the 80s and so on that they would write the algorithm for detecting a dog in\n",
      "[01:07:54.400 --> 01:07:59.200]  an image and they had all these ideas about how the brain does it and first we detect corners and then\n",
      "[01:07:59.200 --> 01:08:02.480]  we detect lines and then we stitch them up and they were like really going at it they were like\n",
      "[01:08:02.480 --> 01:08:06.720]  thinking about how they're going to write the algorithm and this is not the way you build it\n",
      "[01:08:07.520 --> 01:08:12.480]  um and there was a smooth transition where okay uh first we thought we were going to build everything\n",
      "[01:08:13.120 --> 01:08:18.640]  then we were building the features uh so like hog features and things like that uh that detects\n",
      "[01:08:18.640 --> 01:08:22.560]  these little statistical patterns from image patches and then there was a little bit of uh learning\n",
      "[01:08:22.560 --> 01:08:27.840]  on top of it like a support vector machine or binary classifier uh for cat versus dog and images\n",
      "[01:08:27.840 --> 01:08:33.520]  on top of the features so we wrote the features but we trained the last layer sort of the the\n",
      "[01:08:33.520 --> 01:08:37.600]  classifier and then people are like actually let's not even design the features because we can't\n",
      "[01:08:37.600 --> 01:08:42.080]  honestly we're not very good at it so let's also learn the features and then you end up with basically\n",
      "[01:08:42.080 --> 01:08:46.320]  a convolutional neural net where you're learning most of it you're just specifying the architecture\n",
      "[01:08:46.320 --> 01:08:51.680]  and the architecture has tons of the fill-in blanks which is all the knobs and you let the optimization\n",
      "[01:08:51.680 --> 01:08:57.920]  write most of it and so this transition is happening across the industry everywhere and uh suddenly we\n",
      "[01:08:57.920 --> 01:09:02.480]  end up with a ton of code that is written in neural net weights and i was just pointing out that the\n",
      "[01:09:02.480 --> 01:09:07.840]  analogy is actually pretty strong and we have a lot of developer environments for software 1.0 like we\n",
      "[01:09:08.400 --> 01:09:13.520]  i.e.s how you work with code how you debug code how do you how do you run code how do you maintain\n",
      "[01:09:13.520 --> 01:09:17.600]  code we have github so i was trying to make those analogies in the new realm like what is the github\n",
      "[01:09:17.600 --> 01:09:23.600]  i'll software 2.0 turns out it's something that looks like hugging face right now uh you know and so\n",
      "[01:09:23.600 --> 01:09:28.160]  i think some people took it seriously and built cool companies and uh many people originally\n",
      "[01:09:28.160 --> 01:09:32.640]  attacked the post it actually was not built received when i wrote it and i think maybe it has\n",
      "[01:09:32.640 --> 01:09:36.400]  something to do with the title but the post was not well received and i think more people sort of\n",
      "[01:09:36.400 --> 01:09:42.960]  have been coming around to it over time yeah so you were the director of AI at Tesla where\n",
      "[01:09:42.960 --> 01:09:50.480]  i think this idea was really implemented at scale which is how you have engineering teams doing\n",
      "[01:09:50.480 --> 01:09:57.600]  software 2.0 so can you sort of linger on that idea of i think we're in the really early stages\n",
      "[01:09:57.600 --> 01:10:04.080]  of everything you just said which is like github i.e.s like how do we build engineering teams that\n",
      "[01:10:04.880 --> 01:10:11.680]  that work in software 2.0 systems and and the data collection and the data annotation which is\n",
      "[01:10:12.960 --> 01:10:18.800]  all part of that software 2.0 like what do you think is the task of programming a software 2.0\n",
      "[01:10:18.800 --> 01:10:25.040]  is it debugging in the space of hyper parameters or is it also debugging the space of data\n",
      "[01:10:25.760 --> 01:10:33.280]  yeah the way by which you program the computer and influence its algorithm is not by writing the\n",
      "[01:10:33.280 --> 01:10:37.840]  commands yourself you're changing mostly the data set you're changing the\n",
      "[01:10:38.880 --> 01:10:42.640]  loss functions of like what the neural net is trying to do how is trying to predict things but\n",
      "[01:10:42.640 --> 01:10:49.360]  yeah basically the data sets and the architecture so the neural net and so in the case of the\n",
      "[01:10:49.360 --> 01:10:52.640]  autopilot a lot of the data sets had to do with for example detection of objects and\n",
      "[01:10:52.640 --> 01:10:56.400]  lane line markings and traffic lights and so on so you accumulate massive data sets of\n",
      "[01:10:56.400 --> 01:11:02.800]  here's an example here's the desired label and then here's roughly how the architect here's\n",
      "[01:11:02.800 --> 01:11:06.000]  roughly what the algorithm should look like and that's a completion on your own that so the\n",
      "[01:11:06.000 --> 01:11:10.320]  specification of the architecture is like a hint as to what the algorithm should roughly look like\n",
      "[01:11:10.320 --> 01:11:16.320]  and then the feeling the blanks process of optimization is the training process and then you take\n",
      "[01:11:16.320 --> 01:11:19.920]  your neural net that was trained it gives all the right answers on your data set and you deploy it\n",
      "[01:11:20.880 --> 01:11:26.800]  so there's in that case perhaps at all machine learning cases there's a lot of tasks\n",
      "[01:11:27.120 --> 01:11:34.960]  so is coming up formulating a task like a for a multi-headed neural network is\n",
      "[01:11:34.960 --> 01:11:40.320]  formulating a task part of the programming yeah hurry Marcel how you break down a problem\n",
      "[01:11:40.320 --> 01:11:46.880]  yeah into a set of tasks yeah I'm on high level I would say if you look at the software running\n",
      "[01:11:47.520 --> 01:11:52.560]  in in the autopilot I give a number of talks on this topic I would say originally a lot of it was\n",
      "[01:11:52.560 --> 01:11:59.280]  written in software 1.0 there's imagined lots of C++ right and then gradually there was a tiny\n",
      "[01:11:59.280 --> 01:12:03.920]  neural net that was for example predicting given a single image is there like a traffic light or not\n",
      "[01:12:03.920 --> 01:12:08.640]  or is there a lane line marking or not and this neural net didn't have too much to do in this in\n",
      "[01:12:08.640 --> 01:12:13.520]  the scope of the software it was making tiny predictions on individual image and then the rest of\n",
      "[01:12:13.520 --> 01:12:18.400]  the system stitched it up so okay we're actually we don't have just a single camera with eight cameras\n",
      "[01:12:18.400 --> 01:12:22.000]  we actually have eight cameras over time and so what do you do with these predictions how do you\n",
      "[01:12:22.000 --> 01:12:26.000]  put them together how do you do the fusion of all that information and how do you act on it all of\n",
      "[01:12:26.000 --> 01:12:34.080]  that was written by humans in C++ and then we decided okay we don't actually want to do all of\n",
      "[01:12:34.080 --> 01:12:38.400]  that fusion in C++ code because we're actually not good enough to write that algorithm we want the\n",
      "[01:12:38.400 --> 01:12:43.280]  neural net to write the algorithm and we want to port all of that software into the 2.0 stack\n",
      "[01:12:44.160 --> 01:12:48.880]  and so then we actually had neural net that now take all the eight camera images simultaneously\n",
      "[01:12:48.960 --> 01:12:55.360]  and make predictions for all of that so and actually they don't make predictions in the\n",
      "[01:12:55.360 --> 01:13:00.800]  in the space of images they now make predictions directly in 3d and actually they don't in three\n",
      "[01:13:00.800 --> 01:13:08.240]  dimensions around the car and now actually we don't manually fuse the predictions in 3d over time\n",
      "[01:13:08.240 --> 01:13:13.600]  we don't trust ourselves to write that tracker so actually we give the neural net the information\n",
      "[01:13:13.600 --> 01:13:17.760]  over time so it takes these videos now and makes this predictions and so you're serving just like\n",
      "[01:13:17.760 --> 01:13:21.840]  putting more and more power into the neural net more and more processing and at the end of it the\n",
      "[01:13:21.840 --> 01:13:26.800]  eventual sort of goal is to have most of the software potentially be in the 2.0 land\n",
      "[01:13:28.240 --> 01:13:32.400]  because it works significantly better humans are just not very good at writing software basically\n",
      "[01:13:32.400 --> 01:13:38.400]  so the prediction is happening in this like 4d land with three dimensional world over time\n",
      "[01:13:39.040 --> 01:13:46.480]  how do you do annotation in that world what what have you as just a data annotation whether it's\n",
      "[01:13:46.480 --> 01:13:54.880]  self supervised or manual by humans is is a big part of the software 2.0 world right I would say\n",
      "[01:13:54.880 --> 01:13:59.680]  by far in the industry if you're like talking about the industry and how what is the technology of\n",
      "[01:13:59.680 --> 01:14:04.400]  what we have available everything is supervised learning so you need a data sets of input desired\n",
      "[01:14:04.400 --> 01:14:09.920]  output and you need lots of it and there are three properties of it that you need you need it to be\n",
      "[01:14:09.920 --> 01:14:14.880]  very large you need it to be accurate no mistakes and you need it to be diverse you don't want to\n",
      "[01:14:15.120 --> 01:14:20.240]  just have a lot of correct examples of one thing you need to really cover the space of\n",
      "[01:14:20.240 --> 01:14:24.560]  possibility as much as you can and the more you can cover the space of possible inputs the better the\n",
      "[01:14:24.560 --> 01:14:29.600]  algorithm will work at the end now once you have really good data sets that you're collecting curating\n",
      "[01:14:30.560 --> 01:14:36.320]  and cleaning you can train your neural net on top of that so a lot of the work goes into\n",
      "[01:14:36.320 --> 01:14:40.960]  cleaning those data sets now as you pointed out it's probably it could be the question is how do\n",
      "[01:14:41.040 --> 01:14:47.760]  you achieve a ton of if you want to basically predict and 3d you need data and 3d to back that up\n",
      "[01:14:47.760 --> 01:14:53.360]  so in this video we have eight videos coming from all the cameras of the system and this is what\n",
      "[01:14:53.360 --> 01:14:57.520]  they saw and this is the truth of what actually was around there was this car there was this car\n",
      "[01:14:57.520 --> 01:15:00.960]  this car these are the lane line markings this is geometry of the road there's traffic light\n",
      "[01:15:00.960 --> 01:15:06.240]  in this redimensional position you need the ground truth and so the big question that the team\n",
      "[01:15:06.240 --> 01:15:09.920]  was solving of course is how do you how do you arrive at that ground truth because once you have\n",
      "[01:15:09.920 --> 01:15:14.400]  a million of it and it's large clean and diverse then training a neural net on it works extremely\n",
      "[01:15:14.400 --> 01:15:19.680]  well and you can ship that into the car and so there's many mechanisms by which we collected that\n",
      "[01:15:19.680 --> 01:15:24.320]  a trained data you can always go for a human annotation you can go for simulation as a source of\n",
      "[01:15:24.320 --> 01:15:30.560]  ground truth you can also go for what we call the offline tracker that we spoken about at the AI\n",
      "[01:15:30.560 --> 01:15:35.680]  day and so on which is basically an automatic reconstruction process for taking those videos and\n",
      "[01:15:36.160 --> 01:15:41.200]  recovering the three-dimensional sort of reality of what was around that car so basically think\n",
      "[01:15:41.200 --> 01:15:46.400]  of doing like a three-dimensional reconstruction as an offline thing and then understanding that okay\n",
      "[01:15:46.400 --> 01:15:51.520]  there's 10 seconds of video this is what we saw and therefore here's all the lane lines cars and so on\n",
      "[01:15:51.520 --> 01:15:57.040]  and then once you have that annotation you can train your own nets to imitate it and how difficult\n",
      "[01:15:57.040 --> 01:16:01.680]  is the reconstruction the three reconstruction it's difficult but it can be done so there's so\n",
      "[01:16:01.920 --> 01:16:05.040]  there's overlap between the cameras and you do the reconstruction and there's\n",
      "[01:16:07.040 --> 01:16:10.880]  perhaps there's any inaccuracy so that's called an annotation step.\n",
      "[01:16:11.920 --> 01:16:17.120]  Yes the nice thing about the annotation is that it is fully offline you have infinite time you have\n",
      "[01:16:17.120 --> 01:16:21.760]  a chunk of one minute and you're trying to just offline in a super computer somewhere figure out\n",
      "[01:16:21.760 --> 01:16:25.280]  where were the positions of all the cars all the people and you have your full one minute\n",
      "[01:16:25.280 --> 01:16:28.960]  of video from all the angles and you can run all the neural nets you want and they can be very\n",
      "[01:16:28.960 --> 01:16:33.680]  efficient massive neural nets there can be neural nets that can't even run in the car later at\n",
      "[01:16:33.680 --> 01:16:37.680]  test time so they can be even more powerful neural nets than what you can eventually deploy\n",
      "[01:16:37.680 --> 01:16:41.840]  so you can do anything you want three-dimensional reconstruction neural nets anything you want\n",
      "[01:16:41.840 --> 01:16:47.280]  just to recover that truth and then you supervise that truth. What have you learned you said no mistakes\n",
      "[01:16:47.280 --> 01:16:55.440]  about humans doing annotation because I assume humans that there's like a range of things they're\n",
      "[01:16:55.440 --> 01:17:00.880]  good at in terms of clicking stuff on screen isn't that how interesting is that you have a problem\n",
      "[01:17:00.880 --> 01:17:07.360]  of designing an annotator where humans are accurate and do it like what are the even the metrics\n",
      "[01:17:07.360 --> 01:17:12.400]  are efficient or productive all that kind of stuff. Yeah so I grew the annotation team at Tesla\n",
      "[01:17:12.400 --> 01:17:18.640]  from basically zero to a thousand while I was there that was really interesting you know my background\n",
      "[01:17:18.640 --> 01:17:24.480]  is a PhD student researcher so growing that common organization was pretty crazy but\n",
      "[01:17:25.200 --> 01:17:29.280]  yeah I think it's extremely interesting and part of the design process very much behind the\n",
      "[01:17:29.280 --> 01:17:34.080]  autopilot as to where you use humans humans are very good at certain kinds of annotations they're\n",
      "[01:17:34.080 --> 01:17:37.600]  very good for example at two-dimensional annotations of images they're not good at annotating\n",
      "[01:17:38.800 --> 01:17:43.920]  cars over time in three-dimensional space very very hard and so that's why we were very careful\n",
      "[01:17:43.920 --> 01:17:48.160]  to design the tasks that are easy to do for humans versus things that should be left to the off-line\n",
      "[01:17:48.160 --> 01:17:52.240]  tracker like maybe the maybe the computer will do older triangulation in three-degree construction\n",
      "[01:17:52.240 --> 01:17:57.600]  but the human will say exactly these pixels of the image are car exactly these pixels are human\n",
      "[01:17:57.600 --> 01:18:01.680]  and so co-designing the the data annotation pipeline was very much\n",
      "[01:18:02.400 --> 01:18:06.640]  brand and butter was what I was doing daily. Do you think there's still a lot of open problems in that\n",
      "[01:18:06.640 --> 01:18:14.560]  space? Just in general annotation where the stuff the machines are good at machines do and the\n",
      "[01:18:14.560 --> 01:18:20.240]  humans do what they're good at and there's maybe some iterative process. Right. I think to a very\n",
      "[01:18:20.240 --> 01:18:24.480]  large extent we went through a number of iterations and we learned a ton about how to create these\n",
      "[01:18:24.480 --> 01:18:30.480]  data sets. I'm not seeing big open problems like originally when I joined I was like I was really\n",
      "[01:18:31.200 --> 01:18:35.200]  not sure how this would turn out. Yeah but by the time I left I was much more secure and\n",
      "[01:18:35.760 --> 01:18:39.120]  actually we sort of understand the philosophy of how to create these data sets and I was pretty\n",
      "[01:18:39.120 --> 01:18:45.920]  comfortable with where that was at the time. So what are strengths and limitations of cameras for\n",
      "[01:18:45.920 --> 01:18:51.200]  the driving task? In your understanding when you formulate the driving task as a vision task with\n",
      "[01:18:51.200 --> 01:18:56.960]  eight cameras you've seen that the entire you know most of the history of the computer vision field\n",
      "[01:18:56.960 --> 01:19:01.200]  when it has to do with you on that works what just if you step back what are the strengths and\n",
      "[01:19:01.200 --> 01:19:09.040]  limitations of pixels of using pixels to drive. Yeah pixels I think are a beautiful sensory beautiful\n",
      "[01:19:09.040 --> 01:19:13.840]  sensor I would say. The things like cameras are very very cheap and they provide a ton of information\n",
      "[01:19:13.840 --> 01:19:20.080]  ton of bits. So it's extremely cheap sensor for a ton of bits and each one of these bits as a\n",
      "[01:19:20.080 --> 01:19:26.480]  constraint on the state of the world and so you get lots of megapixel images very cheap and it\n",
      "[01:19:26.480 --> 01:19:30.720]  just gives you all these constraints for understanding what's actually out there in the world. So vision\n",
      "[01:19:30.720 --> 01:19:39.440]  is probably the highest bandwidth sensor. It's a very high bandwidth sensor and I love that pixels\n",
      "[01:19:40.000 --> 01:19:48.560]  is a is a constraint on the world. It's this highly complex high bandwidth constraint on the\n",
      "[01:19:48.560 --> 01:19:53.360]  world on the state of the world. It's not just that but again this real real importance of\n",
      "[01:19:54.080 --> 01:20:00.400]  it's the sensor that humans use therefore everything is designed for that sensor. Yeah. The text\n",
      "[01:20:00.400 --> 01:20:07.120]  deriding the flashing signs everything is designed for vision and so you just find it everywhere\n",
      "[01:20:07.120 --> 01:20:11.200]  and so that's why that is the interface you want to be in talking again about these universal\n",
      "[01:20:11.200 --> 01:20:16.000]  interfaces and that's where we actually want to measure the world as well and then develop software\n",
      "[01:20:16.880 --> 01:20:22.800]  for that sensor. But there's other constraints on the state of the world that humans use to\n",
      "[01:20:22.800 --> 01:20:30.000]  understand the world. I mean vision ultimately is the main one but we were like we're like\n",
      "[01:20:30.000 --> 01:20:36.640]  referencing our understanding of human behavior in some common sense physics that could be inferred\n",
      "[01:20:36.640 --> 01:20:42.720]  from vision from from a perception perspective but it feels like we're using some kind of reasoning\n",
      "[01:20:43.760 --> 01:20:48.160]  to predict the world. Yeah. And that's just the pixels. I mean you have a powerful prior\n",
      "[01:20:48.800 --> 01:20:54.720]  so for how the world evolves over time etc. So it's not just about the likelihood term coming up\n",
      "[01:20:54.720 --> 01:20:59.520]  from the data itself telling you about what you are observing but also the prior term of like\n",
      "[01:20:59.520 --> 01:21:04.640]  where the likely things to see and how do they likely move and so on. And the question is how\n",
      "[01:21:04.640 --> 01:21:12.160]  complex is the the range of possibilities that might happen in the driving task.\n",
      "[01:21:12.800 --> 01:21:18.320]  That's still is that to you still an open problem of how difficult is driving like philosophically speaking.\n",
      "[01:21:21.680 --> 01:21:27.280]  All the time you work on driving do you understand how hard driving is? Yeah driving is really hard\n",
      "[01:21:27.920 --> 01:21:31.680]  because it has to do with predictions of all these other agents and the theory of mind and you\n",
      "[01:21:31.680 --> 01:21:35.680]  know what they're going to do and are they looking at you or are they are they looking or are they\n",
      "[01:21:35.680 --> 01:21:41.680]  thinking yeah there's a lot that goes there at the at the full tail off you know the the expansion\n",
      "[01:21:41.680 --> 01:21:46.080]  of the noise that we have to be comfortable with it eventually the final problems are of that form.\n",
      "[01:21:46.080 --> 01:21:50.320]  I don't think those are the problems that are very common. I think eventually they're important\n",
      "[01:21:50.320 --> 01:21:56.080]  but it's like really in the tail end. In the tail end the rare edge cases from the vision\n",
      "[01:21:56.080 --> 01:22:00.240]  perspective what are the toughest parts of the vision problem of driving.\n",
      "[01:22:03.200 --> 01:22:08.160]  Well basically the sensor is extremely powerful but you still need to process that information\n",
      "[01:22:09.440 --> 01:22:14.480]  and so going from brightnesses of these special values to hey here the three-dimensional world\n",
      "[01:22:14.480 --> 01:22:20.400]  is extremely hard and that's what the neural networks are fundamentally doing and so the difficulty\n",
      "[01:22:20.480 --> 01:22:26.240]  really is in just doing an extremely good job of engineering the entire pipeline the entire data\n",
      "[01:22:26.240 --> 01:22:31.760]  engine having the capacity to train these neural nets having the ability to evaluate the system\n",
      "[01:22:31.760 --> 01:22:37.120]  and iterate on it so I would say just doing this in production at scale is like the hard part it's\n",
      "[01:22:37.120 --> 01:22:46.240]  an execution problem. So the data engine but also the deployment of the system such that has low\n",
      "[01:22:46.320 --> 01:22:50.400]  latency performance so it has to do all these steps. Yeah for the neural net specifically just\n",
      "[01:22:50.400 --> 01:22:55.600]  making sure everything fits into the chip on the car and you have a finite budget of flops that\n",
      "[01:22:55.600 --> 01:23:01.040]  you can perform and and memory bandwidth and other constraints and you have to make sure it flies\n",
      "[01:23:01.040 --> 01:23:05.040]  and you can squeeze in as much computer as you can into the tiny. What have you learned from that\n",
      "[01:23:05.040 --> 01:23:10.880]  process because it maybe that's one of the bigger like new things coming from a research background\n",
      "[01:23:11.600 --> 01:23:16.000]  where there's a system that has to run under heavily constrained resources that's\n",
      "[01:23:16.000 --> 01:23:21.920]  to run really fast. What kind of insights have you learned from that? Yeah I'm not sure if it's\n",
      "[01:23:21.920 --> 01:23:27.360]  if there's too many insights you're trying to create a neural net that will fit in what you have\n",
      "[01:23:27.360 --> 01:23:32.000]  available and you're always trying to optimize it and we talked a lot about it on the AI day and\n",
      "[01:23:33.040 --> 01:23:38.320]  basically the triple backflips that the team is doing to make sure it all fits and utilize the\n",
      "[01:23:39.280 --> 01:23:44.160]  so I think it's extremely good engineering and then there's all kinds of little insights\n",
      "[01:23:44.160 --> 01:23:48.640]  peppered in on how to do it properly. Let's actually zoom out because I don't think we talked about\n",
      "[01:23:48.640 --> 01:23:55.680]  the data engine the entirety of the layout of this idea that I think is just beautiful with humans\n",
      "[01:23:55.680 --> 01:24:02.400]  in the loop. Can you describe the data engine? Yeah the data engine is what I call the almost\n",
      "[01:24:02.400 --> 01:24:08.880]  biological feeling like process by which you perfect the training sets for these neural networks.\n",
      "[01:24:10.080 --> 01:24:13.920]  So because most of the programming now is in the level of these data sets and make sure they're\n",
      "[01:24:13.920 --> 01:24:19.600]  large diverse and clean. Basically you have a data set that you think is good. You train your\n",
      "[01:24:19.600 --> 01:24:25.840]  neural net, you deploy it and then you observe how well it's performing and you're trying to always\n",
      "[01:24:25.840 --> 01:24:30.240]  increase the quality of your data set. So you're trying to catch scenarios basically they are\n",
      "[01:24:30.480 --> 01:24:35.200]  basically rare and it is in these scenarios that your own network typically struggle in because\n",
      "[01:24:35.200 --> 01:24:39.600]  they weren't told what to do in those rare cases in the data set but now you can close the loop\n",
      "[01:24:39.600 --> 01:24:44.960]  because if you can now collect all those at scale you can then feed them back into the reconstruction\n",
      "[01:24:44.960 --> 01:24:50.240]  process I described and reconstruct the truth in those cases and add it to the data set. And so the\n",
      "[01:24:50.240 --> 01:24:55.760]  whole thing ends up being like a staircase of improvement of perfecting your training set and you\n",
      "[01:24:55.760 --> 01:25:01.040]  have to go through deployments so that you can mine the parts that are not yet represented well\n",
      "[01:25:01.040 --> 01:25:05.680]  in the data set. So your data set is basically imperfect it needs to be diverse it has pockets\n",
      "[01:25:05.680 --> 01:25:09.440]  there are missing and you need to pat out the pockets you can sort of think of it that way\n",
      "[01:25:10.480 --> 01:25:16.880]  in the data. What role do humans play in this? So what's the this biological system like a human\n",
      "[01:25:16.880 --> 01:25:24.880]  body is made up of cells? What role like how do you optimize the human system? The multiple engineers\n",
      "[01:25:24.880 --> 01:25:32.880]  collaborating figuring out what to focus on what to contribute which tasks to optimize in this\n",
      "[01:25:32.880 --> 01:25:40.480]  neural network? Who is in charge of figuring out which task needs more data? What can you can you\n",
      "[01:25:40.480 --> 01:25:46.400]  speak to the hyperparameters the human system? It really just comes down to extremely good execution\n",
      "[01:25:46.400 --> 01:25:50.240]  from an engineering team who knows what they're doing they understand intuitively the philosophical\n",
      "[01:25:50.240 --> 01:25:55.680]  insights underlying the data engine and the process by which the system improves and how to\n",
      "[01:25:55.680 --> 01:26:00.240]  again like delegate the strategy of the data collection and how that works and then just making\n",
      "[01:26:00.240 --> 01:26:04.160]  sure it's all extremely well executed. And that's where most of the work is is not even the\n",
      "[01:26:04.160 --> 01:26:08.960]  philosophizing or the research or the ideas of it is just extremely good execution is so hard when\n",
      "[01:26:08.960 --> 01:26:13.840]  you're dealing with data at that scale. So your role in the data engine executing well on it\n",
      "[01:26:14.240 --> 01:26:21.280]  is difficult and extremely important is there a priority of like a vision board of saying like\n",
      "[01:26:22.240 --> 01:26:28.560]  we really need to get better at stoplights? Yeah the prioritization of tasks is that essentially\n",
      "[01:26:28.560 --> 01:26:34.160]  and that comes from the data? That comes to the very large extent to what we are trying to achieve in\n",
      "[01:26:34.160 --> 01:26:39.600]  the product roadmap or we're trying to the release we're trying to get out in the feedback from the Q18\n",
      "[01:26:39.600 --> 01:26:43.600]  worth it where the system is struggling or not the things we're trying to improve. And the Q18\n",
      "[01:26:43.600 --> 01:26:50.080]  gives some signal some information in aggregate about the performance of the system in various\n",
      "[01:26:50.080 --> 01:26:54.160]  conditions. And then of course all of us drive it and we can also see it it's really nice to\n",
      "[01:26:54.160 --> 01:26:57.920]  work with a system that you can also experience yourself and you know it drives you home it's\n",
      "[01:26:58.640 --> 01:27:03.360]  is there some insight you can draw from your individual experience that you just can't quite get\n",
      "[01:27:03.360 --> 01:27:10.720]  from an aggregate statistical analysis of data? Yeah it's so weird right? Yes it's not scientific\n",
      "[01:27:10.720 --> 01:27:16.880]  in a sense because you're just one anecdotal sample. Yeah I think there's a ton of it's a source of\n",
      "[01:27:16.880 --> 01:27:21.920]  truth as your interaction with the system and you can see it you can play with it you can perturb it\n",
      "[01:27:21.920 --> 01:27:26.960]  you can get a sense of it you have an intuition for it I think numbers just like have a way of numbers\n",
      "[01:27:26.960 --> 01:27:33.040]  and plots and graphs are you know much harder yeah it hides a lot of it's like if you train a\n",
      "[01:27:33.040 --> 01:27:39.760]  language model it's a really powerful way is by you interacting with it yeah 100 try to build up\n",
      "[01:27:39.760 --> 01:27:45.040]  an intuition yeah I think like Elon also like he always wanted to drive this the system himself\n",
      "[01:27:45.040 --> 01:27:52.240]  he drives a lot and I don't say almost daily so he also sees this as a source of truth you driving\n",
      "[01:27:52.240 --> 01:28:00.880]  the system and it performing and yeah so what do you think tough questions here? So Tesla last\n",
      "[01:28:00.880 --> 01:28:07.280]  year removed radar from from the sensor suite and now just announced there's going to remove all\n",
      "[01:28:07.280 --> 01:28:14.800]  ultrasonic sensors relying solely on vision so camera only does that make the perception probably\n",
      "[01:28:14.800 --> 01:28:20.960]  harder or easier? I would almost reframe the question in some way so the thing is basically\n",
      "[01:28:21.920 --> 01:28:26.160]  you would think that additional sensors by the way can't just interrupt good I wonder if the\n",
      "[01:28:26.160 --> 01:28:31.600]  language model will ever do that if you prompt it let me reframe your question that would be epic\n",
      "[01:28:32.240 --> 01:28:37.200]  this is the wrong prompt sorry it's like a little bit of a wrong question because basically you\n",
      "[01:28:37.200 --> 01:28:43.440]  would think that these sensors are an asset to you yeah but if you fully consider the entire product\n",
      "[01:28:43.440 --> 01:28:49.680]  in its entirety these sensors are actually potentially liability because these sensors aren't free\n",
      "[01:28:49.680 --> 01:28:53.920]  they don't just appear on your car you need suddenly you need to have an entire supply chain you\n",
      "[01:28:53.920 --> 01:28:58.160]  have people procuring it there can be problems with them they may need replacement they are part of\n",
      "[01:28:58.160 --> 01:29:02.640]  the manufacturing process they can hold back the line in production you need to source them you\n",
      "[01:29:02.640 --> 01:29:07.200]  need to maintain them you have to have teams that ride the firmware all of the all of it and then\n",
      "[01:29:07.200 --> 01:29:10.720]  you also have to incorporate them fuse them into the system in some way and so it actually like\n",
      "[01:29:10.720 --> 01:29:17.280]  blotes you're gonna be a lot of it and I think Elon is really good at simplify simplify best part\n",
      "[01:29:17.280 --> 01:29:21.200]  this no part and he always tries to throw away things that are not essential because he understands\n",
      "[01:29:21.200 --> 01:29:27.040]  the entropy in organizations and in the approach and I think in this case the cost is high and you're\n",
      "[01:29:27.040 --> 01:29:30.640]  not potentially seeing it if you're just a computer vision engineer and I'm just trying to improve\n",
      "[01:29:30.640 --> 01:29:35.840]  my network and you know is it more useful or less useful how how useful is it and the thing is\n",
      "[01:29:35.840 --> 01:29:40.480]  if once you consider the full cost of a sensor it actually is potentially a liability and you need\n",
      "[01:29:40.480 --> 01:29:45.280]  to be really sure that it's giving you extremely useful information in this case we looked at\n",
      "[01:29:45.280 --> 01:29:50.640]  using it or not using it and the delta was not massive and so it's not useful is it also\n",
      "[01:29:50.640 --> 01:29:56.800]  blow in the data engine like having more sensors but this is a distraction and these sensors you\n",
      "[01:29:56.800 --> 01:30:00.160]  know they can change over time for example you can have one type of say radar you can have other\n",
      "[01:30:00.160 --> 01:30:03.280]  type of radar they change over time and I suddenly need to worry about it and I'm suddenly\n",
      "[01:30:03.280 --> 01:30:07.520]  have a column in your SQLite telling you oh what sensor type was it and they all have different\n",
      "[01:30:07.520 --> 01:30:13.760]  distributions and then they can they just they contribute noise and entropy into everything and\n",
      "[01:30:13.760 --> 01:30:18.560]  they bloat stuff and also organizationally has been really fascinating to me that it can be very\n",
      "[01:30:18.560 --> 01:30:25.040]  distracting if you if all if you only want to get to work as vision all the resources are on it\n",
      "[01:30:25.040 --> 01:30:29.520]  and you're building out a data engine and you're actually making forward progress because that is the\n",
      "[01:30:29.520 --> 01:30:34.480]  the sensor with the most bandwidth the most constraints on the world and you're investing fully\n",
      "[01:30:34.480 --> 01:30:39.360]  into that and you can make that extremely good if you're you're only a finite amount of sort of spend\n",
      "[01:30:39.440 --> 01:30:46.320]  of focus across different facets of the system and this kind of reminds me of reach\n",
      "[01:30:46.320 --> 01:30:52.480]  sudden it's a bit of less than it just seems like simplifying the system in the long run\n",
      "[01:30:52.480 --> 01:30:56.000]  and of course you know know what the long way it seems to be always the right solution\n",
      "[01:30:56.000 --> 01:31:00.880]  yeah yes in that case it was for RRL but it seems to apply generally across all systems\n",
      "[01:31:00.880 --> 01:31:06.000]  that do computation yeah so where what do you think about the LiDAR as a crutch debate\n",
      "[01:31:06.400 --> 01:31:13.120]  the battle between point clouds and pixels yeah I think this debate is always like slightly\n",
      "[01:31:13.120 --> 01:31:17.360]  confusing to me because it seems like the actual debate should be about like do you have the fleet\n",
      "[01:31:17.360 --> 01:31:21.840]  or not that's like the really important thing about whether you can achieve a really good functioning\n",
      "[01:31:21.840 --> 01:31:27.040]  of an AI system at the scale so data collection systems yeah do you have a fleet or not is\n",
      "[01:31:27.040 --> 01:31:30.640]  significantly more important whether you have LiDAR or not it's just another sensor\n",
      "[01:31:31.120 --> 01:31:37.040]  and yeah I think similar to the radar discussion basically I um\n",
      "[01:31:38.640 --> 01:31:44.800]  yeah I don't think it basically doesn't offer extra extra information it's extremely costly\n",
      "[01:31:44.800 --> 01:31:48.160]  it has all kinds of problems you have to worry about it you have to calibrate it etc it creates\n",
      "[01:31:48.160 --> 01:31:54.000]  bloat and entropy you have to be really sure that you need this this sensor in this case I basically\n",
      "[01:31:54.000 --> 01:31:58.240]  don't think you need it and I think honestly I will make a stronger statement I think the others\n",
      "[01:31:58.320 --> 01:32:01.440]  some of the other companies who are they're using it are probably going to drop it\n",
      "[01:32:02.160 --> 01:32:10.400]  yeah so you have to consider the sensor in the full in considering can you build a big fleet that\n",
      "[01:32:10.400 --> 01:32:16.320]  collects a lot of data and can you integrate that sensor with that data and that sensor into a\n",
      "[01:32:16.320 --> 01:32:21.760]  data engine that's able to quickly find different parts of the data that then continuously\n",
      "[01:32:21.760 --> 01:32:26.560]  improves whatever the model that you're using yeah another way to look at it is like vision is\n",
      "[01:32:26.560 --> 01:32:31.600]  necessary in a sense that the drive the world is designed for human visual consumption so you\n",
      "[01:32:31.600 --> 01:32:37.200]  need vision it's necessary and then also it is sufficient because it has all the information that\n",
      "[01:32:37.200 --> 01:32:41.760]  you that you need for driving and humans obviously is vision to drive so it's both necessary and\n",
      "[01:32:41.760 --> 01:32:45.440]  sufficient so you want to focus resources and you have to be really sure if you're going to bring\n",
      "[01:32:45.440 --> 01:32:50.000]  in other sensors you could you could you could add sensors to infinity at some point you need to\n",
      "[01:32:50.000 --> 01:32:55.600]  draw the line and I think in this case you have to really consider the full cost of any one sensor\n",
      "[01:32:55.600 --> 01:33:00.640]  that you're adopting and do you really need it and I think the answer in this case you know\n",
      "[01:33:00.640 --> 01:33:07.360]  so what do you think about the idea that the other companies are forming high resolution maps and\n",
      "[01:33:07.360 --> 01:33:14.480]  constraining heavily the geographic regions in which they operate is that approach not in your\n",
      "[01:33:14.480 --> 01:33:21.680]  in your view not going to scale over time to the entirety of the United States I think take too long\n",
      "[01:33:21.680 --> 01:33:26.240]  mentioned like they pre map all the environments and they need to refresh the map and they have a\n",
      "[01:33:26.240 --> 01:33:30.960]  perfect centimeter level accuracy map of everywhere they're going to drive it's crazy how are you going\n",
      "[01:33:30.960 --> 01:33:35.680]  to we're talking about the autonomy actually changing the world we're talking about the deployment\n",
      "[01:33:36.480 --> 01:33:41.680]  on the on the global scale autonomous systems for transportation and if you need to maintain a\n",
      "[01:33:41.680 --> 01:33:47.520]  centimeter accurate map for earth or like for many cities and keep them updated it's a huge dependency\n",
      "[01:33:47.520 --> 01:33:52.080]  that you're taking on huge dependency it's a massive massive dependency and now you need to ask\n",
      "[01:33:52.080 --> 01:33:59.120]  yourself do you really need it and humans don't need it right so it's it's very useful to have a\n",
      "[01:33:59.120 --> 01:34:03.360]  low level map of like okay the connectivity of your road you know that there's a fork coming up\n",
      "[01:34:03.360 --> 01:34:06.400]  when you drive an environment you sort of have that high level understanding it's like a small\n",
      "[01:34:06.400 --> 01:34:12.880]  Google map and Tesla uses Google map like similar kind of resolution information in its system\n",
      "[01:34:12.880 --> 01:34:17.040]  but it will not pre map environments to send me to a level accuracy it's a crutch it's a\n",
      "[01:34:17.040 --> 01:34:22.160]  distraction it costs entropy and it diffuses the team it dilutes the team and you're not focusing\n",
      "[01:34:22.160 --> 01:34:28.160]  on what's actually necessary which is the computer version problem what did you learn about machine\n",
      "[01:34:28.160 --> 01:34:35.520]  learning about engineering about life about yourself as one human being from working with Elon Musk\n",
      "[01:34:36.400 --> 01:34:41.360]  I think the most I've learned is about how to sort of run organizations efficiently and how to\n",
      "[01:34:42.160 --> 01:34:46.960]  create efficient organizations and how to fight entropy in an organization so human engineering\n",
      "[01:34:47.760 --> 01:34:52.800]  in the fight against entropy yeah there's a there's a I think Elon is a very efficient warrior\n",
      "[01:34:53.520 --> 01:34:58.240]  in the fight against entropy in organizations what does the entropy in an organization look like\n",
      "[01:34:58.240 --> 01:35:04.240]  exactly it's it's process it's it's process and in it's inefficiencies in the\n",
      "[01:35:04.240 --> 01:35:08.320]  in the machines and that kind of stuff yeah meetings he hates meetings he keeps telling people\n",
      "[01:35:08.320 --> 01:35:14.320]  to skip meetings if they're not useful he basically runs the world's biggest startups I would say\n",
      "[01:35:15.120 --> 01:35:19.760]  Tesla SpaceX are the world's biggest startups Tesla actually is multiple startups I think\n",
      "[01:35:19.760 --> 01:35:26.400]  is better to look at it that way and so I think he's he's extremely good at at that and yeah he's a\n",
      "[01:35:26.400 --> 01:35:31.360]  very good intuition for streamlining processes making everything efficient best part is no part\n",
      "[01:35:31.360 --> 01:35:37.360]  simplifying focusing and just kind of removing barriers moving very quickly making big moves\n",
      "[01:35:38.000 --> 01:35:43.760]  all this is a very start-upy sort of seeming things but at scale so strong drive to simplify\n",
      "[01:35:43.840 --> 01:35:49.760]  from your perspective I mean that that also probably applies to just designing systems and machine\n",
      "[01:35:49.760 --> 01:35:56.000]  learning and otherwise like simplifies yes what do you think is the secret to maintaining the\n",
      "[01:35:56.000 --> 01:36:05.280]  startup culture in a company that grows is there can you introspect that I do think he needs someone\n",
      "[01:36:05.280 --> 01:36:10.720]  in a powerful position with a big hammer like Elon who's like the cheerleader for that idea and\n",
      "[01:36:10.720 --> 01:36:17.040]  ruthlessly ruthlessly pursues it if no one has a big enough hammer everything turns into committees\n",
      "[01:36:17.040 --> 01:36:22.960]  democracy within the company process talking to stakeholders decision-making just everything just\n",
      "[01:36:22.960 --> 01:36:28.320]  crumbles yeah if you have a big person who is also really smart and has a big hammer things move\n",
      "[01:36:28.320 --> 01:36:34.800]  quickly so you said your favorite scene in interstellar is the intense docking scene with the AI\n",
      "[01:36:34.800 --> 01:36:41.600]  and Cooper talking saying Cooper what are you doing docking it's not possible no it's necessary\n",
      "[01:36:42.720 --> 01:36:50.640]  such a good line by the way just so many questions there why in AI in that scene presumably\n",
      "[01:36:50.640 --> 01:36:56.640]  is supposed to be able to compute a lot more than the human it's saying it's not optimal why the\n",
      "[01:36:56.640 --> 01:37:03.440]  human I mean that's a movie but shouldn't the AI know which better than the human anyway what do\n",
      "[01:37:03.440 --> 01:37:11.440]  you think is the value of setting seemingly impossible goals so like are initial intuition which\n",
      "[01:37:11.440 --> 01:37:19.280]  seems like something that you have taken on that Elon espouses that where the initial intuition\n",
      "[01:37:19.280 --> 01:37:24.720]  of the community might say this is very difficult and then you take it on anyway with a crazy deadline\n",
      "[01:37:24.720 --> 01:37:32.960]  you're just from a human engineering perspective have you seen the value of that I wouldn't say that\n",
      "[01:37:32.960 --> 01:37:37.280]  setting impossible goals exactly is is a good idea but I think setting very ambitious goals is a\n",
      "[01:37:37.280 --> 01:37:43.280]  good idea I think there's a what I call sub linear scaling of difficulty which means that 10x\n",
      "[01:37:43.280 --> 01:37:50.960]  problems are not 10x hard usually 10x 10x harder problem is like two or three x harder to execute on\n",
      "[01:37:50.960 --> 01:37:55.040]  because if you want to actually like if you want to improve the system by 10% it costs some\n",
      "[01:37:55.040 --> 01:37:59.680]  amount of work and if you want to 10x improve the system it doesn't cost you know 100x amount\n",
      "[01:38:00.320 --> 01:38:04.320]  and it's because you fundamentally change the approach and if you start with that constraint\n",
      "[01:38:04.320 --> 01:38:08.800]  then some approaches are obviously dumb and not going to work and it forces you to re-evaluate\n",
      "[01:38:09.520 --> 01:38:15.200]  and I think it's a very interesting way of approaching problem solving but it requires us\n",
      "[01:38:15.200 --> 01:38:23.120]  we're kind of thinking just going back to you like PhD days it's like how do you think which ideas\n",
      "[01:38:23.120 --> 01:38:30.640]  in the machine learning community are solvable yes it's it requires what what is that I mean\n",
      "[01:38:30.640 --> 01:38:35.360]  there's the cliche of first principles thinking but like it requires to basically ignore what\n",
      "[01:38:35.360 --> 01:38:41.200]  the community is saying because it doesn't the community doesn't aid community in science usually\n",
      "[01:38:41.200 --> 01:38:47.360]  draw lines of what isn't as impossible right and like it's very hard to break out of that without\n",
      "[01:38:47.360 --> 01:38:52.160]  going crazy yeah I mean I think a good example here is you know the deep learning revolution in some\n",
      "[01:38:52.160 --> 01:38:57.760]  sense because you could be in computer vision at that time during the deep learning sort of\n",
      "[01:38:57.760 --> 01:39:03.280]  revolution of 2012 and so on you could be improving a computer vision stack by 10% or we can\n",
      "[01:39:03.280 --> 01:39:08.080]  just be saying actually all this is useless and how do I do 10x better computer vision well it's\n",
      "[01:39:08.080 --> 01:39:13.200]  not probably by tuning a hog feature detector I need a different approach I need something that\n",
      "[01:39:13.200 --> 01:39:19.600]  is scalable going back to Richard Sutton's and understanding sort of like the philosophy of the\n",
      "[01:39:19.680 --> 01:39:23.680]  bitter lesson and then being like actually I need much more scalable system like in your own\n",
      "[01:39:23.680 --> 01:39:27.840]  network that in principle works and then having some deep believers that can actually\n",
      "[01:39:27.840 --> 01:39:31.360]  execute on that mission make it work so that's the 10x solution\n",
      "[01:39:34.000 --> 01:39:40.480]  what do you think is the timeline to solve the problem of autonomous driving this still in part\n",
      "[01:39:40.480 --> 01:39:46.560]  open question yeah I think the tough thing with timelines of self-driving obviously is that no one\n",
      "[01:39:46.640 --> 01:39:51.680]  has created self-driving yeah so it's not like what do you think is the timeline to build this\n",
      "[01:39:51.680 --> 01:39:56.320]  bridge well we've built million bridges before here's how long that takes it's you know it's\n",
      "[01:39:57.040 --> 01:40:02.800]  no one has built autonomy it's not obvious some parts turn out to be much easier than others\n",
      "[01:40:02.800 --> 01:40:07.600]  it's really hard to forecast you do your best based on trend lines and so on and based on intuition\n",
      "[01:40:07.600 --> 01:40:12.080]  but that's why fundamentally it's just really hard to forecast this no one has to even still like\n",
      "[01:40:12.160 --> 01:40:16.960]  being inside of it it's hard to do yes some things turn out to be much harder and some things\n",
      "[01:40:16.960 --> 01:40:23.440]  turn out to be much easier do you try to avoid making forecasts because like Elon doesn't avoid them\n",
      "[01:40:23.440 --> 01:40:27.760]  right and heads of car companies in the past have not avoided it either\n",
      "[01:40:29.120 --> 01:40:34.880]  Ford and other places have made predictions that we're going to solve a level for driving by 2020\n",
      "[01:40:34.880 --> 01:40:40.480]  2021 whatever and now they're all kind of backtrack in that prediction I you as a\n",
      "[01:40:42.320 --> 01:40:50.080]  as an AI person do you for yourself privately make predictions or do they get in the way of like\n",
      "[01:40:50.080 --> 01:40:56.080]  your actual ability to think about a thing yeah I would say like what's easy to say is that\n",
      "[01:40:56.080 --> 01:41:00.640]  this problem is tractable and that's an easy prediction to make it's tractable it's going to work\n",
      "[01:41:00.640 --> 01:41:04.640]  yes it's just really hard something's turn out to be harder and something's turn out to be easier\n",
      "[01:41:05.040 --> 01:41:10.640]  so but it's it definitely feels tractable and it feels like at least the team at Tesla which is\n",
      "[01:41:10.640 --> 01:41:17.680]  what I saw internally is definitely on track to that how do you form a strong representation that\n",
      "[01:41:17.680 --> 01:41:23.280]  allows you to make a prediction about tractability so like you're the leader of a lot a lot of humans\n",
      "[01:41:24.640 --> 01:41:30.960]  you have to kind of say this is actually possible like yeah how do you build up that intuition\n",
      "[01:41:31.120 --> 01:41:36.320]  doesn't have to be even driving it could be other tasks it could be um and I want to what\n",
      "[01:41:36.320 --> 01:41:41.440]  difficult task did you work on your life I mean classification achieving certain just an\n",
      "[01:41:41.440 --> 01:41:48.480]  image net certain level of superhuman level performance yeah expert intuition it's just intuition\n",
      "[01:41:48.480 --> 01:41:54.720]  it's belief so just like thinking about it long enough like studying looking at sample data like\n",
      "[01:41:54.720 --> 01:42:00.160]  you said driving my intuition was really flawed on this but like I don't have a good intuition\n",
      "[01:42:00.160 --> 01:42:06.320]  about tractability it could be either it could be anything it could be solvable like uh\n",
      "[01:42:07.680 --> 01:42:13.920]  you know the driving task could could be simplified into something quite trivial like uh the\n",
      "[01:42:13.920 --> 01:42:19.360]  solution to the problem would be quite trivial and at scale more and more cars driving perfectly\n",
      "[01:42:20.400 --> 01:42:24.960]  might make the problem much easier yes and the more cars you have driving like people learn how to\n",
      "[01:42:24.960 --> 01:42:32.800]  drive correctly not correctly but in a way that's more optimal for uh heterogeneous system of autonomous\n",
      "[01:42:32.800 --> 01:42:38.960]  and semi-autonomous and manually driven cars that could change stuff then again also I've spent\n",
      "[01:42:38.960 --> 01:42:44.720]  a ridiculous number of hours just staring at pedestrians crossing streets thinking about humans\n",
      "[01:42:45.280 --> 01:42:53.280]  and it feels like the way we use our eye contact it sends really strong signals and there's\n",
      "[01:42:53.280 --> 01:42:57.840]  certain quirks and edge cases of behavior and of course a lot of the fatalities that happen have to\n",
      "[01:42:57.840 --> 01:43:04.880]  do with drunk driving and um both on the pedestrian side and the driver side so there's that problem\n",
      "[01:43:04.880 --> 01:43:08.800]  of driving at night and all that kind of yeah so I wonder you know let's like the space\n",
      "[01:43:10.240 --> 01:43:14.720]  of possible solution autonomous driving includes so many human factor issues\n",
      "[01:43:15.680 --> 01:43:21.600]  that it's almost impossible to predict it could be super clean nice solutions yeah I would say\n",
      "[01:43:21.600 --> 01:43:26.800]  definitely like to use a game analogy there's some fog of war but you definitely also see the\n",
      "[01:43:26.800 --> 01:43:31.760]  frontier of improvement and you can measure historically how much you've made progress and I think\n",
      "[01:43:31.760 --> 01:43:36.960]  for example at least what I've seen in uh roughly five years at Tesla when I joined it barely kept\n",
      "[01:43:36.960 --> 01:43:42.480]  lane on the highway I think going up from Pellalto to SF was like three or four interventions anytime\n",
      "[01:43:42.480 --> 01:43:47.520]  the road would do anything geometrically or turn too much it would just like not work and so going\n",
      "[01:43:47.520 --> 01:43:52.240]  from that to like a pretty competent system in five years and seeing what happens also under the hood\n",
      "[01:43:52.240 --> 01:43:55.360]  and what the scale of which the team is operating now with respect to data and compute to\n",
      "[01:43:55.360 --> 01:44:03.920]  everything else uh is just uh massive progress so this uh you're climbing amount and yes fog but\n",
      "[01:44:03.920 --> 01:44:07.760]  you make a lot of progress fog you're making progress and you see what the next directions are\n",
      "[01:44:07.760 --> 01:44:11.680]  and you're looking at some of the remaining challenges and they're not like uh they're not\n",
      "[01:44:11.680 --> 01:44:14.640]  perturbing you and they're not changing your philosophy and you're not controlling\n",
      "[01:44:14.720 --> 01:44:18.000]  contorting yourself you're like actually these are the things I've always don't need to do\n",
      "[01:44:18.000 --> 01:44:21.920]  yeah the fundamental components of solving the problem seem to be there from the data engine to\n",
      "[01:44:21.920 --> 01:44:26.160]  the compute to the the compute on the car to the compute for the training all that kind of stuff\n",
      "[01:44:27.120 --> 01:44:32.720]  so you've done uh over the years you've been a test you've done a lot of amazing uh\n",
      "[01:44:32.720 --> 01:44:39.520]  breakthrough ideas and engineering all of it um from the data engine to the human side all of it\n",
      "[01:44:40.080 --> 01:44:43.120]  can you speak to why you chose to leave Tesla?\n",
      "[01:44:43.840 --> 01:44:48.960]  basically as I described that ran I think over time during those five years I've kind of uh\n",
      "[01:44:48.960 --> 01:44:53.840]  gotten myself into a little bit of a managerial position uh most of my days were you know meetings\n",
      "[01:44:53.840 --> 01:44:58.800]  and growing the organization and making um decisions about a sort of high level strategic\n",
      "[01:44:58.800 --> 01:45:04.800]  decisions about the team and what it should be working on and so on and uh is it's kind of like a\n",
      "[01:45:04.800 --> 01:45:09.120]  corporate executive role and I can do it I think I'm okay at it uh but it's not like\n",
      "[01:45:09.120 --> 01:45:14.800]  fundamentally what I what I enjoy and so I think uh when I joined um there was no computer vision\n",
      "[01:45:14.800 --> 01:45:18.880]  team because Tesla was just going from the transition of using mobile i a third party vendor for all\n",
      "[01:45:18.880 --> 01:45:22.640]  of its computer vision to having to build its computer vision system so when I showed up there were\n",
      "[01:45:22.640 --> 01:45:27.280]  two people training deep neural networks and they were training them at a computer at their at\n",
      "[01:45:27.280 --> 01:45:32.960]  their legs like yeah downstairs there's a little kind of basic classification task yeah and so\n",
      "[01:45:34.400 --> 01:45:38.640]  I kind of like grew that into what I think is a fairly respectable deep learning team\n",
      "[01:45:38.640 --> 01:45:44.720]  massive computer cluster a very good uh denunciation organization and uh I was very happy with where\n",
      "[01:45:44.720 --> 01:45:50.000]  that was it became quite autonomous and so I kind of stepped away and I uh you know I'm very excited\n",
      "[01:45:50.000 --> 01:45:55.840]  to do much more technical things again yeah and kind of like we focus on AGI what was this soul\n",
      "[01:45:55.840 --> 01:46:00.400]  searching like you took a little time off and think like what um how many mushrooms did you take\n",
      "[01:46:00.400 --> 01:46:06.880]  numbs uh I mean what what was going through your mind the human lifetime is finite yeah he did a\n",
      "[01:46:06.880 --> 01:46:12.400]  few incredible things you're you're one of the best teachers of AI in the world you're one of the\n",
      "[01:46:12.400 --> 01:46:18.320]  best and I don't mean that I mean then the best possible way you're one of the best tinkerers\n",
      "[01:46:18.320 --> 01:46:24.640]  in the AI world meaning like understanding the fundamental fundamentals of how something works by\n",
      "[01:46:24.640 --> 01:46:30.240]  building it from scratch and playing with it with the basic intuitions it's like Einstein Feynman\n",
      "[01:46:30.240 --> 01:46:35.280]  where all really good at this kind of stuff like yeah small example of a thing to to play with it to\n",
      "[01:46:35.280 --> 01:46:42.080]  try to understand it uh so that and obviously now with with us you how build a team of machine learning\n",
      "[01:46:42.880 --> 01:46:48.240]  um uh like engineers and assistant that actually accomplishes something in the real world so\n",
      "[01:46:48.240 --> 01:46:53.840]  given all that like what was the soul searching like what was hard because obviously I love the\n",
      "[01:46:53.840 --> 01:47:00.240]  company a lot and I love I love Elon I love Tesla I want um it was so it was hard to leave I love\n",
      "[01:47:00.240 --> 01:47:07.840]  the team basically um but yeah I think I actually I will be potentially like interested in revisiting it\n",
      "[01:47:07.840 --> 01:47:14.080]  maybe coming back at some point uh working optimists working in agi at tesla uh I think tesla is going\n",
      "[01:47:14.080 --> 01:47:22.320]  to do incredible things it's basically like uh it's a massive large scale robotics kind of company\n",
      "[01:47:22.320 --> 01:47:27.920]  with a ton of in house talent for doing really incredible things and I think uh human or robots\n",
      "[01:47:27.920 --> 01:47:32.080]  are going to be amazing uh I think autonomous transportation is going to be amazing all this\n",
      "[01:47:32.080 --> 01:47:36.560]  happening at tesla so I think it's just a really amazing organization so being part of it and helping\n",
      "[01:47:36.560 --> 01:47:41.120]  it along I think was very basically I enjoyed that a lot yeah it was basically difficult for those\n",
      "[01:47:41.120 --> 01:47:45.680]  reasons because I love the company uh but you know I'm happy to potentially at some point come back\n",
      "[01:47:45.680 --> 01:47:53.120]  for act two but I felt like at this stage I built the team it felt autonomous and uh I became a manager\n",
      "[01:47:53.120 --> 01:47:57.280]  and I wanted to do a lot more technical stuff I wanted to learn stuff I wanted to teach stuff uh\n",
      "[01:47:57.360 --> 01:48:01.760]  and uh I just kind of felt like it was a good time for a for a change of pace a little bit\n",
      "[01:48:01.760 --> 01:48:06.400]  what do you think is uh the best movie sequel of all times speaking apart too\n",
      "[01:48:07.040 --> 01:48:12.800]  because like because most of them suck in movies sequels yeah and you tweeted about movies so\n",
      "[01:48:12.800 --> 01:48:19.280]  just in a tiny tangent is there what's your what's like a favorite movie sequel godfather part two\n",
      "[01:48:20.480 --> 01:48:24.080]  are you a fan of godfather because you didn't even tweet or mention the godfather yeah I don't love\n",
      "[01:48:24.080 --> 01:48:29.360]  that movie I know it hasn't added that out we're gonna edit out the hate towards the godfather how dare you\n",
      "[01:48:29.360 --> 01:48:33.600]  to I think I will make a strong statement I don't know why I don't know why but I basically don't\n",
      "[01:48:33.600 --> 01:48:40.800]  like any movie before 1995 something like that didn't you mention Terminator two okay okay that's\n",
      "[01:48:40.800 --> 01:48:47.200]  like a Terminator two was a little bit later 1990 no I think Terminator two was in the\n",
      "[01:48:47.200 --> 01:48:52.080]  game I like Terminator one as well so okay so like a few exceptions but by and large for some reason\n",
      "[01:48:52.080 --> 01:48:58.000]  I don't like movies before 1995 or something they feel very slow the camera is like zoomed out\n",
      "[01:48:58.000 --> 01:49:04.400]  it's boring it's kind of naive it's kind of weird and also Terminator was very much ahead of its time yes\n",
      "[01:49:04.400 --> 01:49:12.880]  and the godfather there's like no aji so I mean but you have good will hunting was one of the movies\n",
      "[01:49:12.880 --> 01:49:17.440]  you mentioned and that doesn't have any aji either I guess that's mathematics yeah I guess\n",
      "[01:49:17.440 --> 01:49:22.880]  occasionally I do enjoy movies that don't feature or like anchor man that has no that's that's\n",
      "[01:49:22.880 --> 01:49:30.640]  that's so good I don't understand I'm speaking of aji because I don't understand why wolf arrows\n",
      "[01:49:30.640 --> 01:49:35.840]  so funny it doesn't make sense it doesn't compute there's just something about him and he's a\n",
      "[01:49:35.840 --> 01:49:40.720]  singular human because you don't get that many comedies these days and I wonder if I have to do\n",
      "[01:49:40.720 --> 01:49:45.760]  about the culture or the like the machine of Hollywood or does it have to do with just we got\n",
      "[01:49:45.760 --> 01:49:50.240]  lucky with certain people in comedy it came together because he is a singular human\n",
      "[01:49:52.880 --> 01:49:58.080]  that was a ridiculous tangent I apologize but you mentioned humanoid robots so what do you think\n",
      "[01:49:58.080 --> 01:50:03.600]  about Optimus about Tesla bot do you think we'll have robots in the factory and in the home in\n",
      "[01:50:03.600 --> 01:50:08.880]  10 20 30 40 50 years yeah I think it's a very hard project I think it's going to take a while\n",
      "[01:50:08.880 --> 01:50:13.280]  but who else is going to build humanoid robots at scale yeah and I think it is a very good form\n",
      "[01:50:13.360 --> 01:50:16.880]  factor to go after because like I mentioned the here the world is designed for humanoid form\n",
      "[01:50:16.880 --> 01:50:21.360]  factor these things would be able to operate our machines they would be able to sit down in chairs\n",
      "[01:50:21.360 --> 01:50:26.240]  a dry potentially even drive cars basically the world is designed for humans that's the form\n",
      "[01:50:26.240 --> 01:50:30.880]  factor you want to invest into and make work over time I think you know there's another school\n",
      "[01:50:30.880 --> 01:50:35.360]  of thought which is okay pick a problem and design a robot to it but actually designing a robot\n",
      "[01:50:35.360 --> 01:50:39.680]  and getting a whole data engine and everything behind it to work is actually an incredibly hard problem\n",
      "[01:50:39.680 --> 01:50:44.160]  so it makes sense to go after general interfaces that okay they are not perfect for anyone\n",
      "[01:50:44.160 --> 01:50:49.680]  given task but they actually have the generality of just with the prompt with English able to do\n",
      "[01:50:49.680 --> 01:50:55.120]  something across and so I think it makes a lot of sense to go after a general interface\n",
      "[01:50:56.480 --> 01:51:00.800]  in the physical world and I think it's a very difficult project means going to take time\n",
      "[01:51:01.760 --> 01:51:05.440]  but I've seen no other no other company that can execute on that vision I think it's going to be\n",
      "[01:51:05.440 --> 01:51:10.640]  amazing like basically physical labor like if you think transportation is a large market\n",
      "[01:51:10.640 --> 01:51:17.680]  try physical labor but it's not just physical labor to me the thing that's also exciting is\n",
      "[01:51:17.680 --> 01:51:23.520]  social robotics so the the relationship will have on different levels with those robots yeah that's\n",
      "[01:51:23.520 --> 01:51:31.680]  why I was really excited to see optimists like people have criticized me for the excitement but I've\n",
      "[01:51:31.680 --> 01:51:39.840]  worked with a lot of research labs that do humanoid legate robots boss and dynamics unitary a lot\n",
      "[01:51:39.840 --> 01:51:47.120]  there's a lot of companies that do legate robots but that's the the elegance of the movement\n",
      "[01:51:47.760 --> 01:51:54.560]  is a tiny tiny part of the big picture so integrating the two big exciting things to me about\n",
      "[01:51:54.560 --> 01:52:02.160]  Tesla doing humanoid or any legate robots is clearly integrating into the data engine\n",
      "[01:52:02.880 --> 01:52:08.080]  so the the data engine aspect so the actual intelligence for the perception of the\n",
      "[01:52:08.080 --> 01:52:12.480]  and the control and the planning and all that kind of stuff integrating into the huge the fleet\n",
      "[01:52:12.480 --> 01:52:19.360]  that you mentioned right and then speaking of fleet the second thing is the mass manufacturers\n",
      "[01:52:19.360 --> 01:52:29.680]  just knowing culturally driving towards a simple robot that's cheap to produce at scale and doing\n",
      "[01:52:29.680 --> 01:52:33.600]  that well having experience to do that well that changes everything that's why that's a very\n",
      "[01:52:33.600 --> 01:52:40.160]  different culture and style than boss and dynamics who by the way those those robots are just the\n",
      "[01:52:40.160 --> 01:52:46.160]  the way they move it's like it'll be a very long time before Tesla could achieve this smoothness\n",
      "[01:52:46.160 --> 01:52:52.160]  of movement but that's not what it's about it's it's about it's about the entirety of the system\n",
      "[01:52:52.160 --> 01:52:56.720]  like we talked about the data engine and the fleet that's super exciting even the initial sort of\n",
      "[01:52:56.720 --> 01:53:04.080]  models but that too was really surprising that in a few months you can get a prototype yeah and\n",
      "[01:53:04.080 --> 01:53:08.720]  the reason that happened very quickly is as you alluded to there's a ton of copy based from what's\n",
      "[01:53:08.720 --> 01:53:12.880]  happening on the autopilot a lot the amount of expertise that like came out of the woodworks at\n",
      "[01:53:12.880 --> 01:53:18.320]  Tesla for building the human robot was incredible to see like basically Elon said at one point\n",
      "[01:53:18.320 --> 01:53:24.160]  we're doing this and then next day basically like all these CAD models started to appear and people\n",
      "[01:53:24.160 --> 01:53:28.880]  talking about like the supply chain and manufacturing and people showed up with like screwdrivers and\n",
      "[01:53:28.880 --> 01:53:33.120]  everything like the other day and started to like put together the body and I was like whoa like all\n",
      "[01:53:33.120 --> 01:53:36.720]  these people exist at Tesla and fundamentally building a car is actually not that different from\n",
      "[01:53:36.720 --> 01:53:42.720]  building a robot the same and that is true not just for the hardware pieces and also let's not\n",
      "[01:53:42.720 --> 01:53:49.360]  forget hardware not just for demo but manufacturing of that hardware at scale it's like a whole\n",
      "[01:53:49.360 --> 01:53:54.000]  different thing but for software as well basically this robot currently thinks it's a car\n",
      "[01:53:56.400 --> 01:54:02.000]  it's gonna have a mid-life crisis it's something it thinks it's a car some of the earlier demos\n",
      "[01:54:02.000 --> 01:54:05.040]  actually we were talking about potentially doing them outside in the parking lot because that's\n",
      "[01:54:05.040 --> 01:54:10.320]  where all of the computer vision that was like working out of the box instead of like inside\n",
      "[01:54:11.280 --> 01:54:16.400]  but all the operating system everything just copy-paste computer vision mostly copy-paste I mean\n",
      "[01:54:16.400 --> 01:54:19.440]  you have to retrain the neural nuts but the approach on everything and data engine and offline\n",
      "[01:54:19.440 --> 01:54:23.840]  trackers and the way we go about the occupancy tracker and so on everything copy-paste you just need\n",
      "[01:54:23.840 --> 01:54:28.560]  to retrain the neural nuts and then the planning control of course has to change quite a bit\n",
      "[01:54:28.560 --> 01:54:33.360]  but there's a ton of copy-paste from what's happening at Tesla and so if you were to if you were to\n",
      "[01:54:33.360 --> 01:54:37.280]  go with goal of like okay let's build a million human robots and you're not Tesla that's\n",
      "[01:54:37.280 --> 01:54:43.760]  that's a lot to ask if you're Tesla it's actually like it's not it's not that crazy and then the\n",
      "[01:54:43.760 --> 01:54:47.760]  follow-up question is then how difficult just like we're driving how difficult is the manipulation\n",
      "[01:54:47.760 --> 01:54:54.400]  task such that it can have an impact at scale I think depending on the context the really nice\n",
      "[01:54:54.400 --> 01:55:00.640]  thing about robotics is that unless you do a manufacturer and that kind of stuff is there is\n",
      "[01:55:00.640 --> 01:55:06.960]  more room for error driving is so safety critical and so the and also time critical I got robot\n",
      "[01:55:06.960 --> 01:55:12.880]  is allowed to move slower which is nice yes I think it's going to take a long time but the way you\n",
      "[01:55:12.880 --> 01:55:17.040]  want to structure the development as you need to say okay it's going to take a long time how can I\n",
      "[01:55:17.040 --> 01:55:22.800]  set up the product development roadmap so that I'm making revenue along the way I'm not setting\n",
      "[01:55:22.800 --> 01:55:26.480]  myself up for a zero one loss function where it doesn't work until it works you don't want to be\n",
      "[01:55:26.480 --> 01:55:30.880]  in that position you want to make it useful almost immediately and then you want to slowly deploy it\n",
      "[01:55:31.920 --> 01:55:36.720]  and at scale and you want to set up your data engine your improvement loops\n",
      "[01:55:36.800 --> 01:55:42.800]  the telemetry the evaluation the harness and everything and you want to improve the product over time\n",
      "[01:55:42.800 --> 01:55:46.640]  incrementally and you're making revenue along the way that's extremely important because otherwise\n",
      "[01:55:46.640 --> 01:55:51.600]  you cannot build these large undertakings just like don't make sense economically and also from\n",
      "[01:55:51.600 --> 01:55:55.440]  the point of view of the team working on it they need the dopamine along the way they're not just\n",
      "[01:55:55.440 --> 01:56:00.320]  going to make a promise about this being useful this is going to change the world in 10 years when it\n",
      "[01:56:00.320 --> 01:56:04.320]  works this is not where you want to be you want to be in a place like I think auto-pulled this\n",
      "[01:56:04.320 --> 01:56:10.640]  today where it's offering increased safety and and convenience of driving today people pay for it\n",
      "[01:56:10.640 --> 01:56:15.280]  people like it people purchase it and then you also have the greater mission that you're working towards\n",
      "[01:56:16.240 --> 01:56:20.880]  and you see that so the dopamine for the team that that was the source of happiness and\n",
      "[01:56:20.880 --> 01:56:25.520]  success you're deploying this people like it people drive it people pay for it they care about it\n",
      "[01:56:25.520 --> 01:56:29.920]  there's all these YouTube videos your grandma drives it she gives you feedback people like it people\n",
      "[01:56:29.920 --> 01:56:34.880]  engage with it you engage with it huge do people that drive testless like recognize you and give\n",
      "[01:56:34.880 --> 01:56:42.800]  you love like like hey thanks for the for the this nice feature that is doing yeah I think the trick\n",
      "[01:56:42.800 --> 01:56:46.560]  of thing is like some people really love you some people unfortunately like you're working on\n",
      "[01:56:46.560 --> 01:56:50.800]  something that you think is extremely valuable useful etc some people do hate you there's a lot of\n",
      "[01:56:50.800 --> 01:56:55.840]  people who like hate me and the team and what everything the whole project and I think they're\n",
      "[01:56:55.840 --> 01:57:01.680]  Tesla drivers many cases they're not actually yeah that's that's actually makes me sad about\n",
      "[01:57:01.680 --> 01:57:08.240]  humans or the current the ways the humans interact I think that's actually fixable I think humans\n",
      "[01:57:08.240 --> 01:57:12.480]  want to be good to each other I think Twitter and social media is part of the mechanism that\n",
      "[01:57:12.480 --> 01:57:18.400]  actually somehow makes the negativity more viral that it doesn't deserve like disproportionately\n",
      "[01:57:19.360 --> 01:57:25.920]  add of like a viral viral boost yeah negativity but I got I wish people would just get excited about\n",
      "[01:57:27.040 --> 01:57:32.400]  so suppress some of the jealousy some of the ego and just get excited for others and then\n",
      "[01:57:32.960 --> 01:57:36.480]  there's a karma aspect to that you get excited for others they'll get excited for you\n",
      "[01:57:36.480 --> 01:57:40.960]  same thing in academia if you're not careful there is a like a dynamical system there\n",
      "[01:57:41.200 --> 01:57:45.440]  if you if you think of in silos and get jealous of somebody else being successful\n",
      "[01:57:46.000 --> 01:57:52.160]  that actually perhaps counter intuitively uh least the less productivity of you as a community\n",
      "[01:57:52.160 --> 01:57:59.280]  and you individually I feel like if you keep celebrating others that actually makes you more successful\n",
      "[01:57:59.280 --> 01:58:03.680]  yeah and I think people haven't in depending on the industry haven't quite learned that yet\n",
      "[01:58:04.080 --> 01:58:08.400]  some people are also very negative and very vocal so they're very prominently featured but actually\n",
      "[01:58:08.400 --> 01:58:13.440]  there's a ton of people who are a cheerleaders but they're silent cheerleaders and uh when you talk\n",
      "[01:58:13.440 --> 01:58:18.160]  to people just in the world they will all tell you it's amazing it's great especially like people\n",
      "[01:58:18.160 --> 01:58:21.600]  who understand how difficult it is to get the stuff working like people who have built products\n",
      "[01:58:21.600 --> 01:58:26.160]  and makers and entrepreneurs entrepreneurs like making making this work and changing something\n",
      "[01:58:26.880 --> 01:58:30.240]  is incredibly hard those people are more likely to cheerlead you\n",
      "[01:58:30.960 --> 01:58:35.040]  well one of the things that makes me sad is some folks in the robotics community you know how\n",
      "[01:58:35.040 --> 01:58:39.280]  don't do the cheerleading and they should there's uh because they know how difficult it is well\n",
      "[01:58:39.280 --> 01:58:43.200]  they actually sometimes don't know how difficult it is to create a product that scale right yeah\n",
      "[01:58:43.280 --> 01:58:50.240]  they actually deploy in the real world a lot of the development of robots and AI system is done\n",
      "[01:58:50.240 --> 01:58:55.760]  on very specific small benchmarks um and as opposed to real world conditions yes\n",
      "[01:58:57.040 --> 01:59:01.280]  yeah I think it's really hard to work on robotics in academic setting or AI systems that apply\n",
      "[01:59:01.280 --> 01:59:08.640]  in the real world you you've criticized you um flourished and loved for time the image\n",
      "[01:59:08.800 --> 01:59:15.120]  nut the famed image nut dataset and have recently had some words of criticism that the\n",
      "[01:59:15.840 --> 01:59:21.360]  academic research ML community gives a little too much love still to the image nut or like\n",
      "[01:59:21.920 --> 01:59:26.320]  those kinds of benchmarks can you can you speak to the strengths and weaknesses of data sets\n",
      "[01:59:27.040 --> 01:59:32.560]  used in machine learning research actually I don't know that I recall the specific instance where I\n",
      "[01:59:32.560 --> 01:59:38.000]  was uh unhappy or criticizing image nut I think image nut has been extremely valuable uh\n",
      "[01:59:38.800 --> 01:59:44.320]  it was basically a benchmark that allowed the deep learning community to demonstrate that deep\n",
      "[01:59:44.320 --> 01:59:50.720]  neural works actually work yes it was uh there's a massive value in that um so I think image nut\n",
      "[01:59:50.720 --> 01:59:55.360]  was useful but um basically it's become a bit of an emnist at this point so emnist is like\n",
      "[01:59:55.360 --> 01:59:59.760]  little 228 by 28 gray scale digits there's kind of a joke dataset that everyone like\n",
      "[01:59:59.760 --> 02:00:03.600]  crushes you know there's still papers written on emnist though right maybe they should have\n",
      "[02:00:03.760 --> 02:00:09.040]  strong papers like papers that focus on like how do we learn with a small amount of data that\n",
      "[02:00:09.040 --> 02:00:11.760]  could stuff yeah I could see that being helpful but not in sort of like mainline\n",
      "[02:00:11.760 --> 02:00:15.760]  computer vision research anymore of course I think the way I've heard you somewhere maybe I'm\n",
      "[02:00:15.760 --> 02:00:20.240]  just imagining things but I think you said like image nut was a huge contribution to the community\n",
      "[02:00:20.240 --> 02:00:24.560]  for a long time and now it's time to move past those kinds of well emnist has been crushed I mean\n",
      "[02:00:24.560 --> 02:00:31.840]  you know the error rates are uh yeah we're getting like 90% accuracy in in one thousand\n",
      "[02:00:31.920 --> 02:00:39.360]  classification way prediction and I've seen those images and it's like really high this really\n",
      "[02:00:39.360 --> 02:00:44.880]  that's really good if I'm correctly the top five error rate is now like 1% or something given\n",
      "[02:00:44.880 --> 02:00:50.000]  your experience with a gigantic real world dataset would you like to see benchmarks move in a certain\n",
      "[02:00:50.000 --> 02:00:54.320]  direction that the research community uses unfortunately I don't think academics currently have\n",
      "[02:00:54.320 --> 02:00:58.880]  the next image nut uh we've obviously I think we've crushed emnist we've basically kind of crushed\n",
      "[02:00:58.880 --> 02:01:05.040]  emnist nut and there's no next sort of big benchmark that the entire community rel is behind and\n",
      "[02:01:05.040 --> 02:01:10.880]  uses um you know for further development of these networks yeah what it would it takes for data set\n",
      "[02:01:10.880 --> 02:01:16.160]  to captivate the imagination of everybody like where they all get behind it that that could also\n",
      "[02:01:16.160 --> 02:01:22.000]  need like a virus like a leader right yeah somebody with popularity I mean that yeah what did\n",
      "[02:01:22.000 --> 02:01:27.920]  emnist not take off is there is it just the accident of history it was the right amount of difficult\n",
      "[02:01:29.280 --> 02:01:33.920]  it was the right amount of difficult and simple and uh interesting enough it just kind of like it\n",
      "[02:01:33.920 --> 02:01:41.760]  was it was the right time for that kind of a dataset question from reddit uh what are your thoughts\n",
      "[02:01:41.760 --> 02:01:46.640]  on the role of the synthetic data and game engines will play in the future of neural net model development\n",
      "[02:01:48.160 --> 02:01:55.920]  I think um as neural nets converge to humans uh the value of simulation to neural nets will be\n",
      "[02:01:55.920 --> 02:02:03.120]  similar to value of simulation to humans so people use simulation for uh people do simulation\n",
      "[02:02:03.120 --> 02:02:07.920]  because they can learn something in that kind of a system um and uh without having to actually\n",
      "[02:02:07.920 --> 02:02:13.440]  experience it um but are you referring to the simulation with doing our head is it no sorry simulation\n",
      "[02:02:13.440 --> 02:02:19.600]  I mean like video games or uh you know um other forms of simulation for various professionals\n",
      "[02:02:19.600 --> 02:02:24.080]  well let me push back and because that maybe there's simulation that we do in our heads like\n",
      "[02:02:24.720 --> 02:02:30.880]  simulate if I do this what do I think will happen okay that's like internal simulation yeah internal\n",
      "[02:02:30.880 --> 02:02:34.480]  isn't that what we're doing as you assume it before we act oh yeah but that's independent from like\n",
      "[02:02:34.480 --> 02:02:38.800]  the use of simulation in the sense of like computer games or using simulation for training\n",
      "[02:02:38.800 --> 02:02:43.360]  set creation or you know is it independent or is it just loosely correlated because like uh\n",
      "[02:02:44.560 --> 02:02:50.960]  isn't that useful to do like um counterfactual or like edge case simulation to like\n",
      "[02:02:51.920 --> 02:02:57.120]  you know what happens if there's a nuclear war what happens if there's you know like those\n",
      "[02:02:57.120 --> 02:03:01.600]  kinds of things yeah that's a different simulation from like unrelenged that's how I interpreted\n",
      "[02:03:01.600 --> 02:03:08.640]  the question uh so like simulation of the average case um is that what's unrelenged what\n",
      "[02:03:08.640 --> 02:03:15.920]  what what what what what what do you mean by unrelenged so simulating a world yeah physics of that world\n",
      "[02:03:16.320 --> 02:03:22.960]  why is that different like because you also can add behavior to that world and you can try all kinds\n",
      "[02:03:22.960 --> 02:03:28.320]  of stuff right you could throw all kinds of weird things into it yeah so a real engine is not just\n",
      "[02:03:28.320 --> 02:03:33.440]  about simulating I mean I guess it is about simulating the physics of the world it's also doing\n",
      "[02:03:33.440 --> 02:03:38.480]  something with that yeah the graphics the physics and the agents that you put into the environment\n",
      "[02:03:38.480 --> 02:03:42.320]  and stuff like that yeah see I think you I feel like you said that it's not that important\n",
      "[02:03:42.880 --> 02:03:48.560]  I guess for the future of AI development is that is that correct to interpret it either way I think\n",
      "[02:03:49.280 --> 02:03:55.840]  humans use simulators for humans use simulators and they find them useful and so computers\n",
      "[02:03:55.840 --> 02:04:01.680]  will use simulators and find them useful okay so you're saying it's not I don't use simulators very\n",
      "[02:04:01.680 --> 02:04:06.080]  often I play a video game every once in a while but I don't think I derive any wisdom about\n",
      "[02:04:06.080 --> 02:04:12.080]  my own existence from from those video games it's a momentary escape from reality versus\n",
      "[02:04:12.320 --> 02:04:17.520]  a source of wisdom about reality so I don't so I think that's a very polite way of saying\n",
      "[02:04:17.520 --> 02:04:23.200]  simulation is not that useful yeah maybe maybe not I don't see it as like a fundamental really\n",
      "[02:04:23.200 --> 02:04:28.800]  important part of like training neural nets currently but I think as neural nets become more and\n",
      "[02:04:28.800 --> 02:04:36.160]  more powerful I think you will need fewer examples to train additional behaviors and simulation is\n",
      "[02:04:36.160 --> 02:04:39.680]  of course there's a domain gap in a simulation there's not the real world is slightly something\n",
      "[02:04:39.680 --> 02:04:45.600]  different but with a powerful enough neural net you need the domain gap can be bigger I think\n",
      "[02:04:45.600 --> 02:04:49.200]  because neural net will sort of understand that even though it's not the real world it like\n",
      "[02:04:49.200 --> 02:04:53.440]  has all this high level structure that I'm supposed to be like learn from so the neural net will actually\n",
      "[02:04:54.240 --> 02:05:01.200]  yeah we'll be able to leverage this static data better yes by closing the gap but understanding\n",
      "[02:05:01.680 --> 02:05:08.640]  in which ways this is not real data exactly right to do better questions next time that was\n",
      "[02:05:08.640 --> 02:05:16.640]  that was a question that I'm just kidding all right so is it possible do you think speaking of\n",
      "[02:05:16.640 --> 02:05:21.120]  amnesty to construct neural nets and training processes that require very little data\n",
      "[02:05:23.200 --> 02:05:27.600]  so we've been talking about huge data sets like the internet for training I mean one way to say\n",
      "[02:05:27.600 --> 02:05:32.320]  that is like you said like the querying itself is another level of training I guess and that\n",
      "[02:05:32.320 --> 02:05:40.640]  requires a little data yeah but do you see any value in doing research and kind of going down\n",
      "[02:05:40.640 --> 02:05:46.080]  in the direction of can we use very little data to train to construct a knowledge base 100 percent\n",
      "[02:05:46.080 --> 02:05:50.160]  I just think like at some point you need a massive data set and then when you pre-training your\n",
      "[02:05:50.160 --> 02:05:54.880]  massive neural net and get something that you know is like a GPT or something then you're able to\n",
      "[02:05:54.880 --> 02:06:01.840]  be very efficient at training and you're returning new task so a lot of these GPTs you know you can do\n",
      "[02:06:01.840 --> 02:06:06.880]  tasks like sentiment analysis or translation or so on just by being prompted with very few examples\n",
      "[02:06:06.880 --> 02:06:10.480]  here's the kind of thing I want you to do like here's an input sentence here's the translation\n",
      "[02:06:10.480 --> 02:06:15.680]  into German input sentence translation to German input sentence blank and the neural net will complete\n",
      "[02:06:15.680 --> 02:06:20.320]  the translation to German just by looking at sort of the example you've provided and so that's an\n",
      "[02:06:20.320 --> 02:06:25.360]  example of a very few shot learning in the activations of the neural net instead of the weights of\n",
      "[02:06:25.360 --> 02:06:31.600]  the neural net and so I think basically just like humans neural nets will become very data efficient\n",
      "[02:06:31.840 --> 02:06:36.240]  at learning any other new task but at some point you need a massive data set to pre-training your\n",
      "[02:06:36.240 --> 02:06:42.400]  network to get that and we probably we humans have something like that do we do we have something\n",
      "[02:06:42.400 --> 02:06:50.800]  like that do we have a passive in the background background model constructing thing they just\n",
      "[02:06:50.800 --> 02:06:55.360]  runs all the time in a self supervised way we're not conscious of it I think humans definitely I mean\n",
      "[02:06:55.360 --> 02:07:02.000]  obviously we have we learn a lot during during our life span but also we have a ton of hardware\n",
      "[02:07:02.000 --> 02:07:07.120]  that helps us initialize initialization coming from sort of evolution and so I think that's also\n",
      "[02:07:07.120 --> 02:07:10.800]  a really big big component a lot of people in the field I think they just talk about the\n",
      "[02:07:10.800 --> 02:07:14.880]  amounts of like seconds and the you know that a person has lived pretending that this is a\n",
      "[02:07:14.880 --> 02:07:20.480]  taboo or arasa sort of like a zero initialization of a neural net and it's not like you can look at a\n",
      "[02:07:20.480 --> 02:07:27.760]  lot of animals like for example zebra zebra's get born and they see and they can run there's zero\n",
      "[02:07:27.760 --> 02:07:32.960]  trained data in their life span they can just do that so somehow I have no idea how evolution\n",
      "[02:07:32.960 --> 02:07:36.960]  has found a way to encode these algorithms and these neural net initializations that are extremely\n",
      "[02:07:36.960 --> 02:07:42.000]  good into ATCGs and I have no idea how this works but apparently it's possible because here's\n",
      "[02:07:42.000 --> 02:07:49.200]  approved by existence there's something magical about going from a single cell to an organism that\n",
      "[02:07:49.200 --> 02:07:53.920]  is born to the first few years of life I kind of like the idea that the reason we don't remember\n",
      "[02:07:53.920 --> 02:08:00.240]  anything about the first few years of our life is that it's a really painful process like it's a very\n",
      "[02:08:00.240 --> 02:08:08.640]  difficult challenging training process yeah like intellectually like and maybe yeah I mean I don't\n",
      "[02:08:09.520 --> 02:08:14.640]  why don't we remember any of that there might be some crazy training going on and\n",
      "[02:08:14.880 --> 02:08:24.240]  and that the maybe that's the background model training that is very painful and so it's best for\n",
      "[02:08:24.240 --> 02:08:28.880]  the system once it's trained not to remember how it's constructed I think it's just like the hardware\n",
      "[02:08:28.880 --> 02:08:33.520]  for long-term memory is just not fully developed share I kind of feel like the first few years of\n",
      "[02:08:35.120 --> 02:08:40.240]  offense is not actually like learning it's brain maturing yeah we're born premature\n",
      "[02:08:40.880 --> 02:08:45.360]  um there's a theory along those lines because of the birth canal and this along of the brain\n",
      "[02:08:45.360 --> 02:08:49.200]  and so we're born premature and then the first few years were just the brains maturing\n",
      "[02:08:49.200 --> 02:08:54.880]  uh and then there's some learning eventually um it's my current view on it what do you think\n",
      "[02:08:55.600 --> 02:09:01.840]  do you think neural nets can have long-term memory like that approach is something like humans\n",
      "[02:09:01.840 --> 02:09:05.680]  do you think you don't do do you think there needs to be another meta architecture on top of it to\n",
      "[02:09:05.680 --> 02:09:10.560]  add something like a knowledge base that learns facts about the world and all that kind of stuff\n",
      "[02:09:10.560 --> 02:09:16.640]  yes but I don't know to what extent it will be explicitly constructed um it might take on\n",
      "[02:09:16.640 --> 02:09:22.720]  intuitive forms where you are telling the GPT like hey you have a you have a declarative memory bank\n",
      "[02:09:22.720 --> 02:09:27.120]  to which you can store and retrieve data from and whenever you encounter some information that\n",
      "[02:09:27.120 --> 02:09:32.080]  you find useful just save it to your memory bank and here's an example of something you have retrieved\n",
      "[02:09:32.080 --> 02:09:37.200]  and he says how you say it and here's how you load from it you just say load whatever you teach\n",
      "[02:09:37.200 --> 02:09:43.280]  it in text in English and then it might learn to use a memory bank from from that oh so in\n",
      "[02:09:43.280 --> 02:09:48.480]  so the neural net is the architecture for the background model the the the base thing and then\n",
      "[02:09:48.480 --> 02:09:52.000]  yeah everything else is just not top of the so it's not just text right it's uh you're giving it\n",
      "[02:09:52.000 --> 02:09:56.800]  gadgets and gizmos so uh you're teaching some kind of a special language by which we can it can\n",
      "[02:09:56.800 --> 02:10:00.800]  save arbitrary information and retrieve it at a later time yeah and you're you're telling about\n",
      "[02:10:00.800 --> 02:10:05.680]  these special tokens and how to arrange them to use these interfaces it's like hey you can use\n",
      "[02:10:05.680 --> 02:10:11.520]  a calculator here's how you use it just do five three plus four one equals and when equals is there\n",
      "[02:10:12.480 --> 02:10:16.160]  a calculator will actually read out the answer and you don't have to calculate it yourself\n",
      "[02:10:16.160 --> 02:10:20.800]  and you just like tell it in English this might actually work do you think in that sense\n",
      "[02:10:20.800 --> 02:10:25.760]  Goddo is interesting the the deep mind system that it's not just no language but actually throws it all\n",
      "[02:10:26.720 --> 02:10:33.520]  uh in the same pile images actions all that kind of stuff that's basically what we're moving towards\n",
      "[02:10:34.080 --> 02:10:38.720]  yeah I think so so Goddo is is very much a kitchen sink of approach to like\n",
      "[02:10:39.440 --> 02:10:44.320]  um reinforcement learning lots of different environments with a single fixed a transformer\n",
      "[02:10:44.320 --> 02:10:50.720]  a model right um I think it's a very sort of early result in that in that realm but I think uh\n",
      "[02:10:50.720 --> 02:10:54.640]  yeah it's along the lines of what I think things will eventually look like right so this is the\n",
      "[02:10:54.720 --> 02:10:59.840]  early days of a system that eventually will look like this like from a rich is rich sudden perspective\n",
      "[02:10:59.840 --> 02:11:05.120]  yeah I'm not super huge fan of I think all these interfaces that like look very different um I would\n",
      "[02:11:05.120 --> 02:11:10.080]  want everything to be normalized into the same API so for example screen pixels very same API\n",
      "[02:11:10.080 --> 02:11:13.280]  instead of having like different world environments that are very different physics and joint\n",
      "[02:11:13.280 --> 02:11:17.920]  configurations and appearances and whatever and you're having some kind of special tokens for different\n",
      "[02:11:17.920 --> 02:11:22.480]  games that you can plug I'd rather just normalize everything to a single interface\n",
      "[02:11:22.480 --> 02:11:26.480]  so it looks the same to the neural not if that makes sense so it's all going to be pixel-based\n",
      "[02:11:26.480 --> 02:11:36.400]  pong in the end I think so okay uh let me ask you about your own personal life a lot of people\n",
      "[02:11:36.400 --> 02:11:40.560]  want to know you're one of the most productive and brilliant people in the history of AI what is\n",
      "[02:11:40.560 --> 02:11:46.480]  a productive day in the life of Andre Kapati look like what time do you wake up you look as imagine\n",
      "[02:11:47.200 --> 02:11:52.160]  um some kind of dance between the average productive day and a perfect productive day so the\n",
      "[02:11:52.160 --> 02:11:57.680]  perfect productive day is the thing we strive towards in the average is kind of what it kind of converges\n",
      "[02:11:57.680 --> 02:12:02.960]  to get all the mistakes and human eventualities and so on yeah so what times you wake up\n",
      "[02:12:02.960 --> 02:12:08.720]  are you a morning person I'm not a morning person I'm a night owl for sure is stable or not\n",
      "[02:12:08.720 --> 02:12:14.640]  that's semi-stable like eight or nine or something like that during my PhD it was even later I used\n",
      "[02:12:14.640 --> 02:12:20.800]  to go to sleep usually at 3 a.m. I think the a.m. hours are are precious a very interesting time to\n",
      "[02:12:20.800 --> 02:12:26.880]  work because everyone is asleep at at 8 a.m. or 7 a.m. the east coast is awake so there's already\n",
      "[02:12:26.880 --> 02:12:30.400]  activity there's already some text messages whatever there's stuff happening you can go on like\n",
      "[02:12:31.040 --> 02:12:36.480]  some news website and there's stuff happening it's distracting at 3 a.m. everything is totally quiet\n",
      "[02:12:36.480 --> 02:12:42.640]  and so you're not going to be bothered and you have solid chunks of time to do work um so I like those\n",
      "[02:12:42.640 --> 02:12:48.880]  periods night owl by default and then I think like productive time basically um what I like to do is\n",
      "[02:12:48.880 --> 02:12:54.560]  you need you need to like build some momentum on the problem without too much distraction and um\n",
      "[02:12:54.560 --> 02:13:01.280]  you need to load your ram uh your working memory with that problem and then you need to be obsessed\n",
      "[02:13:01.280 --> 02:13:05.200]  with it when you're taking shower when you're falling asleep you need to be obsessed with the problem\n",
      "[02:13:05.200 --> 02:13:09.360]  and it's fully in your memory and you're ready to wake up and work on it right there so it is a\n",
      "[02:13:09.360 --> 02:13:15.440]  skill of uh is this in a scale temporal scale of a single day or a couple of days a week a month so\n",
      "[02:13:15.440 --> 02:13:19.920]  I can't talk about one day basically in isolation because it's a whole process when I want to get\n",
      "[02:13:19.920 --> 02:13:24.720]  when I want to get productive in the problem I feel like I need a span of a few days where I can really\n",
      "[02:13:24.720 --> 02:13:29.680]  get in on that problem and I don't want to be interrupted and I'm going to just uh be completely\n",
      "[02:13:29.680 --> 02:13:34.720]  obsessed with that problem and that's where I do most of my good workouts. You've done a bunch of\n",
      "[02:13:34.720 --> 02:13:39.360]  cool like little projects in a very short amount of time very quickly so that that requires you\n",
      "[02:13:39.360 --> 02:13:43.760]  just focusing on it. Yeah basically I need to load my working memory with the problem and I need to\n",
      "[02:13:43.760 --> 02:13:47.360]  be productive because there's always like a huge fixed cost to approaching any problem.\n",
      "[02:13:48.880 --> 02:13:52.320]  You know like I was struggling with this for example at Tesla because I want to work on\n",
      "[02:13:52.320 --> 02:13:56.480]  like small side project but okay you first need to figure out okay I need to SSH into my cluster I\n",
      "[02:13:56.480 --> 02:14:01.840]  need to bring up a VS code editor so I can like work on this I need to I run into some stupid error\n",
      "[02:14:01.840 --> 02:14:05.520]  because of some reason like you're not at a point where you can be just productive right away.\n",
      "[02:14:05.520 --> 02:14:12.160]  You are facing barriers and so it's about really removing all that barrier and you're able to go\n",
      "[02:14:12.160 --> 02:14:16.960]  into the problem and you have the full problem loaded in your memory and somehow avoiding distractions\n",
      "[02:14:16.960 --> 02:14:24.720]  of all different forms like news stories emails but also distractions from other interesting projects\n",
      "[02:14:24.720 --> 02:14:29.040]  that you previously worked out are currently working on and so on. You just want to really focus\n",
      "[02:14:29.040 --> 02:14:33.920]  your mind and I mean I can take some time off for distractions and in between but I think it can be\n",
      "[02:14:33.920 --> 02:14:40.480]  too much. You know most of your day is sort of like spent on that problem and then you know I drink\n",
      "[02:14:40.480 --> 02:14:46.080]  coffee I had my morning routine I look at some news Twitter hacker news Wall Street Journal etc.\n",
      "[02:14:46.640 --> 02:14:51.840]  So you wake up you have some coffee are you trying to get to work as quickly as possible do you\n",
      "[02:14:51.840 --> 02:14:58.240]  do take in this diet of like what the hell is happening in the world first. I am I do find it\n",
      "[02:14:58.240 --> 02:15:02.480]  interesting to know about the world I don't know that it's useful or good but it is part of my\n",
      "[02:15:02.480 --> 02:15:07.040]  routine right now so I do read through a bunch of news articles and I want to be informed and\n",
      "[02:15:08.000 --> 02:15:12.640]  I'm suspicious of it I'm suspicious of the practice but currently that's where I am. Oh you mean\n",
      "[02:15:12.640 --> 02:15:19.200]  suspicious about the positive effect of that practice on your productivity and your well-being is\n",
      "[02:15:19.200 --> 02:15:23.840]  my well-being psychologically and also on your ability to deeply understand the world because\n",
      "[02:15:24.640 --> 02:15:28.400]  there's a bunch of sources of information you're not really focused on deeply integrating\n",
      "[02:15:28.400 --> 02:15:34.880]  that slowly distracting you're yeah in terms of a perfectly productive day for how long of a\n",
      "[02:15:34.880 --> 02:15:40.800]  stretch of time in one session do you try to work and focus anything a couple hours is it one\n",
      "[02:15:40.800 --> 02:15:46.160]  hours at 30 minutes is 10 minutes I can probably go like a small few hours and then any some breaks\n",
      "[02:15:46.160 --> 02:15:53.120]  in between for like food and stuff and yeah but I think like it's still really hard to accumulate\n",
      "[02:15:53.120 --> 02:15:57.280]  hours I was using a tracker that told me exactly how much time I spent coding any one day and even\n",
      "[02:15:57.280 --> 02:16:02.160]  on a very productive day I still spent only like six or eight hours yeah and it's just because\n",
      "[02:16:02.160 --> 02:16:08.720]  there's so much padding commute talking to people food etc there's like the cost of life\n",
      "[02:16:09.360 --> 02:16:14.960]  just living and sustaining and homeostasis and just maintaining yourself as a human is very high\n",
      "[02:16:15.840 --> 02:16:22.240]  and and there seems to be a desire within the human mind to to participate in society that\n",
      "[02:16:22.240 --> 02:16:27.920]  creates that padding yeah the most productive days of ever had is just completely from start to\n",
      "[02:16:27.920 --> 02:16:32.640]  finish just tuning out everything yeah and just sitting there and then and then you could do more\n",
      "[02:16:32.640 --> 02:16:36.960]  than six and eight hours yeah is there some wisdom about what gives you strength to do like\n",
      "[02:16:37.840 --> 02:16:43.520]  tough days of long focus yeah just like whenever I get obsessed about a problem something just needs\n",
      "[02:16:43.520 --> 02:16:48.560]  to work some she just needs to exist it needs to exist and you're so you're able to deal with bugs\n",
      "[02:16:48.560 --> 02:16:53.840]  and programming issues and technical issues and design decisions that turn out to be the wrong\n",
      "[02:16:53.840 --> 02:16:58.080]  ones you're able to think through all that given given that you want to think to exist yeah it needs\n",
      "[02:16:58.080 --> 02:17:02.720]  to exist and then I think to me also a big factor is you know our other humans are going to appreciate\n",
      "[02:17:02.720 --> 02:17:07.200]  it are they going to like it that's a big part of my motivation if I'm helping humans and they seem\n",
      "[02:17:07.200 --> 02:17:12.880]  happy they say nice things they tweet about it or whatever that gives me pleasure because I'm doing\n",
      "[02:17:12.880 --> 02:17:18.080]  something useful so like you do see yourself sharing it with the world like with San Github with\n",
      "[02:17:18.080 --> 02:17:22.160]  the blog posts or the videos yeah I was thinking about it like suppose I did all these things but\n",
      "[02:17:22.160 --> 02:17:26.400]  did not share them I don't think I would have the same motivation that I can build up you enjoy\n",
      "[02:17:26.400 --> 02:17:33.200]  the feeling of other people gaining value and happiness from the stuff you've created yeah\n",
      "[02:17:34.480 --> 02:17:40.320]  what about diet is there I saw you play in a minute fast you fast is that help with everything\n",
      "[02:17:41.600 --> 02:17:46.800]  with the things you played was been most beneficial to the your ability to mentally focus on\n",
      "[02:17:47.440 --> 02:17:52.960]  and just meant to the mental productivity and happiness you still fast yeah I still fast but I do\n",
      "[02:17:52.960 --> 02:17:57.280]  intermittent fasting but really what it means at the end of the day is I skip breakfast yeah so I do\n",
      "[02:17:58.000 --> 02:18:02.480]  18 six roughly by default when I'm in my steady state if I'm traveling or doing something else I\n",
      "[02:18:02.480 --> 02:18:08.720]  will break the rules but in my steady state I do 18 six so I eat only from 12 to 6 not a hard rule\n",
      "[02:18:08.720 --> 02:18:13.920]  and I break it often but that's my default and then yeah I've done a bunch of random experiments for\n",
      "[02:18:13.920 --> 02:18:18.160]  the most part right now where I've been for the last year and a half I want to say is I'm\n",
      "[02:18:18.800 --> 02:18:23.600]  plant-based or plant-forward I heard plant-forward it sounds better I didn't actually know the\n",
      "[02:18:23.600 --> 02:18:29.600]  differences but it sounds better in my mind but it just means I prefer plant-based food and raw or\n",
      "[02:18:29.600 --> 02:18:37.920]  cooked or I prefer cooked and plant-based so plant-based forgive me I don't actually know how\n",
      "[02:18:37.920 --> 02:18:42.080]  wide the category of plant entails well plant-based just means that you're not\n",
      "[02:18:42.400 --> 02:18:48.880]  like a chickadeean and you can flex and you just prefer to eat plants and you know you're not making\n",
      "[02:18:48.880 --> 02:18:52.960]  you're not trying to influence other people and if someone is you come to someone's house party and\n",
      "[02:18:52.960 --> 02:18:57.680]  they serve you a steak that they're really proud of you will eat it yes right judgment oh that's\n",
      "[02:18:57.680 --> 02:19:03.120]  beautiful I mean that's I'm the flip side of that but I'm very sort of flexible have you tried\n",
      "[02:19:03.120 --> 02:19:09.840]  doing one meal a day I have accidentally not consistently but I've accidentally had that I don't\n",
      "[02:19:09.840 --> 02:19:14.960]  I don't like it I think it makes me feel not good it's too it's too much too much of a hit yeah\n",
      "[02:19:14.960 --> 02:19:20.720]  and so currently I have about two meals a day 12 and six I do that now and stop from doing it no\n",
      "[02:19:20.720 --> 02:19:25.600]  until one meal a day okay it's interesting it's interesting feeling if you ever fasted along\n",
      "[02:19:25.600 --> 02:19:30.000]  with it a day yeah I've done a bunch of water fasts because I'm curious what happens what\n",
      "[02:19:30.640 --> 02:19:34.560]  anything interesting yeah I would say so I mean you know what's interesting is that you're\n",
      "[02:19:34.560 --> 02:19:40.160]  hungry for two days and then I starting day three or so you're not hungry it's like such a weird\n",
      "[02:19:40.160 --> 02:19:44.160]  feeling because you haven't eaten in a few days and you're not hungry isn't that weird it's really\n",
      "[02:19:44.160 --> 02:19:49.120]  one one of the many weird things about human biology yeah it figures something out it finds\n",
      "[02:19:49.120 --> 02:19:54.320]  finds another source of energy or something like that or relaxes the system I don't know how\n",
      "[02:19:54.320 --> 02:19:57.760]  the body is like you're hungry you're hungry and then it just gives up it's like okay I guess we're\n",
      "[02:19:57.760 --> 02:20:02.240]  fasting now there's nothing and then it's just kind of like focuses on trying to make you\n",
      "[02:20:02.240 --> 02:20:07.600]  not hungry and you know not feel the the damage of that and trying to give you some space to figure\n",
      "[02:20:07.600 --> 02:20:15.920]  out the food situation so are you still to this day most productive at night I would say I am but\n",
      "[02:20:15.920 --> 02:20:21.520]  it is really hard to maintain my PhD schedule um especially when I was say working at Tesla and so\n",
      "[02:20:21.520 --> 02:20:27.840]  on it's a non-starter so but even now like you know people want to meet for various events they\n",
      "[02:20:27.840 --> 02:20:32.240]  society lives in a certain period of time yeah and you sort of have to like work so that's it's\n",
      "[02:20:32.240 --> 02:20:37.600]  hard to like do a social thing and then after that return and do work yeah it's just really hard\n",
      "[02:20:39.920 --> 02:20:44.480]  that's why I try to do social thing that's right not to do too too much drinking so I can return\n",
      "[02:20:44.480 --> 02:20:52.400]  and continue doing work but at Tesla is there is there conversions like in the Tesla but\n",
      "[02:20:52.400 --> 02:20:59.040]  any any company is there converged just so was a schedule or is there more is that how humans\n",
      "[02:20:59.040 --> 02:21:03.680]  behave when they collaborate I need to learn about this yeah do they try to keep us consistent\n",
      "[02:21:03.680 --> 02:21:07.920]  schedule you're all awake at the same time I'm gonna do try to create a routine and I try to\n",
      "[02:21:07.920 --> 02:21:13.280]  create a steady state in which I'm comfortable in so I have a morning routine I have a day routine\n",
      "[02:21:13.280 --> 02:21:18.640]  I try to keep things to a steady state and things are predictable and then you can sort of just\n",
      "[02:21:18.640 --> 02:21:22.400]  like your body just sort of like sticks to that and if you try to stress that a little too much it\n",
      "[02:21:22.400 --> 02:21:26.000]  will create you know when you're traveling and you're dealing with jet lag you're not able to\n",
      "[02:21:26.000 --> 02:21:32.080]  really ascend to you know where you need to go yeah yeah that's what you're doing with humans with\n",
      "[02:21:32.080 --> 02:21:37.120]  the habits and stuff uh what are your thoughts on work life balance throughout a human lifetime\n",
      "[02:21:37.600 --> 02:21:44.720]  so Tesla in part was known for sort of pushing people to their limits in terms of what they're\n",
      "[02:21:44.720 --> 02:21:50.080]  able to do in terms of what they're trying to do in terms of how much they work all that kind of\n",
      "[02:21:50.080 --> 02:21:55.600]  stuff yeah I mean I will say Tesla gets old too much bad rep for this because what's happening\n",
      "[02:21:55.600 --> 02:22:01.360]  is Tesla is it's a bursty environment so I would say the baseline my only point of reference is\n",
      "[02:22:01.360 --> 02:22:05.600]  Google where I've interned three times and I saw what it's like inside Google and deep mind\n",
      "[02:22:06.560 --> 02:22:10.800]  I would say the baseline is higher than that but then there's a punctually equilibrium where\n",
      "[02:22:10.800 --> 02:22:15.440]  once in a while there's a fire and someone like people work really hard and so that it's\n",
      "[02:22:15.440 --> 02:22:19.840]  spiky and bursty and then all the stories get collected about the bursts yeah and then it gives\n",
      "[02:22:19.840 --> 02:22:24.800]  the appearance of like total insanity but actually it's just a bit more intense environment and there\n",
      "[02:22:24.800 --> 02:22:31.440]  are fires and sprints and so I think you know it definitely though I I would say it's a more intense\n",
      "[02:22:31.440 --> 02:22:35.520]  environment than something you would get like in your person forget all that just in your own\n",
      "[02:22:35.520 --> 02:22:43.040]  personal life what do you think about the happiness of a human being a brilliant person like yourself\n",
      "[02:22:43.760 --> 02:22:50.480]  about finding a balance between work and life or is it such a thing not a good thought experiment\n",
      "[02:22:51.760 --> 02:22:58.000]  yeah I think I think balance is good but I also love to have sprints that are out of distribution\n",
      "[02:22:58.640 --> 02:23:05.920]  and that's when I think I've been pretty creative and as well so sprints out of distribution means\n",
      "[02:23:05.920 --> 02:23:13.200]  that most of the time you you have a yeah quote unquote balance I have balance most of the time I like\n",
      "[02:23:13.200 --> 02:23:18.000]  being obsessed with something once in a while once in a while is what once a week once a month once\n",
      "[02:23:18.000 --> 02:23:21.920]  a year yeah probably like say once a month or something yeah and that's when we get and you get\n",
      "[02:23:21.920 --> 02:23:26.080]  hub repo for months yeah that's when you're like really care about a problem it must exist this will be\n",
      "[02:23:26.080 --> 02:23:30.640]  awesome you're obsessed with it and now you can't just do it on that day you need to pay the fixed\n",
      "[02:23:30.640 --> 02:23:35.520]  cost of getting into the groove and then you need to stay there for a while and then society will come\n",
      "[02:23:35.520 --> 02:23:39.280]  and they will try to mess with you and they will try to distract you yeah the worst thing is like a\n",
      "[02:23:39.280 --> 02:23:44.400]  person who's like I just need five minutes of your time yeah this is the cost of that is not five\n",
      "[02:23:44.400 --> 02:23:51.280]  minutes and society needs to change how it thinks about just five minutes of your time right it's\n",
      "[02:23:51.280 --> 02:23:56.800]  never it's never just one minute just 30 just just the quick deal are you being so yeah no\n",
      "[02:23:58.720 --> 02:24:04.480]  what's your computer setup what what's like the perfect do you I use somebody that's flexible\n",
      "[02:24:04.480 --> 02:24:12.720]  to no matter what laptop four screens yeah or do you prefer a certain setup that you're most productive\n",
      "[02:24:13.520 --> 02:24:20.240]  I guess the one that I'm familiar with is one large screen 27 inch and my laptop on the side\n",
      "[02:24:20.320 --> 02:24:26.400]  what operating system I do max that's my primary for all tasks I would say OSX but when you're working\n",
      "[02:24:26.400 --> 02:24:31.120]  on deep learning everything is Linux your SSH into a cluster and you're working remotely but what\n",
      "[02:24:31.120 --> 02:24:35.920]  about the actual development like they're using the IDE yeah you would use I think a good way is\n",
      "[02:24:35.920 --> 02:24:41.600]  you just run VS code my favorite editor right now on your Mac but you are actually you have a\n",
      "[02:24:41.600 --> 02:24:46.960]  remote folder through SSH so the actual files that you're manipulating are on the clusters and\n",
      "[02:24:46.960 --> 02:24:55.760]  some more else so what's the best IDE VS code what else the people so I use eMax still let's go\n",
      "[02:24:57.440 --> 02:25:03.280]  it may be cool I don't know if it's maximum productivity so what do you recommend in terms\n",
      "[02:25:03.280 --> 02:25:10.560]  of editors you work the lot of software engineers editors for Python C++ machine learning applications\n",
      "[02:25:11.280 --> 02:25:17.280]  I think the current answers VS code currently I believe that's the best IDE it's got a huge\n",
      "[02:25:17.280 --> 02:25:23.600]  amount of extensions it has a GitHub co-pilot integration which I think is very valuable what do you\n",
      "[02:25:23.600 --> 02:25:28.880]  think about the co-pilot integration I was actually I got to talk a bunch with Guida Ronson was\n",
      "[02:25:28.880 --> 02:25:36.800]  the creative Python and he loves co-pilot he like he programs a lot with it yeah do you yeah use\n",
      "[02:25:36.800 --> 02:25:41.840]  co-pilot I love it and it's free for me but I would pay for it yeah I think it's very good and the\n",
      "[02:25:41.840 --> 02:25:46.160]  utility that I found with it was isn't isn't I would say there is a learning curve and you need to\n",
      "[02:25:46.880 --> 02:25:50.880]  figure out when it's helpful and when to pay attention to its outputs and when it's not going to be\n",
      "[02:25:50.880 --> 02:25:54.400]  helpful where you should not pay attention to it because if you're just reading its suggestions\n",
      "[02:25:54.400 --> 02:25:57.920]  all the time it's not a good way of interacting with it but I think I was able to sort of like mold\n",
      "[02:25:57.920 --> 02:26:03.840]  myself to it I find it's very helpful number one in a copy paste and replace some parts so I don't\n",
      "[02:26:04.400 --> 02:26:08.640]  when the pattern is clear it's really good at completing the pattern and number two sometimes\n",
      "[02:26:08.640 --> 02:26:14.320]  it suggests APIs that I'm not aware of so it tells you about something that you didn't know so\n",
      "[02:26:14.320 --> 02:26:17.920]  and that's an opportunity to discover and you know it's an opportunity to so I would never take\n",
      "[02:26:17.920 --> 02:26:23.360]  co-pilot code as given I almost always copy a copy paste into a Google search and you see what this\n",
      "[02:26:23.360 --> 02:26:27.120]  function is doing and then you're like oh it's actually actually exactly what I need thank you\n",
      "[02:26:27.120 --> 02:26:32.400]  co-pilot so you learned something so it's in part a search engine apart maybe getting the\n",
      "[02:26:32.400 --> 02:26:38.080]  exact syntax correctly that once you see it yeah it's that and be hard things again once you see\n",
      "[02:26:38.080 --> 02:26:42.960]  it you know yes exactly correct exactly you yourself you can very strong you can verify\n",
      "[02:26:42.960 --> 02:26:48.000]  efficiently but you can't generate efficiently and co-pilot really I mean it's it's autopilot for\n",
      "[02:26:48.000 --> 02:26:52.880]  programming right and currently is doing the link following which is like the simple copy paste\n",
      "[02:26:52.880 --> 02:26:57.920]  and sometimes suggest but over time it's going to become more and more autonomous and so the same\n",
      "[02:26:57.920 --> 02:27:02.240]  thing will play out and not just coding but actually across many many different things probably\n",
      "[02:27:02.240 --> 02:27:07.200]  coding is an important one right writing programs yeah what how do you see the future of that\n",
      "[02:27:07.200 --> 02:27:12.320]  developing the program synthesis like being able to write programs that are more and more\n",
      "[02:27:12.320 --> 02:27:19.440]  complicated because right now it's human supervised in interesting ways yes like what it feels like\n",
      "[02:27:19.440 --> 02:27:24.640]  the transition will be very painful my mental model for it is the same thing will happen to us with\n",
      "[02:27:24.640 --> 02:27:30.240]  the autopilot so currently is doing link following is doing some simple stuff and eventually we'll\n",
      "[02:27:30.240 --> 02:27:35.280]  be doing autonomy and people will have to intervene less and less and there could be like you like\n",
      "[02:27:35.280 --> 02:27:41.200]  testing mechanisms like if the rights of function and that function looks pretty damn correct\n",
      "[02:27:41.200 --> 02:27:45.520]  but how do you know it's correct because you're like getting lazy and lazy as a programmer\n",
      "[02:27:46.080 --> 02:27:50.560]  like your ability to because like little bugs but I guess it won't make little\n",
      "[02:27:50.560 --> 02:27:56.240]  noise will it it copilot will make off by one subtle bugs it has done that to me but do you think\n",
      "[02:27:56.240 --> 02:28:02.960]  future systems will or is it really the off by one is actually a fundamental challenge of programming\n",
      "[02:28:02.960 --> 02:28:07.680]  in that case it wasn't fundamental and I think things can improve but yeah I think humans have\n",
      "[02:28:07.680 --> 02:28:12.560]  to supervise I am nervous about people not supervising what comes out and what happens to for\n",
      "[02:28:12.560 --> 02:28:16.640]  example the proliferation of bugs in all of our systems I'm nervous about that but I think\n",
      "[02:28:17.120 --> 02:28:21.040]  there will probably be some other copilates for bug finding and stuff like that at some point\n",
      "[02:28:21.040 --> 02:28:28.800]  because there will be like a lot more automation for man so it's like a program a copilot that\n",
      "[02:28:28.800 --> 02:28:35.520]  generates a compiler one that does a linter yes one that does like a type checker yeah\n",
      "[02:28:37.520 --> 02:28:42.160]  it's a committee of like a GPT serve like and then there will be like a manager for the committee\n",
      "[02:28:42.800 --> 02:28:46.880]  and then there will be somebody that says a new version of this is needed we need to regenerate it\n",
      "[02:28:46.880 --> 02:28:51.120]  yeah there were 10 GPT's they were forwarded and gave 50 suggestions and other one looked at it\n",
      "[02:28:51.120 --> 02:28:55.840]  and picked a few that they like a bug one looked at it and it was like it's probably a bug they got\n",
      "[02:28:55.840 --> 02:29:00.880]  re-ranked by some other thing and then a final ensemble GPT comes in it's like okay given\n",
      "[02:29:00.880 --> 02:29:05.280]  everything you guys have told me this is probably the next token you know the feeling is the number\n",
      "[02:29:05.280 --> 02:29:08.640]  of programmers in the world has been growing and growing very quickly yeah do you think it's\n",
      "[02:29:08.640 --> 02:29:14.400]  possible that it'll actually level out and drop to like a very low number with this kind of world\n",
      "[02:29:14.400 --> 02:29:20.880]  because then you'll be doing software 2.0 programming and you'll be doing this kind of\n",
      "[02:29:21.680 --> 02:29:28.960]  generation of copilot type systems programming but you won't be doing the old school software 1.0\n",
      "[02:29:28.960 --> 02:29:32.800]  programming I don't currently think that they're just going to replace human programmers\n",
      "[02:29:32.880 --> 02:29:39.680]  um it's I'm so hesitant saying stuff like this right because this is going to be replacing five\n",
      "[02:29:39.680 --> 02:29:45.280]  years I know it's going to show that like this is where we thought because I agree with you but\n",
      "[02:29:45.840 --> 02:29:53.600]  I think it might be very surprised right like what what are the next I what's your sense of\n",
      "[02:29:53.600 --> 02:29:57.600]  where we stand with language models like does it feel like the beginning or the middle or the\n",
      "[02:29:57.600 --> 02:30:01.840]  end the beginning hundred percent I think the big question in my mind is for sure GPT will be able\n",
      "[02:30:01.840 --> 02:30:06.960]  to program quite well competently and so on how do you steer the system you still have to provide\n",
      "[02:30:06.960 --> 02:30:11.600]  some guidance to what you actually are looking for and so how do you steer it and how do you say how do\n",
      "[02:30:11.600 --> 02:30:17.600]  you talk to it how do you audit it and verify that what is done is correct and how do you like work\n",
      "[02:30:17.600 --> 02:30:24.800]  with this and it's as much not just an AI problem but a UI UX problem yeah um so beautiful fertile\n",
      "[02:30:24.800 --> 02:30:30.080]  ground for so much interesting work uh for VS code plus plus where you're not just this not just\n",
      "[02:30:30.080 --> 02:30:34.640]  humid programming anymore it's amazing yeah so you're interacting with the system so not just one\n",
      "[02:30:34.640 --> 02:30:39.520]  prompt but it's iterative prompting yeah you're trying to figure out having a conversation with the\n",
      "[02:30:39.520 --> 02:30:44.320]  system yeah that actually I mean to me that's super exciting to have a conversation with the program\n",
      "[02:30:44.320 --> 02:30:49.040]  I'm writing yeah yeah maybe at some point you're just conversing with it's like okay here's what I\n",
      "[02:30:49.040 --> 02:30:54.560]  want to do actually this variable maybe it's not even that low level is variable but you can also\n",
      "[02:30:54.560 --> 02:31:00.480]  imagine like can you translate this to C++ and back to Python and back to it kind of existence\n",
      "[02:31:00.480 --> 02:31:05.440]  no but just like doing it as part of the program experience like I think I'd like to write this\n",
      "[02:31:05.440 --> 02:31:11.440]  function in C++ or like you just keep changing for different uh different programs because\n",
      "[02:31:11.440 --> 02:31:17.040]  of different syntax syntax maybe I want to convert this into a functional language yeah and so like\n",
      "[02:31:17.040 --> 02:31:23.120]  you get to become multilingual as a programmer and dance back and forth efficiently yeah I mean I\n",
      "[02:31:23.120 --> 02:31:27.280]  think the UI X you X of it though is like still very hard to think through because it's not just\n",
      "[02:31:27.280 --> 02:31:32.160]  about writing code on a page you have an entire developer environment you have a bunch of hardware\n",
      "[02:31:32.160 --> 02:31:35.760]  on it uh you have some environmental variables you have some scripts that are running in the\n",
      "[02:31:35.760 --> 02:31:41.200]  Chrome job like there's a lot going on to like working with computers and how do these uh systems\n",
      "[02:31:42.000 --> 02:31:46.960]  setup environment flags and work across multiple machines and setup screen sessions and automate\n",
      "[02:31:46.960 --> 02:31:51.840]  different processes like how all that works and is auditable by humans and so on is like massive\n",
      "[02:31:51.840 --> 02:31:58.880]  question no mom you've built archive sanity what is archive and what is the future of academic\n",
      "[02:31:58.880 --> 02:32:04.080]  research publishing that you would like to see uh so archive is this pre-print server so if you have\n",
      "[02:32:04.080 --> 02:32:08.720]  a paper you can submit it for publication to journals or conferences and then wait six months and\n",
      "[02:32:08.720 --> 02:32:14.000]  then maybe get a decision pass or fail or you can just upload it to archive and then people can\n",
      "[02:32:14.000 --> 02:32:18.080]  tweet about it three minutes later and then everyone sees it everyone reads it and everyone can\n",
      "[02:32:18.080 --> 02:32:24.080]  profit from it uh in their own way and you can cite it and it has an official look to it it feels\n",
      "[02:32:24.080 --> 02:32:28.800]  like a public like it feels like a publication process yeah it feels different than if you just\n",
      "[02:32:28.800 --> 02:32:34.080]  put it in a blog post oh yeah yeah i mean it's a paper and usually the the bar is higher for something\n",
      "[02:32:34.080 --> 02:32:38.240]  that you would expect on archive as opposed to and something you would see in a blog post or the\n",
      "[02:32:38.240 --> 02:32:43.760]  culture created the bar because you could probably yes host a pretty crappy fixer in the archive\n",
      "[02:32:44.320 --> 02:32:49.920]  um so what was that make you feel like what was that make you feel about peer review so rigorous\n",
      "[02:32:49.920 --> 02:32:58.160]  peer review by two three experts versus the peer review of the community right as it's written yeah\n",
      "[02:32:58.160 --> 02:33:02.880]  basically i think the community is very well able to peer review things very quickly on twitter\n",
      "[02:33:03.840 --> 02:33:07.840]  and i think maybe it just has to do something with AI machine learning fields specifically though\n",
      "[02:33:07.840 --> 02:33:14.080]  i feel like things are more easily auditable and the verification is easier potentially than\n",
      "[02:33:14.080 --> 02:33:18.640]  the verification somewhere else so it's kind of like um you can think of these scientific\n",
      "[02:33:18.640 --> 02:33:21.760]  publications as like little blockchains where everyone's building on each other's work and setting\n",
      "[02:33:21.760 --> 02:33:26.320]  each other and you sort of have AI which is kind of like this much faster and loose blockchain\n",
      "[02:33:26.960 --> 02:33:32.800]  but then you have and any one individual entry is like very very cheap to make and then you have\n",
      "[02:33:32.800 --> 02:33:38.320]  other fields where maybe that model doesn't make as much sense um and so i think in AI at least things\n",
      "[02:33:38.320 --> 02:33:42.240]  are pretty easily very viable and so that's why when people upload papers there are a really good\n",
      "[02:33:42.240 --> 02:33:47.440]  idea on someone people can try it out like the next day and they can be the final arbiter of whether\n",
      "[02:33:47.440 --> 02:33:51.920]  it works or not on their problem and the whole thing just moves significantly faster so i kind of\n",
      "[02:33:51.920 --> 02:33:56.640]  feel like academia still has a place so this like conference journal process still has a place but it's\n",
      "[02:33:56.640 --> 02:34:02.400]  sort of like and um it lacks behind i think and it's a bit more um maybe higher quality process\n",
      "[02:34:03.040 --> 02:34:07.680]  but it's not sort of the place where you will discover cutting edge work anymore yeah it used to be\n",
      "[02:34:07.680 --> 02:34:11.440]  the case when i was starting my PhD that you go to conferences and journals and you discuss all\n",
      "[02:34:11.440 --> 02:34:15.600]  the latest research now when you go to conference or journal like no one discusses anything that's\n",
      "[02:34:15.600 --> 02:34:20.960]  there because it's already like three generations ago irrelevant yeah it's which makes me sad about\n",
      "[02:34:21.040 --> 02:34:25.840]  like deep mind for example where they they still they still publish in nature and these big\n",
      "[02:34:25.840 --> 02:34:30.720]  prestigious i mean there's still value i suppose to the prestigious that comes with these big venues\n",
      "[02:34:30.720 --> 02:34:36.800]  but the the result is that they they'll announce some breakthrough performance and it'll take like a\n",
      "[02:34:36.800 --> 02:34:43.680]  year to actually publish the details i mean and those details in if they were published immediately\n",
      "[02:34:43.680 --> 02:34:47.680]  wouldn't inspire the community to move in certain directions or they yeah let's speed up the rest\n",
      "[02:34:47.680 --> 02:34:52.080]  of the community but i don't know to what extent that's part of their objective function also\n",
      "[02:34:52.080 --> 02:34:57.840]  that's true just not just the prestige a little bit of the delay is uh is part yeah they certainly\n",
      "[02:34:57.840 --> 02:35:03.360]  a deep mind specifically has been um working in the regime of having slightly higher quality basically\n",
      "[02:35:03.360 --> 02:35:08.400]  process and latency and uh publishing those papers that way another question from reddit\n",
      "[02:35:09.120 --> 02:35:14.240]  do you or have you suffered from imposter syndrome being the director of a aitessa\n",
      "[02:35:15.040 --> 02:35:20.400]  uh being this person when you're a stanford where like the world looks at you as the expert in\n",
      "[02:35:20.960 --> 02:35:27.040]  ai to teach yeah teach the world about machine learning when i was leaving tesla after five years\n",
      "[02:35:27.040 --> 02:35:32.320]  i spent a ton of time in meeting rooms uh and you know i would read papers in the beginning when i\n",
      "[02:35:32.320 --> 02:35:35.840]  joined tesla i was writing code and then i was writing less and less code and i was reading code\n",
      "[02:35:35.840 --> 02:35:39.600]  and then i was reading less and less code and so this is just a natural progression that happens i\n",
      "[02:35:40.080 --> 02:35:44.080]  and uh definitely i would say near the tail end that's when it sort of like starts to hit you a bit\n",
      "[02:35:44.080 --> 02:35:48.240]  more that you're supposed to be an expert but actually the source of truth is the code that people\n",
      "[02:35:48.240 --> 02:35:52.960]  are writing the github and the actual the actual code itself uh and you're not as familiar with that\n",
      "[02:35:52.960 --> 02:35:57.520]  as you used to be and so i would say maybe there's something like insecurity there yeah that's\n",
      "[02:35:57.520 --> 02:36:01.840]  actually pretty profound that a lot of the insecurity has to do with not writing the code in the\n",
      "[02:36:01.840 --> 02:36:06.080]  computer science space like that because that is the truth that that that that right there the code is\n",
      "[02:36:06.080 --> 02:36:11.360]  the source of truth the papers and everything else it's a high level summary i don't uh yeah\n",
      "[02:36:11.360 --> 02:36:14.800]  just a high level summary but at the end of the day you have to read code it's impossible to translate all\n",
      "[02:36:14.800 --> 02:36:20.720]  that code into actual uh you know uh paper form uh so when when things come out especially when they\n",
      "[02:36:20.720 --> 02:36:24.960]  have a source code available that's my favorite place to go so like i said you're one of the greatest\n",
      "[02:36:24.960 --> 02:36:33.680]  teachers of machine learning a i ever uh from cs231n to today what advice would you give to beginners\n",
      "[02:36:33.680 --> 02:36:40.640]  interested in getting into machine learning beginners are often focused on like what to do and i\n",
      "[02:36:40.640 --> 02:36:45.280]  think the focus should be more like how much you do so i i am kind of like believer on the high level\n",
      "[02:36:45.280 --> 02:36:49.840]  in this uh 10,000 hours kind of concept where you just kind of have to just pick the things where you\n",
      "[02:36:49.840 --> 02:36:53.520]  can spend time and you you care about and you're interested in you literally have to put in 10,000\n",
      "[02:36:53.520 --> 02:36:58.320]  hours of work um it doesn't even like matter as much like where you put it and you're you'll iterate\n",
      "[02:36:58.320 --> 02:37:02.240]  and you'll improve and you'll waste some time i don't know if there's a better way you need to put\n",
      "[02:37:02.240 --> 02:37:05.600]  in 10,000 hours but i think it's actually really nice because i feel like there's some sense of\n",
      "[02:37:05.600 --> 02:37:10.880]  the terminism about uh being an expert at a thing if you spend 10,000 hours you can literally\n",
      "[02:37:10.880 --> 02:37:15.680]  pick an arbitrary thing and i think if you spend 10,000 hours of deliberate effort and work you\n",
      "[02:37:15.680 --> 02:37:22.800]  actually will become an expert at it and so i think it's kind of like a nice thought um and so uh\n",
      "[02:37:22.800 --> 02:37:28.160]  basically i would focus more on like are you spending 10,000 hours and i focus on so and then thinking\n",
      "[02:37:28.240 --> 02:37:33.760]  about what kind of mechanisms maximize your likelihood of getting to 10,000 hours exactly which\n",
      "[02:37:33.760 --> 02:37:39.680]  for us silly humans means probably forming a daily habit of like every single day actually doing\n",
      "[02:37:39.680 --> 02:37:43.840]  the thing whatever helps you so i just think to a large extent is a psychological problem for yourself\n",
      "[02:37:44.640 --> 02:37:48.880]  one other thing that i help that i think is helpful for the psychology of it is many times people\n",
      "[02:37:48.880 --> 02:37:53.840]  compare themselves to others in the area i think it's very harmful only compare yourself to you from\n",
      "[02:37:53.840 --> 02:37:58.800]  some time ago i'll like say a year ago are you better than you year ago this is the only way to think\n",
      "[02:37:59.360 --> 02:38:03.760]  um and i think this then you can see your progress and it's very motivating that's so\n",
      "[02:38:03.760 --> 02:38:09.680]  interesting that focus on the quantity of hours because i think a lot of people uh in the beginner\n",
      "[02:38:09.680 --> 02:38:18.320]  stage but actually throughout get paralyzed by uh the choice like which one do i pick this path\n",
      "[02:38:18.320 --> 02:38:23.120]  or this path yeah like they'll literally get paralyzed by like which i de to use well they're worried\n",
      "[02:38:23.200 --> 02:38:27.360]  yeah they'll worry about all these things but the thing is some of the you you will waste time\n",
      "[02:38:27.360 --> 02:38:31.520]  doing something wrong yes you will eventually figure out it's not right you will accumulate scar tissue\n",
      "[02:38:31.520 --> 02:38:35.920]  and next time you'll grow stronger because next time you'll have the scar tissue and next time you'll\n",
      "[02:38:35.920 --> 02:38:41.520]  learn from it and now next time you come to a similar situation you'll be like oh i i messed up\n",
      "[02:38:41.520 --> 02:38:45.600]  i've spent a lot of time working on things that never materialize into anything and i have all\n",
      "[02:38:45.600 --> 02:38:49.680]  that scar tissue and i have some intuitions about what was useful what wasn't useful how things turned\n",
      "[02:38:50.320 --> 02:38:54.960]  out so all those mistakes where we're not dead work you know so i just think you should\n",
      "[02:38:54.960 --> 02:38:58.240]  did you just focus on working what have you done what have you done last week\n",
      "[02:39:00.480 --> 02:39:04.400]  that's a good question actually to ask for for a lot of things that's just machine learning\n",
      "[02:39:05.440 --> 02:39:10.960]  it's a good way to cut the the way i forgot what the term we used but the fluff the blubber whatever\n",
      "[02:39:10.960 --> 02:39:18.720]  the the inefficiencies in life what do you love about teaching you seem to find yourself\n",
      "[02:39:19.520 --> 02:39:23.760]  often in the like draw into teaching you're very good at it but you're also drawn to it i mean i don't\n",
      "[02:39:23.760 --> 02:39:31.680]  think i love teaching i love happy humans and happy humans like when i teach yes i wouldn't say i\n",
      "[02:39:31.680 --> 02:39:35.920]  hate teaching i tolerate teaching but it's not like the act of teaching that i like it's it's that\n",
      "[02:39:36.720 --> 02:39:42.480]  you know i i have some i have something i'm actually okay at it yes i'm okay at teaching and people\n",
      "[02:39:42.560 --> 02:39:49.120]  appreciate a lot yeah and so i'm just happy to try to be helpful and teaching itself is not like the\n",
      "[02:39:49.120 --> 02:39:53.760]  most i mean it's really it's can be really annoying frustrating i was working on a bunch of lectures\n",
      "[02:39:53.760 --> 02:39:58.880]  just now i was reminded back to my days of 231 and just how much work it is to create some of these\n",
      "[02:39:58.880 --> 02:40:03.200]  materials and make them good the amount of iteration and thought and you go down blind alleys and\n",
      "[02:40:03.200 --> 02:40:08.960]  just how much you change it so creating something good in terms of like educational values really hard\n",
      "[02:40:09.680 --> 02:40:15.360]  and it's not fun it was difficult so for people should definitely go watch your new stuff you\n",
      "[02:40:15.360 --> 02:40:20.720]  put out their lectures we actually building the thing like from like you said the code is truth\n",
      "[02:40:20.720 --> 02:40:26.240]  so discussing uh back propagation by building it by looking through and just the whole thing so\n",
      "[02:40:26.240 --> 02:40:29.360]  how difficult is that to prepare for i think it's a really powerful way to teach\n",
      "[02:40:30.000 --> 02:40:35.280]  how did you have to prepare for that or are you just live thinking through it i will typically do\n",
      "[02:40:35.360 --> 02:40:40.000]  like say three takes and then i take like the the better take uh so i do multiple takes now i\n",
      "[02:40:40.000 --> 02:40:43.680]  take some of the better takes and then i just build out a lecture that way uh sometimes i have to\n",
      "[02:40:43.680 --> 02:40:48.000]  delete 30 minutes of content because it just went down the alley that i didn't like too much there's\n",
      "[02:40:48.000 --> 02:40:53.040]  a bunch of iteration and it probably takes me you know somewhere around 10 hours to create one hour of\n",
      "[02:40:53.040 --> 02:40:58.800]  content to do one hour it's interesting i mean uh is it difficult to go back to the like the basics\n",
      "[02:40:58.800 --> 02:41:03.680]  do you draw a lot of like wisdom from going back to the basics yeah going back to back propagation\n",
      "[02:41:03.680 --> 02:41:07.760]  loss functions where they come from and one thing i like about teaching a lot honestly is it\n",
      "[02:41:07.760 --> 02:41:12.880]  definitely strengthens your understanding uh so it's not a purely altruistic activity it's a way\n",
      "[02:41:12.880 --> 02:41:19.360]  to learn if you have to explain something to someone uh you realize you have gaps in knowledge uh\n",
      "[02:41:19.360 --> 02:41:24.560]  and so i even surprised myself in those lectures like also the result will obviously look at this\n",
      "[02:41:24.560 --> 02:41:28.160]  and then the result doesn't look like it and i'm like okay i thought i understood this yeah\n",
      "[02:41:28.320 --> 02:41:34.640]  so that's why it's really cool to literally code you run it in the notebook and it gives you\n",
      "[02:41:34.640 --> 02:41:39.760]  an result and you're like oh wow yes and like actual numbers actual input actual actual actual code\n",
      "[02:41:39.760 --> 02:41:44.960]  yeah it's not mathematical symbols etc the source of truth is the code it's not slides it's just like\n",
      "[02:41:44.960 --> 02:41:50.160]  let's build it it's beautiful you're a rare human in that sense uh what advice would you give to\n",
      "[02:41:50.160 --> 02:41:56.800]  researchers uh trying to develop and publish idea that have a big impact in the world of AI\n",
      "[02:41:56.800 --> 02:42:03.920]  so maybe um undergrads maybe early graduate students yeah i'm gonna say like they definitely\n",
      "[02:42:03.920 --> 02:42:08.880]  have to be a little bit more strategic than i had to be as a PhD student because of the way AI is\n",
      "[02:42:08.880 --> 02:42:13.840]  evolving it's going the way of physics where you know in physics you used to be able to do experiments\n",
      "[02:42:13.840 --> 02:42:18.000]  when you're bench top and everything was great and you could make progress and now you have to work\n",
      "[02:42:18.000 --> 02:42:24.880]  in like LHC or like CERN and and so AI is going in that direction as well um so there's certain kinds\n",
      "[02:42:25.360 --> 02:42:31.440]  of things that's just not possible to do on the bench top anymore and uh i think um that didn't\n",
      "[02:42:31.440 --> 02:42:37.680]  used to be the case at the time you still think that there's like ganty papers to be written\n",
      "[02:42:38.400 --> 02:42:44.240]  where like uh like very simple idea there requires just one computer to illustrate a simple example\n",
      "[02:42:44.240 --> 02:42:48.640]  i mean one example that's been very influential recently is diffusion models the fusion models are\n",
      "[02:42:48.640 --> 02:42:53.760]  amazing the fusion models are six-year-old uh for the longest time people were kind of ignoring them\n",
      "[02:42:53.840 --> 02:42:58.800]  as far as i can tell and uh they're an amazing generative model especially in uh in images\n",
      "[02:42:58.800 --> 02:43:03.440]  and so stable diffusion and so on it's all the fusion based uh the fusion is new it was not there\n",
      "[02:43:03.440 --> 02:43:07.840]  and came from well it came from google but a researcher could have come up with it in fact some of\n",
      "[02:43:07.840 --> 02:43:13.120]  the first actually know those came from google as well but a researcher could come up with that\n",
      "[02:43:13.120 --> 02:43:18.000]  in an academic institution yeah what do you find most fascinating about diffusion models so\n",
      "[02:43:19.280 --> 02:43:23.520]  from the societal impact of the technical architecture what i like about the fusion is\n",
      "[02:43:23.520 --> 02:43:30.320]  it works so well was that surprising to you the amount of the variety almost the novelty of the\n",
      "[02:43:30.320 --> 02:43:37.680]  synthetic data is generating yes so the stable diffusion images are incredible it's the speed\n",
      "[02:43:37.680 --> 02:43:42.560]  of improvement in generating images has been insane uh we went very quickly from generating like\n",
      "[02:43:42.560 --> 02:43:46.960]  tiny digits to tiny faces and it all looked messed up and now we were stable diffusion and that\n",
      "[02:43:46.960 --> 02:43:51.040]  happened very quickly there's a lot that academia can still contribute uh you know for example um\n",
      "[02:43:51.680 --> 02:43:56.720]  flash attention is a very efficient kernel for running the attention operation inside the\n",
      "[02:43:56.720 --> 02:44:01.120]  transformer that came from academic environment it's a very clever way to structure the kernel\n",
      "[02:44:02.080 --> 02:44:07.520]  that do the duster calculation so it doesn't materialize the attention matrix um and so there's\n",
      "[02:44:07.520 --> 02:44:10.960]  i think there's still like lots of things to contribute but you have to be just more strategic\n",
      "[02:44:10.960 --> 02:44:17.520]  do you think neural networks can be made to reason uh yes do you think they're already reason\n",
      "[02:44:17.600 --> 02:44:22.000]  yes what's your definition of reasoning uh information processing\n",
      "[02:44:24.560 --> 02:44:28.160]  so in the way the humans think through a problem and come up with novel ideas\n",
      "[02:44:31.040 --> 02:44:35.040]  it it feels like reasoning yeah so the the novelty\n",
      "[02:44:37.280 --> 02:44:42.720]  i don't want to say but out of out of distribution ideas you think it's possible\n",
      "[02:44:43.280 --> 02:44:46.240]  yes and i think we're seeing that already in the current neural nets\n",
      "[02:44:46.240 --> 02:44:50.960]  you're able to remix the training set information into true generalization in some sense\n",
      "[02:44:50.960 --> 02:44:55.280]  that doesn't appear it doesn't appear in the way in the training set like you're doing something\n",
      "[02:44:55.280 --> 02:45:00.080]  interesting algorithmically you're manipulating you know some symbols and you're coming up with some\n",
      "[02:45:01.200 --> 02:45:08.480]  correct unique answer and then you said thing what would uh illustrate to you holy shit this\n",
      "[02:45:08.480 --> 02:45:14.400]  thing is definitely thinking to me thinking or reasoning is just information processing and\n",
      "[02:45:14.480 --> 02:45:19.600]  generalization and i think the neural nets already do that today so being able to perceive the world\n",
      "[02:45:19.600 --> 02:45:27.680]  or perceive the whatever the inputs are and to make predictions based on that or action based\n",
      "[02:45:27.680 --> 02:45:31.680]  on that that's that's the reason yeah you're giving correct answers in novel settings\n",
      "[02:45:32.880 --> 02:45:37.440]  uh by manipulating information you've learned the correct algorithm you're not doing just some kind\n",
      "[02:45:37.440 --> 02:45:42.720]  of a lookup table on the earth neighbor search let me ask you about a GI what what are some\n",
      "[02:45:42.720 --> 02:45:48.880]  moonshirt ideas you think might make significant progress towards a GI so maybe another way is\n",
      "[02:45:49.440 --> 02:45:55.440]  what are big blockers that we're missing now so basically i am fairly bullish on our ability to\n",
      "[02:45:55.440 --> 02:46:02.160]  build a GI's uh basically automated systems that we can interact with and are very human like\n",
      "[02:46:02.160 --> 02:46:07.440]  and we can interact with them in a digital realm or a physical realm currently it seems most of the\n",
      "[02:46:07.440 --> 02:46:14.880]  models that sort of do these sort of magical tasks are in a text realm um i think uh as i mentioned\n",
      "[02:46:14.880 --> 02:46:20.560]  i'm suspicious that text realm is not enough to actually build full understanding of the world i do\n",
      "[02:46:20.560 --> 02:46:25.280]  actually think you need to go into pixels and understand the physical world and how it works so i do\n",
      "[02:46:25.280 --> 02:46:30.000]  think that we need to extend these models to consume images and videos and train on a lot more data\n",
      "[02:46:30.000 --> 02:46:35.040]  that is multimodal in that way do you think you need to touch the world to understand it also well\n",
      "[02:46:35.040 --> 02:46:39.680]  that's the big open question i would say in my mind is if you also require the embodiment and the\n",
      "[02:46:39.680 --> 02:46:45.520]  ability to uh sort of interact with the world run experiments and um have a data of that form then\n",
      "[02:46:45.520 --> 02:46:50.880]  you need to go to optimus or something like that and so i would say optimus in some way is like a hedge\n",
      "[02:46:52.640 --> 02:46:59.200]  in a GI because it seems to me that it's possible that just having data from the internet is not\n",
      "[02:47:00.160 --> 02:47:06.720]  if that is the case then optimus may lead to a GI because optimus would to me there's nothing\n",
      "[02:47:06.720 --> 02:47:11.200]  beyond optimus you have like this humanoid form factor that can actually like do stuff in the world\n",
      "[02:47:11.200 --> 02:47:16.400]  you kind of millions of them interacting with humans and so on and uh if that doesn't give a rise\n",
      "[02:47:16.400 --> 02:47:22.080]  to a GI at some point like i'm not sure what will um so from a completeness perspective i think\n",
      "[02:47:22.080 --> 02:47:27.680]  that's the uh that's a really good platform but it's a much more harder platform because uh you are\n",
      "[02:47:27.680 --> 02:47:32.480]  dealing with atoms and you need to actually like build these things and integrate them into society\n",
      "[02:47:32.480 --> 02:47:38.160]  so i think that path takes longer uh but it's much more certain and then there's a path of the internet\n",
      "[02:47:38.160 --> 02:47:43.680]  and just like training these compression models effectively uh on uh trying to compress all the internet\n",
      "[02:47:44.800 --> 02:47:50.080]  and uh that might also give um these agents as well compressed the internet but also interact\n",
      "[02:47:50.080 --> 02:47:57.120]  with the internet yeah so it's not obvious to me in fact i suspect you can reach a GI without\n",
      "[02:47:57.120 --> 02:48:04.160]  ever entering the physical world and which is a little bit more uh concerning because\n",
      "[02:48:05.360 --> 02:48:11.840]  it might that results in it happening faster so it just feels like we're like in boiling water\n",
      "[02:48:11.840 --> 02:48:19.040]  we won't know as it's happening i i would like to i'm not afraid of a GI i'm excited about it\n",
      "[02:48:19.040 --> 02:48:25.920]  there's always concerns but i would like to know when it happens yeah or and have like hints about\n",
      "[02:48:25.920 --> 02:48:30.720]  when it happens like a year from now it will happen that kind of thing yeah i just feel like in the\n",
      "[02:48:30.720 --> 02:48:35.520]  digital realm it just might happen yeah i think all we have available to us because no one has built\n",
      "[02:48:35.520 --> 02:48:42.480]  a GI again so all we have available to us is uh is there enough fertile ground on the periphery i\n",
      "[02:48:42.480 --> 02:48:47.680]  would say yes and we have the progress so far which has been very rapid and uh there are next steps\n",
      "[02:48:47.680 --> 02:48:53.040]  that are available and so i would say uh yeah it's quite likely that will be interacting with digital\n",
      "[02:48:53.040 --> 02:48:58.640]  entities how do you know that we somebody is built a GI it's going to be a slow i think it's going to be\n",
      "[02:48:58.640 --> 02:49:02.400]  a slow incremental transition it's going to be product based and focused it's going to be get up\n",
      "[02:49:02.400 --> 02:49:08.160]  copilot getting better and then uh GPT's helping you write and then these oracles that you can go to\n",
      "[02:49:08.160 --> 02:49:13.360]  with mathematical problems i think we're on a on a verge of being able to ask very complex\n",
      "[02:49:14.400 --> 02:49:18.640]  questions in chemistry physics math uh of these oracles and have them complete solutions\n",
      "[02:49:18.720 --> 02:49:26.800]  so a GI to use primarily focused on intelligence so consciousness doesn't enter into uh into it\n",
      "[02:49:27.600 --> 02:49:32.240]  so in my mind consciousness is not a special thing you will you will figure out and bolt on i think\n",
      "[02:49:32.240 --> 02:49:37.840]  it's an emergent phenomenon of a large enough and complex enough um generative model sort of\n",
      "[02:49:38.320 --> 02:49:45.120]  so um if you have a complex enough world model uh that understands the world then it also understands\n",
      "[02:49:45.200 --> 02:49:50.240]  its predicament in the world as being a language model which to me is a form of consciousness or\n",
      "[02:49:50.240 --> 02:49:55.040]  self awareness and so in order to understand the world deeply you probably have to integrate\n",
      "[02:49:55.040 --> 02:50:00.960]  yourself into the world yeah and in order to interact with humans and other living beings consciousness\n",
      "[02:50:00.960 --> 02:50:07.600]  is a very useful tool yeah i think consciousness is like a modeling insight modeling insight yeah it's\n",
      "[02:50:07.600 --> 02:50:12.320]  a you have a powerful enough model of understanding the world that you actually understand that you are\n",
      "[02:50:12.320 --> 02:50:17.520]  an entity in it yeah but there's also this um perhaps just the narrative we tell ourselves there's\n",
      "[02:50:17.520 --> 02:50:22.960]  if it feels like something to experience the world the hard problem of consciousness yeah but that\n",
      "[02:50:22.960 --> 02:50:27.040]  could be just the narrative that we tell ourselves yeah i don't think well yeah i think it will emerge\n",
      "[02:50:27.040 --> 02:50:31.440]  i think it's going to be something uh very boring like we'll be talking to these uh digital\n",
      "[02:50:31.440 --> 02:50:35.840]  AI's they will claim their conscious they will appear conscious they will do all the things that\n",
      "[02:50:35.840 --> 02:50:41.040]  you would expect of other humans and uh it's going to just be a stalemate i i think there would be a\n",
      "[02:50:41.040 --> 02:50:48.800]  lot of actual fascinating ethical questions like supreme court level questions of whether you're\n",
      "[02:50:48.800 --> 02:50:56.480]  allowed to turn off a conscious AI if you're allowed to build the conscious AI maybe there would have\n",
      "[02:50:56.480 --> 02:51:03.360]  to be the same kind of the base that you have around um sorry to bring up a political topic but\n",
      "[02:51:03.440 --> 02:51:12.080]  you know abortion uh which is the deeper question with abortion is what is life and the deep\n",
      "[02:51:12.080 --> 02:51:18.640]  question with AI is also what is life and what is conscious and i think they'll be very fascinating\n",
      "[02:51:19.760 --> 02:51:27.920]  to bring up you might become illegal to build systems that are capable like of such level of\n",
      "[02:51:27.920 --> 02:51:32.080]  intelligence that consciousness would emerge and therefore the capacity to suffer would emerge\n",
      "[02:51:32.160 --> 02:51:37.440]  and somebody a system that says no please don't kill me um well that's what the lambda compute\n",
      "[02:51:37.440 --> 02:51:42.560]  the lambda chatbot already told um this google engineer right like it's it was talking about not\n",
      "[02:51:42.560 --> 02:51:50.240]  wanting to die or so on so that might become illegal to do that right right because otherwise\n",
      "[02:51:50.240 --> 02:51:56.000]  you might have a lot a lot of creatures that don't want to die and they will uh you can just spawn\n",
      "[02:51:56.240 --> 02:52:02.800]  into your summon cluster and then that might lead to like horrible consequences then there might be a\n",
      "[02:52:02.800 --> 02:52:07.680]  lot of people that secretly love murder and they'll start practicing murder and those systems i mean\n",
      "[02:52:07.680 --> 02:52:13.920]  there's just i to me all of this stuff just brings a beautiful mirror to the human condition\n",
      "[02:52:13.920 --> 02:52:19.680]  and human nature will get to explore it and that's what like the best of uh the supreme court of\n",
      "[02:52:19.680 --> 02:52:24.320]  all the different debates we have about ideas of what it means to be human we get to ask those deep\n",
      "[02:52:24.320 --> 02:52:29.360]  questions that would be asking throughout human history there's always been the other in human\n",
      "[02:52:29.360 --> 02:52:35.600]  history uh where the good guys and that's the bad guys and we're going to uh you know throughout human\n",
      "[02:52:35.600 --> 02:52:40.960]  history let's murder the bad guys and the same will probably happen with robots it'll be the other\n",
      "[02:52:40.960 --> 02:52:45.120]  at first and then we'll get to ask questions of what does it mean to be alive what does it mean to\n",
      "[02:52:45.120 --> 02:52:50.000]  be conscious yeah and i think there's some canary in the coal mines even with what we have today i'm\n",
      "[02:52:50.080 --> 02:52:54.160]  and uh you know like for example these there's these like wifu's that you get like work with and some\n",
      "[02:52:54.160 --> 02:52:58.560]  people are trying to like this company's going to shut down but this person really like yeah love\n",
      "[02:52:58.560 --> 02:53:03.360]  their wifu and like it's trying to like port it somewhere else and like it's not possible and like\n",
      "[02:53:04.000 --> 02:53:10.800]  i think like definitely uh people will have feelings towards uh towards these um systems because\n",
      "[02:53:10.800 --> 02:53:15.920]  in some sense they are like a mirror of humanity because they are like sort of like a big average\n",
      "[02:53:16.000 --> 02:53:21.360]  of humanity in a way that it's trained but we can that average we can actually watch\n",
      "[02:53:22.000 --> 02:53:27.200]  it's nice to be able to interact with the big average of humanity and do like a search query on it\n",
      "[02:53:27.200 --> 02:53:32.560]  yeah yeah it's very fascinating and uh we can also of course also like shape it it's not just a pure\n",
      "[02:53:32.560 --> 02:53:36.400]  average we can mess with the training data we can mess with the objective we can fine tune them in\n",
      "[02:53:36.400 --> 02:53:43.280]  various ways uh so we have some um you know impact on what those systems look like if you want to\n",
      "[02:53:43.280 --> 02:53:50.960]  achieve a g i um and you could uh have a conversation with her and ask her uh talk about anything maybe\n",
      "[02:53:50.960 --> 02:53:55.280]  ask her a question what what kind of stuff would you would you ask i would have some practical questions\n",
      "[02:53:55.280 --> 02:54:00.960]  one mind like uh do i or my loved ones really have to die uh what can we do about that\n",
      "[02:54:02.800 --> 02:54:08.160]  do you think it will answer clearly or would it answer poetically i would expect it to give\n",
      "[02:54:08.160 --> 02:54:12.480]  solutions i would expect it to be like well i've read all of these textbooks and i know all these\n",
      "[02:54:12.480 --> 02:54:15.760]  things that you've produced and it seems to me like here are the experiments that i think it would be\n",
      "[02:54:15.760 --> 02:54:20.960]  useful to run next and here's some gene therapies that i think would be helpful and uh here are the\n",
      "[02:54:20.960 --> 02:54:26.480]  kinds of experiments that you should run okay let's go with the thought experiment okay imagine that\n",
      "[02:54:27.440 --> 02:54:34.640]  mortality is actually uh pre like our pre-requisite for happiness so if we become immortal\n",
      "[02:54:34.640 --> 02:54:40.480]  we'll actually become deeply unhappy and the model is able to know that so what is it supposed to\n",
      "[02:54:40.480 --> 02:54:45.280]  tell you stupid human about it yes you can become immortal but you will become deeply unhappy\n",
      "[02:54:46.160 --> 02:54:52.640]  if the if the model is if the agi system is trying to empathize with you human what is it supposed to\n",
      "[02:54:52.640 --> 02:54:58.560]  tell you that yes you don't have to die but you're really not going to like it because it's going\n",
      "[02:54:58.560 --> 02:55:05.440]  to be deeply honest like there's a interstellar what is it the AI says like humans want 90% honesty\n",
      "[02:55:05.920 --> 02:55:12.640]  hmm so like you have to pick how honest i want to answer these quite practical questions yeah i love\n",
      "[02:55:12.640 --> 02:55:16.880]  AI interstellar by the way i think it's like such a sidekick to the entire story but\n",
      "[02:55:17.920 --> 02:55:22.240]  at the same time it's like really interesting it's kind of limited in certain ways right\n",
      "[02:55:22.240 --> 02:55:26.160]  yeah it's limited and i think that's totally fine by the way i don't think uh i think it's\n",
      "[02:55:27.040 --> 02:55:30.080]  fine and plausible to have a limited and imperfect agis\n",
      "[02:55:30.240 --> 02:55:38.160]  is that the feature almost as an example like it has a fixed mind of compute on its physical body\n",
      "[02:55:38.160 --> 02:55:43.920]  and it might just be that even though you can have a super amazing mega brain super intelligent AI\n",
      "[02:55:43.920 --> 02:55:49.440]  you can also kind of like you know less intelligent AI so you can deploy in a power efficient way\n",
      "[02:55:49.440 --> 02:55:54.560]  and then they're not perfect they might make mistakes no i meant more like say you had infinite compute\n",
      "[02:55:55.200 --> 02:56:00.000]  and it's still good to make mistakes sometimes like in order to integrate yourself like um\n",
      "[02:56:01.040 --> 02:56:04.320]  what is it going back to goodwill hunting robin Williams character\n",
      "[02:56:05.040 --> 02:56:10.320]  says like the human imperfections that's good stuff right yeah isn't it isn't that the\n",
      "[02:56:10.880 --> 02:56:17.760]  like we don't want perfect we want flaws in part to form connected with each other because it\n",
      "[02:56:17.760 --> 02:56:24.000]  feels like something you can attach your feelings to the the the flaws and in that same way you\n",
      "[02:56:24.000 --> 02:56:29.520]  want to AI that's flawed i don't know i feel like perfectionist but then you're saying okay yeah\n",
      "[02:56:29.520 --> 02:56:35.360]  but that's not agi but see agi would need to be intelligent enough to give answers to humans\n",
      "[02:56:35.360 --> 02:56:40.400]  that humans don't understand and i think perfect isn't something humans can't understand because\n",
      "[02:56:40.400 --> 02:56:47.280]  even science doesn't give perfect answers there's always gabs and mysteries and i don't know i i don't\n",
      "[02:56:47.280 --> 02:56:53.120]  know humans want perfect yeah i can imagine just uh having a conversation with this kind of\n",
      "[02:56:53.120 --> 02:56:59.440]  oracle entity as you'd imagine them and uh yeah maybe it can tell you about you know based on\n",
      "[02:56:59.440 --> 02:57:04.400]  my analysis of human condition uh you might not want this and hear some of the things that might\n",
      "[02:57:04.800 --> 02:57:12.240]  might but every every dumb human will say yeah yeah trust me i can give me the truth i can handle it\n",
      "[02:57:12.240 --> 02:57:15.360]  but that's the beauty like people can choose uh so but then\n",
      "[02:57:17.760 --> 02:57:21.360]  the old marshmallow test with the kids and so on i feel like too many people\n",
      "[02:57:22.320 --> 02:57:28.080]  uh like you can't handle the truth probably including myself like the deep truth of the human\n",
      "[02:57:28.080 --> 02:57:33.040]  condition i don't i don't know if i can handle it like what if there's some darks that what\n",
      "[02:57:33.040 --> 02:57:38.000]  what if we are an alien science experiment and it realizes that what if it hacked i mean yeah\n",
      "[02:57:38.720 --> 02:57:45.920]  i mean this is the matrix you know the matrix again i don't know i would what would i talk about\n",
      "[02:57:45.920 --> 02:57:52.640]  i don't even yeah i probably i'll go with the safe for scientific questions at first that\n",
      "[02:57:52.640 --> 02:57:59.440]  have nothing to do with my own personal life yeah immortality just like about physics and so on\n",
      "[02:57:59.440 --> 02:58:04.720]  yeah to build up like let's see where is that or maybe see if it has a sense of humor that's\n",
      "[02:58:04.720 --> 02:58:10.160]  another question would it be able to presumably in order to if it understands humans deeply would\n",
      "[02:58:10.160 --> 02:58:17.760]  able to generate uh yeah to generate humor yeah i think that's actually a wonderful benchmark\n",
      "[02:58:17.760 --> 02:58:22.480]  almost like is it able i think that's a really good point basically to it to make you laugh yeah\n",
      "[02:58:22.480 --> 02:58:26.000]  if it's able to be like a very effective stand-up comedian that is doing something very interesting\n",
      "[02:58:26.000 --> 02:58:34.240]  computationally i think being funny is extremely hard yeah because it's hard in a way like a\n",
      "[02:58:34.240 --> 02:58:40.240]  touring test the original intent of the touring test is hard because you have to convince humans\n",
      "[02:58:40.240 --> 02:58:46.480]  and there's nothing that's why that's why when you comedian talk about this like there's this is\n",
      "[02:58:46.480 --> 02:58:51.280]  deeply honest because if people can't help but laugh and if they don't laugh that means you're not\n",
      "[02:58:51.280 --> 02:58:55.760]  funny if they laugh yeah it's funny and you're showing you need a lot of knowledge to create to create\n",
      "[02:58:55.760 --> 02:58:59.280]  humor about like the argument you mentioned human condition and so on and then you need to be clever\n",
      "[02:58:59.360 --> 02:59:05.360]  with it uh you mentioned a few movies you tweeted movies that i've seen five plus times but\n",
      "[02:59:06.160 --> 02:59:11.840]  i'm ready and willing to keep watching interstellar gladiator contact good will hunting the matrix\n",
      "[02:59:11.840 --> 02:59:18.160]  load of the rings all three avatar fifth element so on it goes on term day two mean girls i'm not\n",
      "[02:59:18.160 --> 02:59:26.000]  gonna ask about that i think i think girls is great um what are some of the jump onto your memory that\n",
      "[02:59:26.000 --> 02:59:32.960]  you love in why like you mentioned the matrix as a as a computer person why do you love the matrix\n",
      "[02:59:34.320 --> 02:59:38.240]  there's so many properties that make it like beautiful and interesting so there's all these philosophical\n",
      "[02:59:38.240 --> 02:59:44.000]  questions but then there's also a g is and there's simulation and it's cool and there's you know\n",
      "[02:59:44.000 --> 02:59:50.160]  the black you know the look of it the feel of it the feel of it the action the bullet time it was\n",
      "[02:59:50.400 --> 02:59:56.640]  just like innovating in so many ways and then uh good well good will hunting way like that one\n",
      "[02:59:57.440 --> 03:00:04.000]  yeah i just i really like this uh torture genius sort of character who's like grappling with whether\n",
      "[03:00:04.000 --> 03:00:08.880]  or not he has like any responsibility or like what to do with this gift that he was given or like\n",
      "[03:00:08.880 --> 03:00:14.480]  how to think about the whole thing and uh there's also dance between the genius and the the\n",
      "[03:00:14.480 --> 03:00:19.440]  personal like what it means to love another human being and there's a lot of things there it's just\n",
      "[03:00:19.440 --> 03:00:24.160]  a beautiful movie and then the fatherly figure the mentor in in the in the psychiatrist and the\n",
      "[03:00:24.160 --> 03:00:28.480]  it like really like uh it messes with you you know there's some movies that's just like really mess\n",
      "[03:00:28.480 --> 03:00:36.160]  with you uh on a deep level do you relate to that movie at all no it's not your fault as i said\n",
      "[03:00:36.880 --> 03:00:44.080]  lore the rings that's self-explanatory termator two which is interesting you rewatch that a lot\n",
      "[03:00:44.160 --> 03:00:50.080]  is that better than termator one you like you like Arnold i do like terminator one as well uh i\n",
      "[03:00:50.080 --> 03:00:53.360]  like terminator two a little bit more but in terms of like its surface properties\n",
      "[03:00:55.760 --> 03:01:03.920]  do you think sky net is it all a possibility uh yes like the actual sort of autonomous weapon\n",
      "[03:01:03.920 --> 03:01:10.000]  system kind of thing do you worry about that stuff i do worry i being useful war i 100%\n",
      "[03:01:10.000 --> 03:01:15.200]  worry about it and so the i mean the you know some of these uh fears of ag is and how this will plan\n",
      "[03:01:15.200 --> 03:01:19.760]  out i mean these will be like very powerful entities probably at some point and so um for a long\n",
      "[03:01:19.760 --> 03:01:23.440]  time there are going to be tools in the hands of humans uh you know people talk about like alignment\n",
      "[03:01:23.440 --> 03:01:30.000]  of ag is and how to make the problem is like even humans are not aligned uh so uh how this will be\n",
      "[03:01:30.000 --> 03:01:35.840]  used and what this is going to look like is um yes troubling so do you think it'll happen so\n",
      "[03:01:35.840 --> 03:01:41.920]  slowly enough that we'll be able to as a as a human civilization think through the problems yes\n",
      "[03:01:41.920 --> 03:01:46.560]  that's my hope is that it happens slowly enough and in an open enough way where a lot of people can\n",
      "[03:01:46.560 --> 03:01:51.200]  see and participate in it just uh figure out how to deal with this transition i think which it\n",
      "[03:01:51.200 --> 03:01:56.480]  was kind of interesting i draw a lot of inspiration from nuclear weapons because i sure thought it\n",
      "[03:01:56.480 --> 03:02:04.080]  would be it would be fucked once they develop nuclear weapons but like it's almost like uh when uh\n",
      "[03:02:04.160 --> 03:02:09.040]  when the systems are not so dangerous they destroy human civilization we deploy them and learn\n",
      "[03:02:09.040 --> 03:02:14.480]  the lessons and then we quickly if it's too dangerous we'll quickly quickly we might still deploy\n",
      "[03:02:15.200 --> 03:02:19.520]  uh but you very quickly learn not to use them and so there'll be like this balance achieved\n",
      "[03:02:19.520 --> 03:02:25.360]  humans are very clever as a species it's interesting we uh exploit the resources as much as we can\n",
      "[03:02:25.360 --> 03:02:30.720]  but we don't we avoid destroying ourselves it seems like well i don't know about that actually\n",
      "[03:02:30.720 --> 03:02:36.720]  i hope it continues um i mean i'm definitely like concerned about nuclear weapons and so on\n",
      "[03:02:36.720 --> 03:02:42.000]  not just as a result of the recent conflict even before that uh that's probably my number one\n",
      "[03:02:42.000 --> 03:02:49.760]  concern for the society so if humanity uh destroys itself or destroys you know 90 percent\n",
      "[03:02:49.760 --> 03:02:55.840]  of people that would be because of nukes i think so um and it's not even about full destruction to\n",
      "[03:02:55.840 --> 03:03:00.400]  me it's bad enough if we reset society that would be like terrible it would be really bad and i\n",
      "[03:03:00.400 --> 03:03:06.080]  can't believe we're like so close to it yeah it's like so crazy to me it feels like we might be\n",
      "[03:03:06.080 --> 03:03:12.320]  a few tweets away from something like that yeah basically it's extremely unnerving but and has been\n",
      "[03:03:12.320 --> 03:03:23.360]  for me for a long time it seems unstable that world leaders just having a bad mood can like um take one\n",
      "[03:03:23.360 --> 03:03:30.400]  step towards a bad direction and it escalates yeah and because of a collection of bad moods it\n",
      "[03:03:30.400 --> 03:03:38.000]  can escalate without being able to uh stop yeah it's just a huge amount of uh power and then also\n",
      "[03:03:38.000 --> 03:03:42.480]  with the proliferation i basically i don't i don't actually really see i don't actually know what\n",
      "[03:03:42.480 --> 03:03:48.080]  the good outcomes are here uh so i'm definitely worried about that a lot and then a g i is not currently\n",
      "[03:03:48.160 --> 03:03:54.560]  there but i think at some point will more and more become something like it the danger with a g i\n",
      "[03:03:54.560 --> 03:04:00.720]  even is that i think it's even less likely worse in a sense that uh there are good outcomes of a g i\n",
      "[03:04:01.200 --> 03:04:07.200]  and then the bad outcomes are like an epsilon away like a tiny one away and so i think um capitalism\n",
      "[03:04:07.200 --> 03:04:12.480]  and humanity and so on will drive for the positive uh ways of using that technology but then if bad\n",
      "[03:04:12.480 --> 03:04:18.240]  outcomes are just like a tiny like flip a minus sign away uh that's a really bad position to be in\n",
      "[03:04:18.240 --> 03:04:24.080]  a tiny perturbation of the system results in the destruction of the human species so weird\n",
      "[03:04:24.080 --> 03:04:27.920]  line to walk yeah i think in general it was really weird about like the dynamics of humanity and\n",
      "[03:04:27.920 --> 03:04:32.240]  this explosion was talked about it's just like being saying coupling afforded by technology\n",
      "[03:04:32.240 --> 03:04:37.920]  yeah and uh just the instability of the whole dynamical system i think it's just doesn't look good\n",
      "[03:04:38.080 --> 03:04:43.520]  honestly yes that explosion could be destructive or constructive and the probabilities are non-zero\n",
      "[03:04:43.520 --> 03:04:48.960]  in both yeah i'm gonna have to i do feel like i have to try to be optimistic and so on and yes\n",
      "[03:04:48.960 --> 03:04:52.400]  i think even in this case i still am predominantly optimistic but there's definitely\n",
      "[03:04:53.680 --> 03:05:00.240]  me too uh do you think we'll become a multi-point area species probably yes but i don't know if it's\n",
      "[03:05:00.240 --> 03:05:06.880]  dominant feature of uh future humanity uh there might be some people on some planets and so on but\n",
      "[03:05:06.880 --> 03:05:12.720]  i'm not sure if it's like yeah if it's like a major player in our culture and so on we still have\n",
      "[03:05:12.720 --> 03:05:18.480]  to solve the drivers of self-destruction here on earth so just having a backup on Mars is not\n",
      "[03:05:18.480 --> 03:05:22.960]  gonna solve the problem so by the way i love the backup on Mars i think that's amazing we should\n",
      "[03:05:22.960 --> 03:05:29.040]  absolutely do that yes and i'm so thankful uh would you uh would you go to Mars uh personally\n",
      "[03:05:29.040 --> 03:05:34.800]  know i do like earth quite a lot okay uh i'll go to Mars i'll go feed that's i'll tweet at you from\n",
      "[03:05:35.280 --> 03:05:39.840]  maybe eventually i would once it's safe enough but i don't actually know if it's on my lifetime\n",
      "[03:05:39.840 --> 03:05:45.280]  scale unless i can extend it by a lot i do think that for example a lot of people might disappear into\n",
      "[03:05:46.080 --> 03:05:50.160]  virtual realities and stuff like that and i think that could be the major thrust of um\n",
      "[03:05:50.160 --> 03:05:55.520]  sort of the cultural development of humanity if it survives uh so it might not be it's just really\n",
      "[03:05:55.520 --> 03:06:00.320]  hard to work in physical realm and go out there and i think ultimately all your experiences are in\n",
      "[03:06:00.320 --> 03:06:06.560]  your brain yeah and so it's much easier to disappear into digital realm and i think people will\n",
      "[03:06:06.560 --> 03:06:12.320]  find them more compelling easier safer more interesting so you're a little bit captivated by virtual\n",
      "[03:06:12.320 --> 03:06:16.960]  reality by the possible worlds whether it's the metaverse or some other manifestation of that yeah\n",
      "[03:06:18.160 --> 03:06:25.120]  yeah it's really interesting and so um i'm interested just just talking a lot to karma\n",
      "[03:06:25.200 --> 03:06:31.040]  where's the where's the thing that's currently preventing that yeah i'm to be clear i think what's\n",
      "[03:06:31.040 --> 03:06:37.840]  interesting about the future is um it's not that i kind of feel like the variance in the human\n",
      "[03:06:37.840 --> 03:06:42.320]  condition grows that's the primary thing that's changing it's not as much the mean of the\n",
      "[03:06:42.320 --> 03:06:45.760]  distribution is like the variance of it so there will probably be people in Mars and there will be\n",
      "[03:06:45.760 --> 03:06:50.080]  people in VR and there will people here on earth it's just like there will be so many more ways of being\n",
      "[03:06:50.880 --> 03:06:55.040]  and so i kind of feel like i see it as like a spreading out of a human experience there's something\n",
      "[03:06:55.040 --> 03:07:00.000]  about the internet that allows you to discover those little groups and you gravitate to something\n",
      "[03:07:00.000 --> 03:07:04.480]  about your biology likes that kind of world that you find each other yeah and we'll have transhumanists\n",
      "[03:07:04.480 --> 03:07:08.240]  and then we'll have the omnis and they're gonna everything is just gonna coexist yeah the cool thing\n",
      "[03:07:08.240 --> 03:07:13.360]  about it because i've interacted with a bunch of internet communities is um they don't know\n",
      "[03:07:13.920 --> 03:07:19.120]  about each other like you can have a very happy existence just like having a very close\n",
      "[03:07:19.120 --> 03:07:23.440]  neck community and not knowing about each other i mean even you even sense this just having\n",
      "[03:07:23.440 --> 03:07:30.240]  traveled to Ukraine there's they they don't know so many things about america yeah you you like\n",
      "[03:07:30.240 --> 03:07:33.840]  when you travel across the world i think you experience this too there are certain cultures\n",
      "[03:07:33.840 --> 03:07:38.880]  that are like they have their own thing going on they don't and so you you can see that happening\n",
      "[03:07:38.880 --> 03:07:43.200]  more and more and more and more in the future we have little communities yeah yeah i think so that\n",
      "[03:07:43.200 --> 03:07:47.920]  seems to be the that seems to be how it's going right now and i don't see that trend like really\n",
      "[03:07:47.920 --> 03:07:52.720]  reversing i think people are diverse and they're able to choose their own like path and existence\n",
      "[03:07:52.720 --> 03:07:58.240]  and i sort of like celebrate that um and so we use been something much time in the metaverse in the\n",
      "[03:07:58.240 --> 03:08:06.160]  virtual reality or which community area i use the physicalist the the the the physical reality\n",
      "[03:08:06.160 --> 03:08:12.320]  enjoyer or uh do you see drawing a lot of uh pleasure and fulfillment in the digital world\n",
      "[03:08:13.280 --> 03:08:18.000]  yeah i think well currently the virtual reality is not that compelling uh i do think it can\n",
      "[03:08:18.000 --> 03:08:23.120]  improve a lot but i don't really know to what extent maybe you know there's actually like even more\n",
      "[03:08:23.120 --> 03:08:29.360]  exotic things you can think about with like neural links or stuff like that so um currently i kind of\n",
      "[03:08:29.360 --> 03:08:35.520]  see myself as mostly a team human person i love nature yeah i love harmony i love people i love\n",
      "[03:08:35.520 --> 03:08:41.920]  humanity i love emotions of humanity um and i i just want to be like in this like solar\n",
      "[03:08:41.920 --> 03:08:47.440]  punk little utopia that's my happy place yes my happy place is like uh people i love thinking about\n",
      "[03:08:47.440 --> 03:08:53.760]  cool problems surround by a lush beautiful dynamic nature yeah and secretly high tech in places that\n",
      "[03:08:53.760 --> 03:09:00.880]  count places like uh the use technology to empower that love for other humans and nature yeah i think\n",
      "[03:09:00.880 --> 03:09:05.760]  a technology used like very sparingly uh i don't love when it sort of gets in the way of humanity in\n",
      "[03:09:05.760 --> 03:09:11.840]  many ways uh i like just people being humans in a way we sort of like slightly evolved and prefer\n",
      "[03:09:11.840 --> 03:09:16.480]  i think just by default people kept asking me because they they know you love reading are there\n",
      "[03:09:16.480 --> 03:09:24.800]  particular books that you enjoyed that had an impact on you for silly or for profound reasons\n",
      "[03:09:24.800 --> 03:09:31.120]  that you would recommend you mentioned the vital question many of course i think in biology as an\n",
      "[03:09:31.120 --> 03:09:36.320]  example the vital question is a good one anything by Nikolaine really uh life ascending i would say\n",
      "[03:09:36.320 --> 03:09:43.040]  is like a bit more potentially uh representative is like a summary of a lot of the things he's been\n",
      "[03:09:43.040 --> 03:09:47.680]  talking about i was very impacted by the selfish gene i thought that was a really good book that\n",
      "[03:09:47.680 --> 03:09:51.680]  helped me understand altruism as an example and where it comes from and just realizing that\n",
      "[03:09:51.680 --> 03:09:55.520]  you know the selection is on a level of genes was a huge insight for me at the time and it sort of\n",
      "[03:09:55.600 --> 03:10:01.440]  cleared up a lot of things for me what do you think about the the idea that ideas are the organisms\n",
      "[03:10:01.440 --> 03:10:09.040]  the meats love it 100% are you able to walk around with that notion for a while that\n",
      "[03:10:10.000 --> 03:10:14.640]  that there is an evolutionary kind of process with ideas as well there absolutely is there's memes\n",
      "[03:10:14.640 --> 03:10:20.480]  just like genes and they compete and they live in our brains it's beautiful are we silly humans\n",
      "[03:10:20.560 --> 03:10:25.840]  thinking that we're the organisms is it possible that the primary organisms are the ideas\n",
      "[03:10:27.040 --> 03:10:31.280]  yeah i would say like the the idea is kind of live in the software of like our civilization\n",
      "[03:10:32.000 --> 03:10:37.200]  in the in the minds and so on we think as humans that the hardware is the fundamental thing\n",
      "[03:10:37.840 --> 03:10:45.680]  i human is a hardware entity but it could be the software right yeah yeah i would say like there\n",
      "[03:10:45.680 --> 03:10:50.240]  needs to be some grounding at some point to like a physical reality but if we clone an Andre\n",
      "[03:10:51.440 --> 03:10:58.320]  the software is a thing like is this thing that makes that thing special right yeah i guess i\n",
      "[03:10:58.320 --> 03:11:02.320]  you're right but then cloning might be exceptionally difficult like there might be a deep\n",
      "[03:11:02.320 --> 03:11:06.560]  integration between the software and the hardware in ways we don't quite understand well from the\n",
      "[03:11:06.560 --> 03:11:11.040]  old shim from a view like what makes me special is more like the the gang of genes that are\n",
      "[03:11:11.040 --> 03:11:15.520]  writing in my chromosomes i suppose right like they're they're they're duplicating unit i suppose\n",
      "[03:11:16.000 --> 03:11:23.040]  no but that's just for the cute the thing that makes you special sure well the reality is\n",
      "[03:11:23.840 --> 03:11:31.360]  what makes you special is your ability to survive based on the software that runs on the hardware\n",
      "[03:11:31.360 --> 03:11:36.560]  that was built by the genes so the software is the thing that makes you survive not the hardware\n",
      "[03:11:37.200 --> 03:11:41.280]  all right it's a little bit of both you know it's just like a second layer it's a new second layer\n",
      "[03:11:41.280 --> 03:11:45.280]  that hasn't been there before the brain they both they both coexist but there's also layers of the\n",
      "[03:11:45.280 --> 03:11:52.400]  software i mean it's it's not there it's a it's a abstraction at the top of abstractions but okay\n",
      "[03:11:52.400 --> 03:11:58.960]  so so soft as gene and the plane i would say sometimes books are like not sufficient i like to reach\n",
      "[03:11:58.960 --> 03:12:04.320]  for textbooks sometimes i kind of feel like books are for too much of a general consumption\n",
      "[03:12:04.320 --> 03:12:08.960]  sometime and they just kind of like they're too high up in a lot of abstraction and it's not good\n",
      "[03:12:08.960 --> 03:12:15.040]  enough yeah so i like textbooks i like the cell i think the cell is pretty cool that's why\n",
      "[03:12:15.040 --> 03:12:21.120]  also i like the writing of uh necklain is because he's pretty willing to step one level down\n",
      "[03:12:21.120 --> 03:12:27.120]  and he doesn't uh yeah he sort of he's willing to go there uh but he's also willing to sort of be\n",
      "[03:12:27.120 --> 03:12:31.200]  throughout the stack so he'll go down to a lot of detail but then he will come back up and\n",
      "[03:12:31.200 --> 03:12:36.560]  i think he is a yeah basically i really appreciate that this way i love college early college\n",
      "[03:12:36.560 --> 03:12:42.000]  even high school just textbooks on the basics uh yeah i've computer science and mathematics of\n",
      "[03:12:42.480 --> 03:12:49.760]  biology of chemistry yes those are they condense down like uh it's sufficiently general they\n",
      "[03:12:49.760 --> 03:12:55.040]  can understand both the philosophy and the details but also like you get homework problems and you\n",
      "[03:12:55.040 --> 03:13:00.240]  you get to play with it as much as you would if you were in yeah programming stuff yeah and then i'm\n",
      "[03:13:00.240 --> 03:13:04.880]  also suspicious of textbooks honestly because as an example in deep learning uh there's no like\n",
      "[03:13:04.880 --> 03:13:08.800]  amazing textbooks and i feel this changing very quickly i imagine the same as true and say\n",
      "[03:13:09.600 --> 03:13:14.400]  synthetic biology and so on these books like the cell are kind of outdated they're still high level\n",
      "[03:13:14.400 --> 03:13:18.880]  like what is the actual real source of truth it's people in wetlabs working with cells\n",
      "[03:13:18.880 --> 03:13:25.840]  yeah you know sequencing genomes and yeah actually working with working with it and uh i don't have\n",
      "[03:13:25.840 --> 03:13:30.080]  that much exposure to that or what that looks like so i still don't fully i'm reading through the cell\n",
      "[03:13:30.080 --> 03:13:33.600]  and it's kind of interesting and i'm learning but it's still not sufficient i would say in terms of\n",
      "[03:13:33.680 --> 03:13:39.600]  understanding what it's a clean summarization of the mainstream narrative yeah but you have to\n",
      "[03:13:39.600 --> 03:13:44.640]  learn that before you break out yeah at the towards the cutting edge yeah but what is the actual\n",
      "[03:13:44.640 --> 03:13:48.720]  process of working with the cells and growing them and incubating them and you know it's kind of\n",
      "[03:13:48.720 --> 03:13:52.560]  like a massive cooking recipes of making sure your self-slows and proliferate and then you're\n",
      "[03:13:52.560 --> 03:13:56.960]  sequencing them running experiments and uh just how that works i think is kind of like the source of\n",
      "[03:13:56.960 --> 03:14:02.000]  truth of at the end of the day what's really useful in terms of creating therapies and so on yeah\n",
      "[03:14:02.000 --> 03:14:06.800]  i wonder what in the future AI textbooks will be because you know there's artificial intelligence\n",
      "[03:14:06.800 --> 03:14:11.920]  the modern approach i actually haven't read if it's come out the recent version the recent there's\n",
      "[03:14:11.920 --> 03:14:16.880]  been a recent addition i also saw there's a signs of deep learning book i'm waiting for textbooks\n",
      "[03:14:16.880 --> 03:14:23.280]  that worth recommending worth reading yeah it's it's tricky because it's like papers and code code\n",
      "[03:14:23.280 --> 03:14:28.240]  code honestly i think papers are quite good i especially like the appendix appendix of any paper as\n",
      "[03:14:28.240 --> 03:14:35.360]  well it's like it's like the most detail you can have it doesn't have to be cohesive connected\n",
      "[03:14:35.360 --> 03:14:40.000]  anything else you just described me very specific ways of the particular thing yeah many times papers\n",
      "[03:14:40.000 --> 03:14:44.080]  can be actually quite readable not always but sometimes the introduction and the abstract is readable\n",
      "[03:14:44.080 --> 03:14:48.480]  even for someone outside of the field uh not this is not always true and sometimes i think\n",
      "[03:14:48.480 --> 03:14:53.840]  unfortunately scientists use complex terms even when it's not necessary i think that's harmful\n",
      "[03:14:53.920 --> 03:14:58.480]  i think there and there's no reason for that and papers sometimes are longer than they need to be\n",
      "[03:14:58.480 --> 03:15:04.480]  in the in the parts that don't matter yeah appendix would be long but then the paper itself\n",
      "[03:15:04.480 --> 03:15:09.120]  you know look at Einstein make it simple yeah but suddenly i've come across papers i would say\n",
      "[03:15:09.120 --> 03:15:12.560]  like synthetic biology or something that i thought were quite readable for the abstract and the\n",
      "[03:15:12.560 --> 03:15:16.160]  introduction and then you're reading the rest of it and you don't fully understand but you kind\n",
      "[03:15:16.160 --> 03:15:23.600]  of are getting a gist and i think it's cool what uh advice you give advice to folks interested\n",
      "[03:15:23.600 --> 03:15:28.080]  in machine learning and research but in general life advice to a young person high school\n",
      "[03:15:29.040 --> 03:15:34.320]  uh early college about how to have a career they can be proud of or life they can be proud of\n",
      "[03:15:35.520 --> 03:15:39.280]  yeah i think i'm very hesitant to give general advice i think it's really hard i've mentioned\n",
      "[03:15:39.280 --> 03:15:42.640]  like some of the stuff i've mentioned is fairly general i think like focus on just the\n",
      "[03:15:42.640 --> 03:15:48.000]  monotork you're spending unlike a thing uh compare yourself only to yourself not to others\n",
      "[03:15:48.000 --> 03:15:50.720]  that's good i think those are fairly general how do you pick the thing\n",
      "[03:15:51.600 --> 03:15:57.280]  uh you just have like a deep interest in something uh or like try to like find the argmax\n",
      "[03:15:57.280 --> 03:16:01.200]  over like the things that you're interested in argmax at that moment and stick with it how do you\n",
      "[03:16:01.200 --> 03:16:06.080]  now get distracted and switch to another thing uh you can if you like\n",
      "[03:16:07.680 --> 03:16:12.560]  well if you if you do an argmax repeatedly every week if those won't converge it doesn't\n",
      "[03:16:12.560 --> 03:16:16.160]  it's a problem yeah you can like low pass filter yourself uh in terms of like what has\n",
      "[03:16:16.240 --> 03:16:22.880]  consistently been true for you um but yeah i definitely see how it can be hard but i would say like\n",
      "[03:16:22.880 --> 03:16:27.120]  you're going to work the hardest on the thing that you care about the most uh so low pass filter\n",
      "[03:16:27.120 --> 03:16:31.360]  yourself and really introspect in your past where are the things that gave you energy and what are\n",
      "[03:16:31.360 --> 03:16:36.480]  the things that took energy away from you concrete examples and usually uh from those concrete\n",
      "[03:16:36.480 --> 03:16:40.720]  examples sometimes parents can emerge i like i like it when things look like this when i'm these\n",
      "[03:16:40.720 --> 03:16:44.720]  positions so that's not necessarily the field but the kind of stuff you're doing in a particular field\n",
      "[03:16:44.800 --> 03:16:50.560]  so for you it seems like you were energized by implementing still building actual things\n",
      "[03:16:50.560 --> 03:16:56.160]  yeah being low level learning and then also uh communicating so that others can go through the same\n",
      "[03:16:56.160 --> 03:17:01.200]  realization and shortening that gap um because i usually have to do way too much work to understand\n",
      "[03:17:01.200 --> 03:17:05.520]  a thing and then i'm like okay this is actually like okay i think i get it and like why was it so much\n",
      "[03:17:05.520 --> 03:17:11.040]  work it should have been much less work and that gives me a lot of frustration and that's why\n",
      "[03:17:11.120 --> 03:17:18.240]  sometimes go teach so aside from the teaching you're doing now putting out videos aside from a potential\n",
      "[03:17:19.520 --> 03:17:26.880]  godfather part two uh with the aji at tesla and beyond uh what does the future phonjakapathy hold\n",
      "[03:17:26.880 --> 03:17:33.280]  have you figured that out yet or no i mean uh as you see through the fog of war that is all of our\n",
      "[03:17:33.280 --> 03:17:39.440]  future do you do you start seeing silhouettes of what that possible future could look like\n",
      "[03:17:40.800 --> 03:17:45.040]  the consistent thing i've been always interested in for me at least is is a i and um\n",
      "[03:17:46.480 --> 03:17:50.720]  uh that's probably why i'm spending my rest of my life on because i just care about a lot\n",
      "[03:17:50.720 --> 03:17:55.360]  and i actually care about like many other problems as well like say aging which i basically view as\n",
      "[03:17:55.360 --> 03:18:01.280]  disease and uh i care about that as well but i don't think it's a good idea to go after it\n",
      "[03:18:01.280 --> 03:18:06.400]  specifically i don't actually think that humans will be able to come up with the answer i think\n",
      "[03:18:06.400 --> 03:18:10.720]  the correct thing to do is to ignore those problems and you solve a i and then use that to solve\n",
      "[03:18:10.720 --> 03:18:14.080]  everything else and i think there's a chance that this will work i think it's a very high chance\n",
      "[03:18:14.720 --> 03:18:20.240]  and uh that's kind of like the the way i'm betting at least so when you think about a i are you\n",
      "[03:18:20.240 --> 03:18:27.200]  interested in all kinds of applications yes kinds of domains and any domain you focus on will allow\n",
      "[03:18:27.200 --> 03:18:32.000]  you to get insights to the big problem of a g i yeah for me is the ultimate meta problem i don't\n",
      "[03:18:32.000 --> 03:18:35.280]  want to work on any one specific problem there's too many problems so how can you work on all\n",
      "[03:18:35.280 --> 03:18:39.760]  problems simultaneously you solve the meta problem uh which to me is just intelligence and how do\n",
      "[03:18:39.760 --> 03:18:47.200]  you uh automate it is there cool small projects like uh archived sanity and and so on that you're\n",
      "[03:18:47.200 --> 03:18:54.320]  thinking about the the the the world the ml world can anticipate there's always like some fun side\n",
      "[03:18:54.320 --> 03:18:58.720]  projects yeah um archived sanity is one uh yet basically like there's way too many archive papers\n",
      "[03:18:58.720 --> 03:19:04.720]  how can i organize it and recommend papers and so on uh i transcribed all of your yeah podcasts\n",
      "[03:19:04.720 --> 03:19:10.640]  what is you learn from that experience uh from transcribing the process of like you like consuming\n",
      "[03:19:10.640 --> 03:19:17.680]  audiobooks and and podcasts and so on and here's a process that achieves um closer to human level\n",
      "[03:19:17.680 --> 03:19:23.120]  performance and annotation yeah well i definitely was like surprised that uh transcription with\n",
      "[03:19:23.120 --> 03:19:27.840]  opening as whisperer was working so well compared to what i'm familiar with from seary and like\n",
      "[03:19:27.840 --> 03:19:34.000]  a few other systems i guess it worked so well and uh that's what gave me some energy to like try it\n",
      "[03:19:34.000 --> 03:19:39.920]  out and i thought it could be fun to run on podcasts it's kind of not obvious to me why whisper\n",
      "[03:19:39.920 --> 03:19:42.880]  is so much better compared to anything else because i feel like there should be a lot of incentive\n",
      "[03:19:42.880 --> 03:19:46.560]  for a lot of companies to produce transcription systems and that they've done so over a long time\n",
      "[03:19:46.560 --> 03:19:52.480]  whisperer is not a super exotic model it's a transformer it takes male spectrograms and you know\n",
      "[03:19:52.560 --> 03:19:57.280]  just outputs tokens of text it's not crazy uh the model and everything has been around for a long time\n",
      "[03:19:58.320 --> 03:20:03.600]  i'm not actually 100% sure why it's not obvious to me either it makes me feel like i'm missing\n",
      "[03:20:03.600 --> 03:20:08.880]  something for the missing something yeah because there is huge even google and so on youtube\n",
      "[03:20:09.600 --> 03:20:15.120]  transcription yeah yeah it's unclear but some of it is also integrating into a bigger system\n",
      "[03:20:15.760 --> 03:20:20.080]  yeah that so the user interface how's deployed and all that kind of stuff maybe\n",
      "[03:20:20.640 --> 03:20:25.440]  running it as an independent thing is eat much easier like an order magnitude easier than\n",
      "[03:20:25.440 --> 03:20:31.920]  deploying to a large integrated system like youtube transcription or anything like meetings like\n",
      "[03:20:31.920 --> 03:20:38.880]  zoom has transcription that's kind of crappy but creating interface where the text is different\n",
      "[03:20:38.880 --> 03:20:47.200]  individual speakers it's able to display it in compelling ways run it real time all that kind of\n",
      "[03:20:47.200 --> 03:20:54.080]  stuff maybe that's difficult i still the explanation i have because like i'm currently paying\n",
      "[03:20:54.080 --> 03:21:00.640]  uh quite a bit for human transcription human caption right annotation and like it seems like\n",
      "[03:21:01.920 --> 03:21:06.000]  there's a huge incentive to automate that yeah it's very confusing and i think i mean i don't know if\n",
      "[03:21:06.000 --> 03:21:10.800]  you look at some of the whisper transcripts but they're quite good they're good and especially\n",
      "[03:21:10.800 --> 03:21:17.280]  in tricky cases yeah i've seen uh whisper's performance on like super tricky cases and it\n",
      "[03:21:17.280 --> 03:21:23.520]  doesn't credibly well so i don't know apothec is pretty simple it's like high quality audio and\n",
      "[03:21:23.520 --> 03:21:30.080]  you're speaking usually pretty clearly and so i don't know it uh i don't know what open AI's\n",
      "[03:21:30.080 --> 03:21:35.600]  plans are yeah either but yeah there's always like fun fun projects basically and stable diffusion\n",
      "[03:21:35.600 --> 03:21:40.560]  also is opening up a huge amount of experimentation i would say in the visual realm and generating\n",
      "[03:21:40.560 --> 03:21:45.200]  images and videos and movies ultimately videos now and so that's going to be pretty crazy\n",
      "[03:21:45.200 --> 03:21:49.680]  uh that's going to that's going to almost certainly work and it's going to be really interesting when\n",
      "[03:21:49.680 --> 03:21:54.560]  the cost of content creation is going to fall to zero you used to need a painter for a few months\n",
      "[03:21:54.560 --> 03:22:00.000]  to paint a thing and now it's going to be speak to your phone to get your video so Hollywood will\n",
      "[03:22:00.000 --> 03:22:08.240]  start using it to generate scenes um which completely opens up yeah so you can make a like a movie like\n",
      "[03:22:08.240 --> 03:22:14.880]  avatar eventually for under a million dollars much less maybe just by talking to your phone i mean\n",
      "[03:22:14.880 --> 03:22:20.400]  i know it sounds kind of crazy and then there'd be some voting mechanism like how do you have a\n",
      "[03:22:20.400 --> 03:22:26.640]  like would there be a show on Netflix as generated completely uh automatedly so you can potentially\n",
      "[03:22:26.640 --> 03:22:32.720]  yeah and what does it look like also when you can generate it on demand and it's um and there's\n",
      "[03:22:32.720 --> 03:22:41.840]  infinity of it yeah oh man all the synthetic content i mean it's humbling because we treat ourselves\n",
      "[03:22:41.840 --> 03:22:47.280]  as special for being able to generate art and ideas and all that kind of stuff if that can be\n",
      "[03:22:47.280 --> 03:22:53.840]  done in an automated way by AI yeah i think it's fascinating to me how these uh the predictions of AI\n",
      "[03:22:53.840 --> 03:22:57.520]  and what is going to look like and what's going to be capable of are completely inverted and wrong\n",
      "[03:22:57.600 --> 03:23:02.960]  and sci-fi of 50s and 60s which is just like totally not right they imagine AI is like super\n",
      "[03:23:02.960 --> 03:23:07.520]  calculating the ear-improvers and we're getting things that can talk to you about emotions it can do\n",
      "[03:23:07.520 --> 03:23:14.880]  art it's just like weird are you excited about that feature just AI's like hybrid systems\n",
      "[03:23:15.600 --> 03:23:20.800]  heterogeneous systems of humans and AI's talking about emotions Netflix and chill with an AI\n",
      "[03:23:20.800 --> 03:23:26.960]  system that's where the netflix thing you watch is also generated by AI i think it's uh it's going\n",
      "[03:23:26.960 --> 03:23:32.640]  to be interesting for sure and i think i'm cautiously optimistic but it's not it's not obvious\n",
      "[03:23:33.520 --> 03:23:41.360]  well the sad thing is your brain and mine developed in a time where before twitter before the\n",
      "[03:23:41.840 --> 03:23:46.720]  before the internet so i wonder people that are born inside of it might have a different experience\n",
      "[03:23:47.600 --> 03:23:53.760]  like i and maybe you can still resist it and the people born now will not\n",
      "[03:23:54.560 --> 03:23:59.760]  well i do feel like humans are extremely malleable yeah and uh you're probably right\n",
      "[03:24:00.880 --> 03:24:08.960]  what is the meaning of life Andre we we talked about sort of the universe having a conversation\n",
      "[03:24:08.960 --> 03:24:16.240]  with us humans or with the systems we create to try to answer for the universe for the creator of\n",
      "[03:24:16.240 --> 03:24:22.480]  the universe to notice us we're trying to create systems that are loud enough to answer back\n",
      "[03:24:23.760 --> 03:24:27.200]  i don't know that's the meaning of life that's like meaning of life for some people the first\n",
      "[03:24:27.200 --> 03:24:31.360]  level answer i would say is anyone can choose their own meaning of life because we are conscious\n",
      "[03:24:31.360 --> 03:24:37.120]  entity and it's beautiful um number one uh but uh i do think that like a deeper meaning of life\n",
      "[03:24:37.120 --> 03:24:43.520]  as someone is interested is uh along the lines of like what the hell is all this and like why and\n",
      "[03:24:44.160 --> 03:24:48.000]  if you look at the into fundamental physics and the quantum field theory and the standard model\n",
      "[03:24:48.080 --> 03:24:54.160]  they're like way it looked very complicated and um there's this like you know 19 free parameters\n",
      "[03:24:54.160 --> 03:24:59.520]  of our universe and like what's going on with all this stuff and why is it here and can i hack it\n",
      "[03:24:59.520 --> 03:25:03.840]  can i work with it is there a message for me am i supposed to create a message and so i think\n",
      "[03:25:03.840 --> 03:25:08.320]  there's some fundamental answers there uh but i think there's actually even like you can't actually\n",
      "[03:25:08.320 --> 03:25:13.360]  like really make dent in those without more time and so to me also there's a big question\n",
      "[03:25:13.360 --> 03:25:17.680]  around just getting more time honestly yeah that's kind of like what i think about quite a bit as well\n",
      "[03:25:18.000 --> 03:25:26.160]  so kind of the ultimate or at least first way to sneak up to the why question is to try to escape\n",
      "[03:25:27.440 --> 03:25:34.880]  uh the system the universe yeah and then for that you sort of uh backtrack and say okay for that\n",
      "[03:25:34.880 --> 03:25:39.440]  that's going to be take a very long time so the why question boils down from an engineering\n",
      "[03:25:39.440 --> 03:25:43.360]  perspective to how do we extend yeah i think that's the question number one practically speaking\n",
      "[03:25:43.440 --> 03:25:48.880]  because you can't uh you're not gonna calculate the answer to the deeper questions in time you have\n",
      "[03:25:48.880 --> 03:25:53.680]  and that could be extending your own lifetime or extending just the lifetime of human civilization\n",
      "[03:25:53.680 --> 03:25:59.200]  of whoever wants to not many people might not want that uh but i think people who do want that i think\n",
      "[03:26:00.240 --> 03:26:05.600]  i think it's probably possible uh and i don't i don't know that people fully realize this i kind\n",
      "[03:26:05.600 --> 03:26:10.400]  of feel like people think of death as an inevitability but at the end of the day this is a physical\n",
      "[03:26:10.400 --> 03:26:16.240]  system something's go wrong uh it makes sense why things like this happen evolutionarily speaking\n",
      "[03:26:16.720 --> 03:26:21.680]  and uh there's most certainly interventions that uh that mitigate it i mean that'd be\n",
      "[03:26:21.680 --> 03:26:28.800]  interesting if death is eventually looked at as as uh a fascinating thing they used to happen to humans\n",
      "[03:26:28.800 --> 03:26:36.240]  i don't think it's unlikely i think it's i think it's likely and it's uh up to our imagination to\n",
      "[03:26:36.320 --> 03:26:42.560]  try to predict what the world without death looks like yes hard to i think the values will completely\n",
      "[03:26:42.560 --> 03:26:50.560]  change could be i don't i don't really buy all these ideas that oh without death there's no meaning\n",
      "[03:26:50.560 --> 03:26:56.080]  there's nothingness i i don't intuitively buy all those arguments i think there's plenty of meaning\n",
      "[03:26:56.080 --> 03:26:59.520]  plenty of things to learn they're interesting exciting i want to know i want to calculate\n",
      "[03:27:00.480 --> 03:27:05.120]  i want to improve the condition of all the humans and organisms they're alive\n",
      "[03:27:05.600 --> 03:27:10.880]  at the way we find meaning might change we there is a lot of humans probably including myself\n",
      "[03:27:10.880 --> 03:27:16.480]  that finds meaning in the finiteness of things but that doesn't mean that's the only source of meaning\n",
      "[03:27:16.480 --> 03:27:21.920]  yeah i do think many people will will go with that which i think is great i love the idea that\n",
      "[03:27:21.920 --> 03:27:27.200]  people can just choose their own adventure like you you are born as a conscious free entity by default\n",
      "[03:27:27.200 --> 03:27:34.080]  i'd like to think and um you have your amy lian bull rights for uh life uh in the pursuit of\n",
      "[03:27:34.080 --> 03:27:40.080]  happiness i don't know if you have that in the nature the landscape of happiness you can choose\n",
      "[03:27:40.080 --> 03:27:45.760]  your own adventure mostly and that's not the fully true but i still am pretty sure i'm an NPC\n",
      "[03:27:45.760 --> 03:27:54.080]  but um an NPC can't know it's an NPC there could be different degrees and levels of consciousness\n",
      "[03:27:54.080 --> 03:28:00.320]  i don't think there's a more beautiful way to end it uh andre a year an incredible person i'm\n",
      "[03:28:00.320 --> 03:28:04.000]  really honored you would talk with me everything you've done for the machine learning world\n",
      "[03:28:04.000 --> 03:28:10.320]  for the ai world to just inspire people to educate millions of people it's been it's been great and\n",
      "[03:28:10.320 --> 03:28:14.080]  i can't wait to see what you do next it's been an honor man thank you so much for talking today\n",
      "[03:28:14.080 --> 03:28:19.760]  awesome thank you thanks for listening to this conversation with adra karpathi to support this\n",
      "[03:28:19.760 --> 03:28:25.840]  podcast please check out our sponsors in the description and now let me leave you with some words\n",
      "[03:28:25.920 --> 03:28:34.160]  from samuel carlin the purpose of models is not to fit the data but to sharpen the questions\n",
      "[03:28:35.440 --> 03:28:38.080]  thanks for listening and hope to see you next time\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "import whisper\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n",
    "TEMP_DIR = Path(\"..\") / Path(\"temp\")\n",
    "TRANSCRIPT_FILE = (Path(\"..\") / Path(\"transcription.txt\"))\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TRANSCRIPT_FILE.exists():\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory(dir=TEMP_DIR) as temp_dir:\n",
    "        # Define the URL of the YouTube video\n",
    "\n",
    "        # Set up yt-dlp options\n",
    "        ydl_opts = {\n",
    "            \"verbose\": True,\n",
    "            \"format\": \"m4a/bestaudio/best\",\n",
    "            \"outtmpl\": str(Path(temp_dir) / Path(\"%(title)s.%(ext)s\" )),\n",
    "        }\n",
    "\n",
    "        # Download the audio\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            error_code = ydl.download([YOUTUBE_VIDEO])\n",
    "            if error_code != 0:\n",
    "                raise Exception(\"Error downloading the video\")\n",
    "\n",
    "        # Load whisper model\n",
    "        whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "        # Foreach audio file in the temporary directory\n",
    "        for file in Path(temp_dir).iterdir():\n",
    "            if file.suffix == \".m4a\":\n",
    "                # Transcribe the audio\n",
    "                transcription = whisper_model.transcribe(str(file), fp16=False, verbose=True)[\"text\"].strip()\n",
    "            \n",
    "            # Write the transcription to a file\n",
    "            with TRANSCRIPT_FILE.open(\"w\") as file:\n",
    "                    file.write(transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
