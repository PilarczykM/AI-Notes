{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "The context we want to send the model comes from a YouTube video.\n",
    "\n",
    "Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/index/whisper/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n",
    "TEMP_DIR = Path(\"..\") / Path(\"temp\")\n",
    "TRANSCRIPT_FILE = (Path(\"..\") / Path(\"transcription.txt\"))\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TRANSCRIPT_FILE.exists():\n",
    "    import tempfile\n",
    "    import whisper\n",
    "    from yt_dlp import YoutubeDL\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory(dir=TEMP_DIR) as temp_dir:\n",
    "        # Define the URL of the YouTube video\n",
    "\n",
    "        # Set up yt-dlp options\n",
    "        ydl_opts = {\n",
    "            \"verbose\": True,\n",
    "            \"format\": \"m4a/bestaudio/best\",\n",
    "            \"outtmpl\": str(Path(temp_dir) / Path(\"%(title)s.%(ext)s\" )),\n",
    "        }\n",
    "\n",
    "        # Download the audio\n",
    "        with YoutubeDL(ydl_opts) as ydl:\n",
    "            error_code = ydl.download([YOUTUBE_VIDEO])\n",
    "            if error_code != 0:\n",
    "                raise Exception(\"Error downloading the video\")\n",
    "\n",
    "        # Load whisper model\n",
    "        whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "        # Foreach audio file in the temporary directory\n",
    "        for file in Path(temp_dir).iterdir():\n",
    "            if file.suffix == \".m4a\":\n",
    "                # Transcribe the audio\n",
    "                transcription = whisper_model.transcribe(str(file), fp16=False, verbose=True)[\"text\"].strip()\n",
    "            \n",
    "            # Write the transcription to a file\n",
    "            with TRANSCRIPT_FILE.open(\"w\") as file:\n",
    "                    file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the transcription and display the first few characters to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it's possible that physics has exploits and we should be trying to find them. arranging some\n",
      "Length: 215854\n"
     ]
    }
   ],
   "source": [
    "with TRANSCRIPT_FILE.open(\"r\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "print(transcription[:100])\n",
    "print(f\"Length: {len(transcription)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the entire transcription as a context\n",
    "\n",
    "If you try to invoke the chain using the transcription as context, the model probably will return an **error** indicating the context is too long.\n",
    "\n",
    "> *LLMs support limited context size*.\n",
    "\n",
    "E.g. most books are far beyond context size so we need to find the solution to somehow truncate context off to pick essential parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4.\n"
     ]
    }
   ],
   "source": [
    "from httpx import ConnectError\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from ollama import ResponseError\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "try:\n",
    "    print(model.invoke(\"What is 2 + 2?\"))\n",
    "except ConnectError as e:\n",
    "    print(f\"Error connecting to the model: {e}\")\n",
    "    print(\"Please make sure the model is running and try again.\")\n",
    "    exit(1)\n",
    "except ResponseError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Looks like this model handles that big context..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video appears to be a conversation between two individuals, likely Andre Karpathy (a computer scientist) and someone else (whose identity is not specified), discussing various topics related to artificial intelligence, consciousness, mortality, and the meaning of life. The conversation is likely inspired by existential questions and explores different perspectives on these topics.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"context\": transcription, \"question\": \"What is the video about?\"}))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How token works\n",
    "\n",
    "[Tiktokenizer](https://tiktokenizer.vercel.app/)\n",
    "\n",
    "https://tiktokenizer.vercel.app/?model=cl100k_base - Basically we can assume that one word is one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiting the transcription\n",
    "\n",
    "Since we **shouldn't use** the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks.\n",
    "\n",
    "We can then invoke the model using only the relevant chunks to answer a particular question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text document\n",
    "\n",
    "[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html#textloader) - using langchain_community there is a simple TextLoader which can help us to load file as Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(TRANSCRIPT_FILE)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting document\n",
    "There are many different ways to split document. In this example we will use simple splitter that splits document into **chunks** of fixed size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../transcription.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"),\n",
       " Document(metadata={'source': '../transcription.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(document)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How embed works\n",
    "\n",
    "[Link to playground](https://dashboard.cohere.com/playground/embed)\n",
    "\n",
    "An embedding is a **mathematical representation** of the semantic meaning of a word, sentence, or document.\n",
    "\n",
    "It's a projection of a concept in a high-dimensional space.\n",
    "\n",
    "Embeddings have a simple characteristic:\n",
    "* The projection of related concepts will be **close to each other**\n",
    "* While concepts with different meanings will **lie far away**\n",
    "\n",
    "![embed](../images/embed_subreddit_titles.png)\n",
    "\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
    "\n",
    "![embed](../images/system3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate embeddings for an arbitrary query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded length 768\n",
      "[0.04998529, 0.028479237, -0.12638704, -0.0074824137, -0.0010402759, 0.012939695, -0.053792614, 0.0043878914, -0.0021878767, -0.013889158]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "try:\n",
    "    embedded_query = embeddings.embed_query(\"My mother has two sisters and one brother.\")\n",
    "except ConnectError as e:\n",
    "    print(f\"Error connecting to the model: {e}\")\n",
    "    print(\"Please make sure the model is running and try again.\")\n",
    "    exit(1)\n",
    "except ResponseError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Embedded length {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"How much sister does Biden have?\")\n",
    "sentence2 = embeddings.embed_query(\"Joanna has one twin brother and two older sisters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6786960345598081, 0.7727273532690365)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "query_sentence1_similarity , query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Vector Store\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches.\n",
    "\n",
    "![System 4](../images/system4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the vector store to find the most similar embeddings to a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       "  0.8880504987256601),\n",
       " (Document(metadata={}, page_content='Mary has two siblings'),\n",
       "  0.8841278683723297),\n",
       " (Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       "  0.5819170276443917)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the vector store to the chain\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model.\n",
    "\n",
    "Here is how we can connect the vector store to the chain:\n",
    "\n",
    "![System](../images/chain4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure a [Retriever](https://python.langchain.com/docs/how_to/#retrievers). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       " Document(metadata={}, page_content='Mary has two siblings'),\n",
       " Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       " Document(metadata={}, page_content=\"Pedro's mother is a teacher\")]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "retriever.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
    "\n",
    "We can create a map with the two inputs by using the `RunnableParallel` and `RunnablePassthrough` classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='Patricia likes white cars'),\n",
       "  Document(metadata={}, page_content='Lucia drives an Audi'),\n",
       "  Document(metadata={}, page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(metadata={}, page_content=\"Mary's sister is Susana\")],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain using another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audi.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading transcription into the vector store\n",
    "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store2 = DocArrayInMemorySearch.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new chain using the correct vector store. This time we are using a different equivalent syntax to specify the `RunnableParallel` portion of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'source': '../transcription.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conversation with Andre Kappathi, previously the director of AI at Tesla. And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and educators in the history of artificial intelligence. This is the Lex Friedman podcast to support it. Please check out our sponsors and now to your friends. Here's Andre Kappathi. What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the\"), Document(metadata={'source': '../transcription.txt'}, page_content=\"honestly, that we're manipulating like seven symbols, uh, serially, we're using vocal chords. It's all happening over like multiple seconds. It's just like kind of embarrassing when you step down to the frequencies at which computers operate or are able to operate on. And so basically, it does seem like, um, synthetic intelligences are kind of like the next stage of development. And, um, I don't know where it leads to like at some point, I suspect the universe is some kind of a puzzle. And these synthetic AIs will uncover that puzzle and solve it. And then what happens after, right? Like what? Because if you just like fast forward earth, many billions of years, it's like, it's quiet. And then it's like, to the terminal, you see like city lights and stuff like that. And then what happens at like at the end, like, is it like a pool? Or is it like a calming? Is it explosion? Is it like earth like open like a giant? Because you said, admit roasters like we'll start emitting like like a\"), Document(metadata={'source': '../transcription.txt'}, page_content=\"as special, but it was obvious. All it was already written in the code that you would have greater and greater intelligence emerging. And then the other explanation, which is something truly special happened, something like a rare event, whether it's like crazy rare event, like space Odyssey. What would it be? See, if you say like the invention of fire, or the, as Richard and Rangham says, the beta males deciding a clever way to kill the alpha males by collaborating. So just optimizing the collaboration, the multi-agent aspect of the multi-agent. And that really being constrained on resources and trying to survive the collaboration aspect is what created the complex intelligence. But it seems like it's a natural algorithm to the evolution process. What could possibly be a magical thing that happened? Like a rare thing that would say that humans are actually human level intelligence, actually a really rare thing in the universe. Yeah, I'm hesitant to say that it is rare, by the way,\"), Document(metadata={'source': '../transcription.txt'}, page_content=\"there be a show on Netflix that's generated completely automatically? So, yeah, and what does it look like also when you can generate it on demand? And it's, and there's infinity of it. Yeah. Oh, man. All the synthetic content. I mean, it's humbling because we treat ourselves as special for being able to generate art and ideas and all that kind of stuff. If that can be done in an automated way by AI. Yeah. I think it's fascinating to me how these, the predictions of AI and what it's going to look like and what it's going to be capable of are completely inverted and wrong. And sci-fi of 50s and 60s is just like totally not right. They imagine AI as like super calculating their approvers and we're getting things that can talk to you about emotions. It can do art. It's just like weird. Are you excited about that feature? Just AI's like hybrid systems, heterogeneous systems of humans and AI's talking about emotions, Netflix and children and AI system that's where the Netflix thing you\")], 'question': 'What is synthetic intelligence?'}\n"
     ]
    }
   ],
   "source": [
    "setup2 = RunnableParallel(context=vector_store2.as_retriever(), question=RunnablePassthrough())\n",
    "\n",
    "print(setup2.invoke(\"What is synthetic intelligence?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic intelligence refers to the use of artificial intelligence (AI) techniques to create intelligent machines or systems that can simulate human-like intelligence. In other words, it's a field of research and development aimed at creating machines that can think, learn, and behave like humans.\n",
      "\n",
      "In the context of the conversation provided, synthetic intelligence is mentioned as the \"next stage of development\" after physics, and it's believed to be capable of uncovering and solving complex puzzles, including those related to the nature of the universe.\n"
     ]
    }
   ],
   "source": [
    "chain2 = (\n",
    "    setup2\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "print(chain2.invoke(\"What is synthetic intelligence?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
