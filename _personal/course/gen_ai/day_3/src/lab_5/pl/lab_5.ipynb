{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwbLHm0-nn-C"
      },
      "source": [
        "# Setup - Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vZ3YXNxTUcd",
        "outputId": "f7713bf8-7f1c-447c-aed4-3d4b6b61bb4f"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install colab-xterm\n",
        "# %load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSyebNl292N4"
      },
      "source": [
        "Lanch xtrem terminal in window.\n",
        "\n",
        "> %xterm\n",
        "\n",
        "Download and run ollama server\n",
        "\n",
        "> curl -sSL https://ollama.ai/install.sh | sh && ollama serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre requisition\n",
        "Validate if ollama runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! curl http://localhost:11434/api/pull -d '{  \"model\": \"llama3.2\" }'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULgkseXH_pJH",
        "outputId": "3441e8cc-d628-4129-d247-a98192320565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-02-19T11:21:48.892546Z\",\"response\":\"The sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the Earth's atmosphere.\\n\\nHere's a simplified explanation:\\n\\n1. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\\n2. These molecules are much smaller than the wavelength of light, which means they scatter the shorter (blue) wavelengths of light more than the longer (red) wavelengths.\\n3. As a result, the blue light is dispersed in all directions by the tiny molecules, giving the sky its blue appearance.\\n4. The scattering effect is more pronounced when the sun is overhead and the air is clear. This is why the sky often appears blue during the daytime.\\n\\nThere are a few other factors that can influence the color of the sky:\\n\\n* Atmospheric conditions: Dust, pollution, and water vapor in the air can scatter light in different ways, changing the apparent color of the sky.\\n* Time of day: The angle of the sun's rays changes throughout the day, which affects the amount of blue light that is scattered.\\n* Altitude: At higher altitudes, the atmosphere is thinner, and the scattering effect is less pronounced, resulting in a more vibrant blue sky.\\n\\nSo, to summarize, the sky appears blue because of the scattering of sunlight by tiny molecules of gases in the Earth's atmosphere!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,10445,374,279,13180,6437,30,128009,128006,78191,128007,271,791,13180,8111,6437,1606,315,264,25885,2663,72916,11,902,13980,994,40120,84261,449,279,13987,35715,315,45612,304,279,9420,596,16975,382,8586,596,264,44899,16540,1473,16,13,3277,40120,29933,279,9420,596,16975,11,433,35006,13987,35715,315,45612,1778,439,47503,320,45,17,8,323,24463,320,46,17,4390,17,13,4314,35715,527,1790,9333,1109,279,46406,315,3177,11,902,3445,814,45577,279,24210,320,12481,8,93959,315,3177,810,1109,279,5129,320,1171,8,93959,627,18,13,1666,264,1121,11,279,6437,3177,374,77810,304,682,18445,555,279,13987,35715,11,7231,279,13180,1202,6437,11341,627,19,13,578,72916,2515,374,810,38617,994,279,7160,374,32115,323,279,3805,374,2867,13,1115,374,3249,279,13180,3629,8111,6437,2391,279,62182,382,3947,527,264,2478,1023,9547,430,649,10383,279,1933,315,279,13180,1473,9,87597,4787,25,33093,11,25793,11,323,3090,38752,304,279,3805,649,45577,3177,304,2204,5627,11,10223,279,10186,1933,315,279,13180,627,9,4212,315,1938,25,578,9392,315,279,7160,596,45220,4442,6957,279,1938,11,902,22223,279,3392,315,6437,3177,430,374,38067,627,9,24610,3993,25,2468,5190,4902,21237,11,279,16975,374,65355,11,323,279,72916,2515,374,2753,38617,11,13239,304,264,810,34076,6437,13180,382,4516,11,311,63179,11,279,13180,8111,6437,1606,315,279,72916,315,40120,555,13987,35715,315,45612,304,279,9420,596,16975,0],\"total_duration\":10730492084,\"load_duration\":825380750,\"prompt_eval_count\":31,\"prompt_eval_duration\":1815000000,\"eval_count\":283,\"eval_duration\":8087000000}"
          ]
        }
      ],
      "source": [
        "!curl http://localhost:11434/api/generate -d '{  \"model\": \"llama3.2\",  \"prompt\":\"Why is the sky blue?\", \"stream\": false}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pDPQs6W_340"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kQPDPaXu_-hc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OLtpQD65AhSz"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "  model = \"llama3.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8kENUFAIbO"
      },
      "source": [
        "# Funkcje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_WGeLatsAJRB"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, context, top_k, top_p, temp):\n",
        "    r = requests.post('http://localhost:11434/api/generate',\n",
        "                     json={\n",
        "                         'model': CFG.model, 'prompt': prompt,\n",
        "                         'context': context,\n",
        "                         'options':{\n",
        "                             'top_k': top_k,\n",
        "                             'temperature':top_p,\n",
        "                             'top_p': temp\n",
        "                         } },\n",
        "                     stream=False)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for line in r.iter_lines():\n",
        "        body = json.loads(line)\n",
        "        response_part = body.get('response', '')\n",
        "        print(response_part)\n",
        "        if 'error' in body:\n",
        "            raise Exception(body['error'])\n",
        "\n",
        "        response += response_part\n",
        "\n",
        "        if body.get('done', False):\n",
        "            context = body.get('context', [])\n",
        "            return response, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqX0tePeALQR"
      },
      "source": [
        "Ta funkcja `generate` służy do komunikacji z lokalnym serwerem API, który uruchamia model językowy. Przyjrzyjmy się jej działaniu krok po kroku:\n",
        "\n",
        "Funkcja przyjmuje pięć parametrów:\n",
        "- prompt: tekst, który zostanie przesłany do modelu\n",
        "- context: kontekst dla modelu, pomocny w zachowaniu spójności rozmowy\n",
        "- top_k, top_p i temp: parametry kontrolujące sposób generowania tekstu przez model\n",
        "\n",
        "W pierwszej części funkcja wysyła zapytanie POST do serwera działającego na lokalnym komputerze (localhost) na porcie 11434. Zapytanie zawiera dane w formacie JSON z nazwą modelu (pobraną z klasy CFG), tekstem promptu, kontekstem oraz opcjami generowania.\n",
        "\n",
        "Następnie funkcja sprawdza, czy zapytanie zakończyło się sukcesem (`raise_for_status()`). Jeśli wystąpił błąd HTTP, zostanie zgłoszony wyjątek.\n",
        "\n",
        "W głównej części funkcji następuje przetwarzanie odpowiedzi z serwera. Odpowiedź jest przesyłana w formie strumienia linii tekstu, które funkcja przetwarza jedna po drugiej. Każda linia jest konwertowana z formatu JSON na obiekt Pythona.\n",
        "\n",
        "Z każdej linii pobierany jest fragment odpowiedzi (response_part) i jest on wyświetlany na ekranie. Jeśli w odpowiedzi pojawi się błąd, funkcja zgłasza wyjątek z treścią błędu.\n",
        "\n",
        "Fragmenty odpowiedzi są łączone w jeden tekst. Gdy serwer zasygnalizuje zakończenie generowania (pole 'done' ma wartość true), funkcja pobiera zaktualizowany kontekst i zwraca dwie wartości: pełną odpowiedź oraz nowy kontekst, który może być użyty w kolejnych wywołaniach.\n",
        "\n",
        "Jest to przykład integracji z API modelu językowego, gdzie odpowiedź jest przesyłana strumieniowo (czyli część po części), co pozwala na wyświetlanie tekstu w czasie rzeczywistym, zamiast czekać na całą odpowiedź."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0KM0U5nkALe9"
      },
      "outputs": [],
      "source": [
        "def chat(input, chat_history, top_k, top_p, temp):\n",
        "\n",
        "    chat_history = chat_history or []\n",
        "\n",
        "    global context\n",
        "    output, context = generate(input, context, top_k, top_p, temp)\n",
        "\n",
        "    chat_history.append((input, output))\n",
        "\n",
        "    return chat_history, chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVsDev_0AQvr"
      },
      "source": [
        "Ta funkcja `chat` zarządza historią rozmowy między użytkownikiem a modelem językowym. Jej działanie jest bardzo interesujące z punktu widzenia zarządzania stanem konwersacji.\n",
        "\n",
        "Przyjmuje ona pięć parametrów:\n",
        "- input: wiadomość od użytkownika\n",
        "- chat_history: lista zawierająca poprzednie wiadomości w rozmowie\n",
        "- top_k, top_p, temp: parametry kontrolujące generowanie tekstu, przekazywane dalej do funkcji generate\n",
        "\n",
        "Na początku funkcja sprawdza, czy chat_history istnieje - jeśli nie, tworzy pustą listę. Jest to zabezpieczenie przed sytuacją, gdy historia rozmowy nie została jeszcze zainicjowana.\n",
        "\n",
        "Następnie funkcja używa słowa kluczowego `global`, aby uzyskać dostęp do zmiennej context zdefiniowanej poza funkcją. Jest to ważne, ponieważ context przechowuje informacje potrzebne do zachowania spójności rozmowy między kolejnymi wywołaniami.\n",
        "\n",
        "W sercu funkcji znajduje się wywołanie poznane wcześniej funkcji generate. Przekazuje ona aktualną wiadomość użytkownika (input) wraz z kontekstem i parametrami generowania. Funkcja zwraca dwie wartości: wygenerowaną odpowiedź (output) oraz zaktualizowany kontekst, który zostaje zapisany w zmiennej globalnej.\n",
        "\n",
        "Po otrzymaniu odpowiedzi od modelu, funkcja dodaje nową parę (wiadomość użytkownika, odpowiedź modelu) do historii rozmowy za pomocą metody append.\n",
        "\n",
        "Na końcu funkcja zwraca dwie kopie zaktualizowanej historii rozmowy. Służą one dwóm różnym celom w interfejsie Gradio:\n",
        "1. Pierwsza kopia aktualizuje widżet czatu, który wyświetla rozmowę użytkownikowi\n",
        "2. Druga kopia aktualizuje wewnętrzny stan aplikacji, który przechowuje historię rozmowy między interakcjami\n",
        "\n",
        "Jest to rozwiązanie, które pozwala na synchronizację tego, co widzi użytkownik, z tym, co \"pamięta\" aplikacja. Dzięki temu historia rozmowy jest zachowana nawet po odświeżeniu interfejsu lub ponownym uruchomieniu aplikacji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j0fwm39ARaB"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uDi36Q8JAfWx"
      },
      "outputs": [],
      "source": [
        "context = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "536yM8TpASEu",
        "outputId": "acf77387-ea28-497a-a0ae-4ba60e070439"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marcinpilarczyk/projects/python/AI-Notes/_personal/course/gen_ai/day_3/.venv/lib/python3.10/site-packages/gradio/components/chatbot.py:284: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://462e0ef0be6d7795fa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://462e0ef0be6d7795fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\n",
            "'m\n",
            " just\n",
            " a\n",
            " language\n",
            " model\n",
            ",\n",
            " so\n",
            " I\n",
            " don\n",
            "'t\n",
            " have\n",
            " emotions\n",
            " or\n",
            " feelings\n",
            " like\n",
            " humans\n",
            " do\n",
            ".\n",
            " However\n",
            ",\n",
            " I\n",
            "'m\n",
            " functioning\n",
            " properly\n",
            " and\n",
            " ready\n",
            " to\n",
            " help\n",
            " with\n",
            " any\n",
            " questions\n",
            " or\n",
            " tasks\n",
            " you\n",
            " may\n",
            " have\n",
            "!\n",
            " How\n",
            " about\n",
            " you\n",
            "?\n",
            " How\n",
            "'s\n",
            " your\n",
            " day\n",
            " going\n",
            "?\n",
            "\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://462e0ef0be6d7795fa.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block = gr.Blocks()\n",
        "\n",
        "with block:\n",
        "\n",
        "    gr.Markdown(\"\"\"<h1><center> My private chatbot </center></h1>\n",
        "    \"\"\")\n",
        "\n",
        "    message = gr.Textbox(placeholder=\"Type here\")\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "\n",
        "    state = gr.State()\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "\n",
        "      with gr.Row():\n",
        "          top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
        "          top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
        "          temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
        "\n",
        "\n",
        "    submit = gr.Button(\"SEND\")\n",
        "\n",
        "    submit.click(chat, inputs=[message, state, top_k, top_p, temp], outputs=[chatbot, state])\n",
        "\n",
        "block.launch(debug=True, share = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-dCsaPCAYjp"
      },
      "source": [
        "Ta część kodu tworzy interfejs graficzny aplikacji przy użyciu biblioteki Gradio. Przeanalizujmy jego strukturę i funkcjonalność:\n",
        "\n",
        "Najpierw tworzymy blok - podstawowy kontener interfejsu Gradio - używając `gr.Blocks()`. Wszystkie elementy interfejsu będą umieszczone wewnątrz tego bloku.\n",
        "\n",
        "W głównej części interfejsu znajduje się nagłówek HTML z tytułem \"Muh private chatbot\", wycentrowanym na stronie. Pod nim umieszczone są trzy kluczowe elementy:\n",
        "- Pole tekstowe (gr.Textbox) z podpowiedzią \"Type here\", gdzie użytkownik wpisuje swoje wiadomości\n",
        "- Komponent chatbota (gr.Chatbot), który wyświetla historię rozmowy\n",
        "- Obiekt stanu (gr.State) przechowujący dane między interakcjami\n",
        "\n",
        "Szczególnie interesujący jest rozwijaną sekcja \"Advanced Settings\". Zawiera ona trzy suwaki kontrolujące parametry generowania tekstu:\n",
        "\n",
        "1. top_k (zakres 0-100, domyślnie 40): Pomaga redukować nonsensowne odpowiedzi. Wyższa wartość oznacza bardziej zróżnicowane odpowiedzi, niższa - bardziej zachowawcze.\n",
        "\n",
        "2. top_p (zakres 0-1, domyślnie 0.9): Współdziała z top_k. Wyższa wartość prowadzi do bardziej różnorodnych odpowiedzi, niższa generuje bardziej skupiony i konserwatywny tekst.\n",
        "\n",
        "3. temperature (zakres 0-2, domyślnie 0.8): Kontroluje \"kreatywność\" modelu. Wyższa temperatura skutkuje bardziej kreatywnymi odpowiedziami.\n",
        "\n",
        "Na dole interfejsu znajduje się przycisk \"SEND\". Po jego kliknięciu wywoływana jest funkcja chat z odpowiednimi parametrami wejściowymi (wiadomość, stan, parametry generowania) i wyjściowymi (chatbot i stan).\n",
        "\n",
        "Ostatnia linia uruchamia aplikację w trybie debugowania (debug=True) z opcją udostępniania (share=True), co pozwala na dostęp do interfejsu przez internet, nie tylko lokalnie.\n",
        "\n",
        "To połączenie prostego w obsłudze interfejsu dla zwykłych użytkowników z zaawansowanymi opcjami dla bardziej doświadczonych osób, którzy chcą dostroić zachowanie modelu do swoich potrzeb."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
