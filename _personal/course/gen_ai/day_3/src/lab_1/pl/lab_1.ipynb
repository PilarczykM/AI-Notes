{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d26f2dc9",
      "metadata": {
        "papermill": {
          "duration": 0.006432,
          "end_time": "2025-01-29T16:30:35.283909",
          "exception": false,
          "start_time": "2025-01-29T16:30:35.277477",
          "status": "completed"
        },
        "tags": [],
        "id": "d26f2dc9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "938a79ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:30:35.294284Z",
          "iopub.status.busy": "2025-01-29T16:30:35.293981Z",
          "iopub.status.idle": "2025-01-29T16:30:44.598474Z",
          "shell.execute_reply": "2025-01-29T16:30:44.597739Z"
        },
        "papermill": {
          "duration": 9.311407,
          "end_time": "2025-01-29T16:30:44.600117",
          "exception": false,
          "start_time": "2025-01-29T16:30:35.288710",
          "status": "completed"
        },
        "tags": [],
        "id": "938a79ff"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6f109a",
      "metadata": {
        "papermill": {
          "duration": 0.00467,
          "end_time": "2025-01-29T16:30:44.609974",
          "exception": false,
          "start_time": "2025-01-29T16:30:44.605304",
          "status": "completed"
        },
        "tags": [],
        "id": "6d6f109a"
      },
      "source": [
        "`AutoTokenizer` służy do przetwarzania tekstu na format zrozumiały dla modelu. Zamienia on słowa i zdania na ciągi liczb (tokeny), które model może przetworzyć.\n",
        "\n",
        "`AutoModelForCausalLM` to klasa odpowiedzialna za ładowanie modeli językownych typu przyczynowego (causal). Modele te przewidują kolejne tokeny na podstawie wcześniejszego kontekstu, co pozwala im generować spójny tekst.\n",
        "\n",
        "`set_seed` to funkcja ustalająca ziarno losowości. Dzięki niej można uzyskać powtarzalne wyniki przy każdym uruchomieniu kodu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4e7d6b8b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:30:44.620270Z",
          "iopub.status.busy": "2025-01-29T16:30:44.619879Z",
          "iopub.status.idle": "2025-01-29T16:30:44.623169Z",
          "shell.execute_reply": "2025-01-29T16:30:44.622504Z"
        },
        "papermill": {
          "duration": 0.009841,
          "end_time": "2025-01-29T16:30:44.624432",
          "exception": false,
          "start_time": "2025-01-29T16:30:44.614591",
          "status": "completed"
        },
        "tags": [],
        "id": "4e7d6b8b"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    model = \"Qwen/Qwen2-0.5B\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41566f9e",
      "metadata": {
        "papermill": {
          "duration": 0.00444,
          "end_time": "2025-01-29T16:30:44.633469",
          "exception": false,
          "start_time": "2025-01-29T16:30:44.629029",
          "status": "completed"
        },
        "tags": [],
        "id": "41566f9e"
      },
      "source": [
        "`\"Qwen\"` to rodzina modeli językowych stworzonych przez Alibaba Cloud. Liczba \"2\" wskazuje na drugą generację tego modelu. \"0.5B\" oznacza, że model ma około 500 milionów parametrów - są to wartości liczbowe, które model dostosowuje w trakcie uczenia się, aby lepiej rozumieć i generować tekst.\n",
        "\n",
        "Rozmiar 500 milionów parametrów plasuje ten model w kategorii mniejszych modeli językowych. Dla porównania, modele takie jak GPT-3 mają setki miliardów parametrów. Mniejszy rozmiar ma jednak swoje zalety - model wymaga mniej zasobów obliczeniowych, może działać szybciej i być używany na słabszym sprzęcie."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc4fdcf",
      "metadata": {
        "papermill": {
          "duration": 0.004429,
          "end_time": "2025-01-29T16:30:44.642447",
          "exception": false,
          "start_time": "2025-01-29T16:30:44.638018",
          "status": "completed"
        },
        "tags": [],
        "id": "dcc4fdcf"
      },
      "source": [
        "# Tokenizacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2755fdfe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:30:44.652390Z",
          "iopub.status.busy": "2025-01-29T16:30:44.652182Z",
          "iopub.status.idle": "2025-01-29T16:30:49.474071Z",
          "shell.execute_reply": "2025-01-29T16:30:49.473107Z"
        },
        "papermill": {
          "duration": 4.828667,
          "end_time": "2025-01-29T16:30:49.475645",
          "exception": false,
          "start_time": "2025-01-29T16:30:44.646978",
          "status": "completed"
        },
        "tags": [],
        "id": "2755fdfe"
      },
      "outputs": [],
      "source": [
        "prompt = \"It was a dark and stormy\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62bd5ec",
      "metadata": {
        "papermill": {
          "duration": 0.005093,
          "end_time": "2025-01-29T16:30:49.487168",
          "exception": false,
          "start_time": "2025-01-29T16:30:49.482075",
          "status": "completed"
        },
        "tags": [],
        "id": "c62bd5ec"
      },
      "source": [
        "`tokenizer = AutoTokenizer.from_pretrained(CFG.model)` Tokenizer działa jak tłumacz między ludzkim językiem a językiem modelu. Gdy wywołujemy metodę `from_pretrained`, następuje automatyczne pobranie odpowiedniego tokenizera z serwisu Hugging Face, gdzie przechowywane są modele i ich komponenty.\n",
        "\n",
        "Tokenizer zamienia tekst na sekwencję tokenów - podstawowych jednostek, które model potrafi przetwarzać. Tokeny mogą być pojedynczymi słowami, częściami słów, lub nawet pojedynczymi znakami. Na przykład, zdanie \"It was a dark and stormy\" zostanie podzielone na tokeny w sposób specyficzny dla modelu Qwen. Tokenizer pamięta też, jak później połączyć tokeny z powrotem w tekst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "79fd8300",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-01-29T16:30:49.498240Z",
          "iopub.status.busy": "2025-01-29T16:30:49.497968Z",
          "iopub.status.idle": "2025-01-29T16:30:49.505227Z",
          "shell.execute_reply": "2025-01-29T16:30:49.504558Z"
        },
        "papermill": {
          "duration": 0.013973,
          "end_time": "2025-01-29T16:30:49.506458",
          "exception": false,
          "start_time": "2025-01-29T16:30:49.492485",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79fd8300",
        "outputId": "c4782aee-89b6-4fb0-819e-41f552f870ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2132, 572, 264, 6319, 323, 13458, 88]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "input_ids = tokenizer(prompt).input_ids\n",
        "\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41df55a5",
      "metadata": {
        "papermill": {
          "duration": 0.004879,
          "end_time": "2025-01-29T16:30:49.516395",
          "exception": false,
          "start_time": "2025-01-29T16:30:49.511516",
          "status": "completed"
        },
        "tags": [],
        "id": "41df55a5"
      },
      "source": [
        "Najpierw tokenizer dzieli tekst na mniejsze części, czyli tokeny. Następnie każdemu tokenowi przypisuje unikalny numer identyfikacyjny (ID). Te identyfikatory są używane przez model, ponieważ pracuje on na liczbach, nie na tekście.\n",
        "\n",
        "Gdy wywołujemy `tokenizer(prompt)`, otrzymujemy obiekt zawierający różne informacje o tokenizacji. Atrybut `.input_ids` wydobywa z tego obiektu konkretnie listę identyfikatorów numerycznych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "30d7bd7e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:30:49.527076Z",
          "iopub.status.busy": "2025-01-29T16:30:49.526806Z",
          "iopub.status.idle": "2025-01-29T16:31:01.176239Z",
          "shell.execute_reply": "2025-01-29T16:31:01.175283Z"
        },
        "papermill": {
          "duration": 11.656287,
          "end_time": "2025-01-29T16:31:01.177631",
          "exception": false,
          "start_time": "2025-01-29T16:30:49.521344",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30d7bd7e",
        "outputId": "45e39bcb-4956-4540-f6b2-230a07f52a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2132 \t:  It\n",
            "572 \t:   was\n",
            "264 \t:   a\n",
            "6319 \t:   dark\n",
            "323 \t:   and\n",
            "13458 \t:   storm\n",
            "88 \t:  y\n"
          ]
        }
      ],
      "source": [
        "for t in input_ids:\n",
        "    print(t, \"\\t: \", tokenizer.decode(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d4befc",
      "metadata": {
        "papermill": {
          "duration": 0.005137,
          "end_time": "2025-01-29T16:31:01.188461",
          "exception": false,
          "start_time": "2025-01-29T16:31:01.183324",
          "status": "completed"
        },
        "tags": [],
        "id": "a3d4befc"
      },
      "source": [
        "Następnie używamy metody `tokenizer.decode(t)`, która zamienia pojedynczy identyfikator z powrotem na tekst. To bardzo pouczające, ponieważ pokazuje nam, jak model \"widzi\" nasz tekst - jakie konkretnie fragmenty tekstu odpowiadają poszczególnym numerom."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bac304d",
      "metadata": {
        "papermill": {
          "duration": 0.005059,
          "end_time": "2025-01-29T16:31:01.198713",
          "exception": false,
          "start_time": "2025-01-29T16:31:01.193654",
          "status": "completed"
        },
        "tags": [],
        "id": "9bac304d"
      },
      "source": [
        "# Prawdopodobienstwo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8fec3457",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:01.210512Z",
          "iopub.status.busy": "2025-01-29T16:31:01.209794Z",
          "iopub.status.idle": "2025-01-29T16:31:28.471153Z",
          "shell.execute_reply": "2025-01-29T16:31:28.470508Z"
        },
        "papermill": {
          "duration": 27.268543,
          "end_time": "2025-01-29T16:31:28.472509",
          "exception": false,
          "start_time": "2025-01-29T16:31:01.203966",
          "status": "completed"
        },
        "tags": [],
        "id": "8fec3457"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(CFG.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7496e121",
      "metadata": {
        "papermill": {
          "duration": 0.005327,
          "end_time": "2025-01-29T16:31:28.484046",
          "exception": false,
          "start_time": "2025-01-29T16:31:28.478719",
          "status": "completed"
        },
        "tags": [],
        "id": "7496e121"
      },
      "source": [
        "`AutoModelForCausalLM` to specjalna klasa z biblioteki transformers, która jest przeznaczona do modeli przyczynowych (causal). Określenie \"przyczynowy\" oznacza, że model generuje tekst sekwencyjnie, token po tokenie, gdzie każdy nowy token zależy tylko od poprzednich tokenów - podobnie jak człowiek piszący tekst, który widzi tylko to, co już napisał, nie to, co dopiero zamierza napisać.\n",
        "\n",
        "Metoda `from_pretrained()` robi kilka ważnych rzeczy za kulisami. Po pierwsze, pobiera pliki modelu z repozytorium Hugging Face - to jak ściąganie specjalistycznego programu, który nauczył się rozumieć i generować tekst. W przypadku Qwen2-0.5B pobierane są pliki zawierające wszystkie parametry modelu - te 500 milionów liczb, które model wykorzystuje do przetwarzania języka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "342e89e3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:28.495816Z",
          "iopub.status.busy": "2025-01-29T16:31:28.495563Z",
          "iopub.status.idle": "2025-01-29T16:31:28.878736Z",
          "shell.execute_reply": "2025-01-29T16:31:28.877980Z"
        },
        "papermill": {
          "duration": 0.390587,
          "end_time": "2025-01-29T16:31:28.880049",
          "exception": false,
          "start_time": "2025-01-29T16:31:28.489462",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "342e89e3",
        "outputId": "60c032df-0bc8-424c-c456-bd0cd3345eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 151936])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "input_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
        "outputs = model(input_ids)\n",
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa33ab8",
      "metadata": {
        "papermill": {
          "duration": 0.005787,
          "end_time": "2025-01-29T16:31:28.892077",
          "exception": false,
          "start_time": "2025-01-29T16:31:28.886290",
          "status": "completed"
        },
        "tags": [],
        "id": "faa33ab8"
      },
      "source": [
        "`input_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids` robi nieco więcej niż poprzednia tokenizacja, którą widzieliśmy.\n",
        "* Parametr `return_tensors = \"pt\"` mówi tokenizerowi, żeby przekształcił wynik w format tensora PyTorch. To jak przekładanie tekstu nie tylko na liczby, ale na specjalny format matematyczny, który jest zoptymalizowany do obliczeń na GPU lub CPU. \"pt\" oznacza właśnie PyTorch - framework, którego używamy do obliczeń.\n",
        "\n",
        "`outputs = model(input_ids)` to moment, gdy nasz tekst przechodzi przez model. To trochę jak przepuszczenie sygnału przez skomplikowany układ elektroniczny. Model przetwarza nasze tokeny, używając swoich 500 milionów parametrów, aby przewidzieć, jakie słowa mogłyby naturalnie wystąpić po naszym promptcie.\n",
        "\n",
        "`outputs.logits.shape` pokazuje kształt (wymiary) wyjścia modelu. Wynik jest w postaci tak zwanych logitów - są to surowe wartości liczbowe przed przekształceniem ich w prawdopodobieństwa. Kształt tego tensora ma trzy wymiary:\n",
        "\n",
        "1. Liczba sekwencji (batch size) - zazwyczaj 1, jeśli przetwarzamy pojedynczy prompt\n",
        "2. Długość sekwencji - liczba tokenów w naszym inpucie\n",
        "3. Rozmiar słownika - liczba wszystkich możliwych tokenów, które model zna\n",
        "\n",
        "Dla każdej pozycji w naszej sekwencji model generuje wartość logitu dla każdego możliwego tokenu w swoim słowniku. Te wartości reprezentują \"przekonanie\" modelu o tym, jak dobrze każdy token pasowałby jako następny w sekwencji. Im wyższa wartość logitu, tym bardziej model \"uważa\", że dany token powinien wystąpić w tym miejscu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e298c430",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:28.904904Z",
          "iopub.status.busy": "2025-01-29T16:31:28.904628Z",
          "iopub.status.idle": "2025-01-29T16:31:29.212790Z",
          "shell.execute_reply": "2025-01-29T16:31:29.211923Z"
        },
        "papermill": {
          "duration": 0.316574,
          "end_time": "2025-01-29T16:31:29.214288",
          "exception": false,
          "start_time": "2025-01-29T16:31:28.897714",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e298c430",
        "outputId": "35c06022-397c-4533-a9dd-2bc086aed9d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3729)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "final_logits = model(input_ids).logits[0,-1]\n",
        "final_logits.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a203dbb",
      "metadata": {
        "papermill": {
          "duration": 0.005464,
          "end_time": "2025-01-29T16:31:29.225739",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.220275",
          "status": "completed"
        },
        "tags": [],
        "id": "2a203dbb"
      },
      "source": [
        "Te dwie linie kodu są kluczowym momentem w procesie generowania tekstu. Skupiają się na wyborze następnego tokenu w naszej sekwencji.\n",
        "\n",
        "W linii `final_logits = model(input_ids).logits[0,-1]` wyodrębniamy konkretny fragment z wyjścia modelu. Rozbijmy indeksowanie `[0,-1]`:\n",
        "- `0` wybiera pierwszą (i w tym przypadku jedyną) sekwencję z naszej paczki danych\n",
        "- `-1` wybiera ostatnią pozycję w sekwencji - tam, gdzie model będzie przewidywał następny token\n",
        "\n",
        "Te logity reprezentują \"pewność\" modelu co do każdego możliwego następnego tokenu. Można to porównać do muzyka, który zna wiele możliwych akordów i musi wybrać ten, który najlepiej pasuje do aktualnej melodii.\n",
        "\n",
        "Druga linia `final_logits.argmax()` znajduje pozycję tokenu o najwyższej wartości logitu. Metoda `argmax()` to jak wskazanie palcem na najwyższy słupek w wykresie - zwraca indeks największej wartości. W kontekście naszego modelu językowego, ten indeks odpowiada tokenowi, który model uważa za najbardziej prawdopodobny jako następny w sekwencji.\n",
        "\n",
        "To trochę jak wybieranie następnego słowa w grze w skojarzenia - model przeanalizował nasz prompt \"It was a dark and stormy\" i wskazuje token, który jego zdaniem najlepiej pasuje jako kontynuacja, bazując na wszystkim, czego nauczył się podczas treningu. Może to być na przykład słowo \"night\", które często występuje po frazie \"dark and stormy\", ale model mógł też wybrać coś bardziej zaskakującego, bazując na innych wzorcach, które rozpoznał w danych treningowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2fafc903",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:29.237723Z",
          "iopub.status.busy": "2025-01-29T16:31:29.237483Z",
          "iopub.status.idle": "2025-01-29T16:31:29.242230Z",
          "shell.execute_reply": "2025-01-29T16:31:29.241382Z"
        },
        "papermill": {
          "duration": 0.012122,
          "end_time": "2025-01-29T16:31:29.243576",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.231454",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2fafc903",
        "outputId": "2ae3ddd5-9727-47e8-ac1a-60ef67c2e8a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' night'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tokenizer.decode(final_logits.argmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fc4e9fb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:29.267257Z",
          "iopub.status.busy": "2025-01-29T16:31:29.266917Z",
          "iopub.status.idle": "2025-01-29T16:31:29.280605Z",
          "shell.execute_reply": "2025-01-29T16:31:29.279923Z"
        },
        "papermill": {
          "duration": 0.021204,
          "end_time": "2025-01-29T16:31:29.281835",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.260631",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc4e9fb0",
        "outputId": "0fd099ec-b200-4a69-f41d-53559e6c0646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " night\n",
            " evening\n",
            " day\n",
            " morning\n",
            " winter\n",
            " afternoon\n",
            " Saturday\n",
            " Sunday\n",
            " Friday\n",
            " October\n"
          ]
        }
      ],
      "source": [
        "top10_logits = torch.topk(final_logits, 10)\n",
        "\n",
        "for index in top10_logits.indices:\n",
        "    print(tokenizer.decode(index))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2db95ca",
      "metadata": {
        "papermill": {
          "duration": 0.005544,
          "end_time": "2025-01-29T16:31:29.293180",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.287636",
          "status": "completed"
        },
        "tags": [],
        "id": "d2db95ca"
      },
      "source": [
        "Pierwsza linia `top10_logits = torch.topk(final_logits,10)` używa funkcji `topk` z biblioteki PyTorch, która znajduje dziesięć najwyższych wartości w tensorze logitów. To jak patrzenie na dziesięć najsilniejszych intuicji modelu, zamiast tylko na tę najsilniejszą. Parametr 10 określa, ile najlepszych wyborów chcemy zobaczyć."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "15fcf4e0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:29.305157Z",
          "iopub.status.busy": "2025-01-29T16:31:29.304895Z",
          "iopub.status.idle": "2025-01-29T16:31:29.313097Z",
          "shell.execute_reply": "2025-01-29T16:31:29.312370Z"
        },
        "papermill": {
          "duration": 0.015697,
          "end_time": "2025-01-29T16:31:29.314514",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.298817",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15fcf4e0",
        "outputId": "c5de2d6a-d769-4fb9-91cc-32a6c2021a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " night    88.71%\n",
            " evening  4.30%\n",
            " day      2.19%\n",
            " morning  0.49%\n",
            " winter   0.45%\n",
            " afternoon0.27%\n",
            " Saturday 0.25%\n",
            " Sunday   0.19%\n",
            " Friday   0.17%\n",
            " October  0.16%\n"
          ]
        }
      ],
      "source": [
        "top10 = torch.topk(final_logits.softmax(dim=0), 10)\n",
        "\n",
        "for value, index in zip(top10.values, top10.indices):\n",
        "    print(f\"{tokenizer.decode(index):<10}{value.item():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef1f1bc",
      "metadata": {
        "papermill": {
          "duration": 0.005686,
          "end_time": "2025-01-29T16:31:29.325987",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.320301",
          "status": "completed"
        },
        "tags": [],
        "id": "9ef1f1bc"
      },
      "source": [
        "Pierwsza linia `top10 = torch.topk(final_logits.softmax(dim=0),10)` zawiera kluczową transformację -\n",
        "* funkcję `softmax`. Jest to matematyczna operacja, która przekształca surowe logity (które mogą być dowolnymi liczbami) na prawdopodobieństwa, które zawsze sumują się do 100%.\n",
        "  * Parametr `dim=0` określa, wzdłuż którego wymiaru ma być wykonana operacja softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df88e5c",
      "metadata": {
        "papermill": {
          "duration": 0.005929,
          "end_time": "2025-01-29T16:31:29.337902",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.331973",
          "status": "completed"
        },
        "tags": [],
        "id": "7df88e5c"
      },
      "source": [
        "# Generowanie tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0c177a0f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:29.350515Z",
          "iopub.status.busy": "2025-01-29T16:31:29.350313Z",
          "iopub.status.idle": "2025-01-29T16:31:31.618193Z",
          "shell.execute_reply": "2025-01-29T16:31:31.617207Z"
        },
        "papermill": {
          "duration": 2.276056,
          "end_time": "2025-01-29T16:31:31.619821",
          "exception": false,
          "start_time": "2025-01-29T16:31:29.343765",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c177a0f",
        "outputId": "93a63819-16a8-4f70-c7dd-9b5f3f1c21bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "output_ids = model.generate(input_ids, max_new_tokens = 20)\n",
        "decoded_text = tokenizer.decode(output_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9ed7c68e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:31.646917Z",
          "iopub.status.busy": "2025-01-29T16:31:31.646652Z",
          "iopub.status.idle": "2025-01-29T16:31:31.652647Z",
          "shell.execute_reply": "2025-01-29T16:31:31.651953Z"
        },
        "papermill": {
          "duration": 0.013549,
          "end_time": "2025-01-29T16:31:31.653838",
          "exception": false,
          "start_time": "2025-01-29T16:31:31.640289",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ed7c68e",
        "outputId": "967a1fc3-e2b8-4270-82e6-8136c92e0013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs tensor([ 2132,   572,   264,  6319,   323, 13458,    88])\n",
            "Output IDs tensor([[ 2132,   572,   264,  6319,   323, 13458,    88,  3729,    13,   576,\n",
            "         12884,   572,  6319,   323,   279,  9956,   572,  1246,  2718,    13,\n",
            "           576, 11174,   572, 50413,  1495,   323,   279]])\n",
            "Generated text: It was a dark and stormy night. The sky was dark and the wind was howling. The rain was pouring down and the\n"
          ]
        }
      ],
      "source": [
        "print(\"Input IDs\", input_ids[0])\n",
        "print(\"Output IDs\", output_ids)\n",
        "print(f\"Generated text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2ca2f9",
      "metadata": {
        "papermill": {
          "duration": 0.005768,
          "end_time": "2025-01-29T16:31:31.665793",
          "exception": false,
          "start_time": "2025-01-29T16:31:31.660025",
          "status": "completed"
        },
        "tags": [],
        "id": "4d2ca2f9"
      },
      "source": [
        "Te trzy linijki kodu pokazują nam pełny obraz procesu generowania tekstu - od początku do końca. Przeanalizujmy szczegółowo, co widzimy na każdym etapie.\n",
        "\n",
        "`print(\"Input IDs\", input_ids[0])` pokazuje numeryczne reprezentacje tokenów naszego początkowego tekstu \"It was a dark and stormy\". Jest to jak zerknięcie na \"surowe dane\", które model otrzymuje na wejściu. Każda liczba w tej sekwencji reprezentuje konkretny token, który model rozpoznaje jako część swojego słownika.\n",
        "\n",
        "`print(\"Output IDs\", output_ids)` przedstawia pełną sekwencję tokenów po generacji. To, co zobaczymy, to połączenie oryginalnych tokenów wejściowych oraz nowych tokenów, które model wygenerował. Jest to szczególnie interesujące, ponieważ możemy zobaczyć, jak model rozbudował oryginalną sekwencję. Pierwsze liczby będą identyczne z input_ids, a następnie pojawią się nowe liczby reprezentujące wygenerowaną kontynuację.\n",
        "\n",
        "`print(f\"Generated text: {decoded_text}\")` pokazuje nam końcowy rezultat w formie czytelnej dla człowieka. Jest to moment transformacji - wszystkie numeryczne tokeny zostają przekształcone z powrotem w tekst. Zobaczymy nasz oryginalny prompt \"It was a dark and stormy\" wraz z wygenerowaną kontynuacją.\n",
        "\n",
        "Porównanie tych trzech wyjść jest niezwykle pouczające. Możemy zaobserwować, jak ten sam tekst jest reprezentowany na różnych poziomach abstrakcji:\n",
        "- jako sekwencja liczb, którymi operuje model\n",
        "- jako rozszerzona sekwencja zawierająca nowe tokeny\n",
        "- jako zrozumiały tekst w języku naturalnym\n",
        "\n",
        "Jest to jak obserwowanie procesu tłumaczenia między trzema różnymi językami: językiem komputera (ID tokenów), językiem modelu (rozszerzona sekwencja) i językiem ludzkim (tekst). Każda reprezentacja niesie te same informacje, ale w formie odpowiedniej dla różnych etapów przetwarzania."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8f21def6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:31.678724Z",
          "iopub.status.busy": "2025-01-29T16:31:31.678505Z",
          "iopub.status.idle": "2025-01-29T16:31:39.140370Z",
          "shell.execute_reply": "2025-01-29T16:31:39.139548Z"
        },
        "papermill": {
          "duration": 7.469606,
          "end_time": "2025-01-29T16:31:39.141753",
          "exception": false,
          "start_time": "2025-01-29T16:31:31.672147",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f21def6",
        "outputId": "eb5deba2-489c-4b5a-c261-4f8fcd651b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night. The wind was howling, and the rain was pouring down. The sky was dark and gloomy, and the air was filled with the\n"
          ]
        }
      ],
      "source": [
        "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "beam_output = model.generate(\n",
        "input_ids,\n",
        "num_beams=5,\n",
        "max_new_tokens=30,\n",
        "attention_mask=attention_mask,\n",
        "pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18636e3a",
      "metadata": {
        "papermill": {
          "duration": 0.006163,
          "end_time": "2025-01-29T16:31:39.155146",
          "exception": false,
          "start_time": "2025-01-29T16:31:39.148983",
          "status": "completed"
        },
        "tags": [],
        "id": "18636e3a"
      },
      "source": [
        "### Tworzenie attention_mask\n",
        "* `input_ids` → Jest to tensor z tokenami wejściowymi dla modelu (zakodowany tekst).\n",
        "* `tokenizer.pad_token_id` → To ID tokena paddingu (<pad>), który jest używany do wyrównywania sekwencji w batchu.\n",
        "* `input_ids.ne(tokenizer.pad_token_id)`\n",
        "  * `.ne()` oznacza \"not equal\" (czyli !=), co tworzy maskę True dla wszystkich tokenów różnych od <pad>.\n",
        "  * Tokeny `<pad>` będą miały False, a reszta True.\n",
        "* .long() → Konwertuje wartości True/False na 1/0 (long() = int64)\n",
        "\n",
        "co daje tensor:\n",
        "```txt\n",
        "[1, 1, 1, 0, 0]\n",
        "```\n",
        "jeśli tokeny input_ids wyglądają tak:\n",
        "```txt\n",
        "[101, 1023, 2045, 0, 0]\n",
        "```\n",
        "### Generowanie tekstu:\n",
        "✅ **Co tu się dzieje?**  \n",
        "\n",
        "- **input_ids** → Model dostaje tokeny wejściowe do przetworzenia.  \n",
        "\n",
        "- **num_beams=5**  \n",
        "  - Włącza beam search z 5 ścieżkami (model generuje 5 możliwych kontynuacji i wybiera najlepszą).  \n",
        "  - To poprawia jakość tekstu (lepsze niż standardowy greedy decoding).  \n",
        "\n",
        "- **max_new_tokens=30**  \n",
        "  - Ogranicza generowanie do 30 nowych tokenów (nie licząc wejścia).  \n",
        "\n",
        "- **attention_mask=attention_mask**  \n",
        "  - Informuje model, na które tokeny zwracać uwagę, a które ignorować (pomaga unikać błędów związanych z `<pad>`).  \n",
        "\n",
        "- **pad_token_id=tokenizer.eos_token_id**  \n",
        "  - Określa token końcowy (`<eos>` jako `pad_token_id`), co pomaga uniknąć błędów w dekodowaniu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "997b697a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-29T16:31:39.169355Z",
          "iopub.status.busy": "2025-01-29T16:31:39.169107Z",
          "iopub.status.idle": "2025-01-29T16:31:48.023674Z",
          "shell.execute_reply": "2025-01-29T16:31:48.022577Z"
        },
        "papermill": {
          "duration": 8.863616,
          "end_time": "2025-01-29T16:31:48.025036",
          "exception": false,
          "start_time": "2025-01-29T16:31:39.161420",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "997b697a",
        "outputId": "c94dba07-d51b-45d2-b57c-f690f7474f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- It was a dark and stormy night. The sky was filled with thunder and lightning, and the wind howled in the distance. It was raining cats and dogs, and the streets were covered in puddles of water.\n"
          ]
        }
      ],
      "source": [
        "beam_output = model.generate(\n",
        "input_ids,\n",
        "num_beams=5,\n",
        "repetition_penalty=2.0,\n",
        "max_new_tokens=38,\n",
        "attention_mask=attention_mask,\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "markdown_output = \"\\n\".join(f\"- {tokenizer.decode(output)}\" for output in beam_output)\n",
        "print(markdown_output)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 78.729273,
      "end_time": "2025-01-29T16:31:51.520651",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-01-29T16:30:32.791378",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}