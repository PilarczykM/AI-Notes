{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBo3k7AGGH05"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-10-20T13:36:00.094672Z",
          "iopub.status.busy": "2024-10-20T13:36:00.094247Z",
          "iopub.status.idle": "2024-10-20T13:37:45.850022Z",
          "shell.execute_reply": "2024-10-20T13:37:45.848742Z"
        },
        "id": "3aee77db",
        "outputId": "16fc1390-a1e9-4c14-ca9f-4874486769c6",
        "papermill": {
          "duration": 105.763259,
          "end_time": "2024-10-20T13:37:45.852648",
          "exception": false,
          "start_time": "2024-10-20T13:36:00.089389",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHA9TEaYGF6j"
      },
      "source": [
        "\"transformers\" od Hugging Face bezpośrednio z repozytorium GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UuVzMr7LGc5",
        "outputId": "eab2e42e-1d13-45c3-ff45-4e4800467401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pnRpIqJLHKd"
      },
      "source": [
        "Kluczowym elementem jest tutaj parametr `--no-build-isolation`. W normalnych warunkach pip tworzy izolowane środowisko do budowania pakietu, co zapewnia czystą instalację bez konfliktów z innymi bibliotekami. Jednak w przypadku flash-attn może to prowadzić do problemów, ponieważ ta biblioteka ma specyficzne zależności i wymaga dostępu do określonych wersji narzędzi kompilacyjnych oraz bibliotek systemowych.\n",
        "\n",
        "Wyłączenie izolacji budowania pozwala flash-attn korzystać z już zainstalowanych w systemie zależności i narzędzi, co często jest niezbędne do poprawnej kompilacji tej biblioteki. Jest to szczególnie istotne w środowiskach, gdzie mamy już skonfigurowane odpowiednie wersje CUDA i innych bibliotek potrzebnych do wydajnego działania operacji na GPU.\n",
        "\n",
        "Warto pamiętać, że użycie `--no-build-isolation` może teoretycznie prowadzić do niespójności w zależnościach, ale w przypadku flash-attn jest to często konieczny kompromis, aby zapewnić prawidłową instalację i optymalne działanie biblioteki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-20T13:37:45.860450Z",
          "iopub.status.busy": "2024-10-20T13:37:45.859630Z",
          "iopub.status.idle": "2024-10-20T13:38:07.198479Z",
          "shell.execute_reply": "2024-10-20T13:38:07.197477Z"
        },
        "id": "96507bad",
        "papermill": {
          "duration": 21.345083,
          "end_time": "2024-10-20T13:38:07.200954",
          "exception": false,
          "start_time": "2024-10-20T13:37:45.855871",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJHphHAGF6k"
      },
      "source": [
        "Z biblioteki `transformers` importowane są dwa kluczowe komponenty: `AutoProcessor` i `AutoModelForVision2Seq`.\n",
        "- Procesor automatycznie przygotowuje dane wejściowe (w tym przypadku obrazy) do formatu wymaganego przez model.\n",
        "- Model `Vision2Seq` to architektura, która przekształca obrazy (Vision) w sekwencje tekstu (2Seq) - innymi słowy, generuje tekstowe opisy obrazów.\n",
        "\n",
        "Funkcja `load_image` z `transformers.image_utils` zapewnia wygodny sposób wczytywania obrazów w formacie odpowiednim dla modeli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-20T13:38:07.208443Z",
          "iopub.status.busy": "2024-10-20T13:38:07.207573Z",
          "iopub.status.idle": "2024-10-20T13:38:07.212108Z",
          "shell.execute_reply": "2024-10-20T13:38:07.211223Z"
        },
        "id": "c83353d5",
        "papermill": {
          "duration": 0.010147,
          "end_time": "2024-10-20T13:38:07.214020",
          "exception": false,
          "start_time": "2024-10-20T13:38:07.203873",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    device = 'cuda'\n",
        "    model = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
        "    dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-10-20T13:38:07.249323Z",
          "iopub.status.busy": "2024-10-20T13:38:07.248594Z",
          "iopub.status.idle": "2024-10-20T13:44:02.024385Z",
          "shell.execute_reply": "2024-10-20T13:44:02.023574Z"
        },
        "id": "f1f6b994",
        "papermill": {
          "duration": 354.781701,
          "end_time": "2024-10-20T13:44:02.026820",
          "exception": false,
          "start_time": "2024-10-20T13:38:07.245119",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(CFG.model)\n",
        "model = AutoModelForVision2Seq.from_pretrained(CFG.model,\n",
        "                                               torch_dtype = CFG.dtype,\n",
        "                                                _attn_implementation=\"eager\" ).to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VL4V2b7GF6k"
      },
      "source": [
        "Te dwie linie kodu przygotowują najważniejsze komponenty systemu - procesor danych i model neuronowy.\n",
        "\n",
        "Pierwsza linia `processor = AutoProcessor.from_pretrained(CFG.model)` ładuje procesor, który jest odpowiedzialny za przygotowanie danych wejściowych. Procesor ten został wcześniej wytrenowany razem z modelem SmolVLM-Instruct i wie dokładnie, jak przekształcić surowe obrazy na format, który model potrafi zrozumieć. Obejmuje to szereg operacji, takich jak zmiana rozmiaru obrazu, normalizacja wartości pikseli czy dodanie specjalnych znaczników.\n",
        "\n",
        "Druga linia jest bardziej złożona i ładuje sam model neuronowy z dodatkowymi parametrami:\n",
        "- `AutoModelForVision2Seq.from_pretrained(CFG.model)` pobiera architekturę i wagi modelu z Hugging Face Hub\n",
        "- `torch_dtype = CFG.dtype` ustawia format liczb na bfloat16, co pozwala zaoszczędzić pamięć GPU bez znaczącej straty jakości\n",
        "- `_attn_implementation=\"eager\"` określa sposób implementacji mechanizmu uwagi (attention). Tryb \"eager\" oznacza standardową implementację bez optymalizacji, która jest bardziej przewidywalna i łatwiejsza w debugowaniu\n",
        "- `.to('cuda')` przenosi model na GPU, co jest kluczowe dla wydajności"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBWKYSy3HMBr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-20T13:44:02.044282Z",
          "iopub.status.busy": "2024-10-20T13:44:02.043937Z",
          "iopub.status.idle": "2024-10-20T13:44:04.663157Z",
          "shell.execute_reply": "2024-10-20T13:44:04.662076Z"
        },
        "id": "03052534",
        "papermill": {
          "duration": 2.630282,
          "end_time": "2024-10-20T13:44:04.665229",
          "exception": false,
          "start_time": "2024-10-20T13:44:02.034947",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "image_path1 = \"/content/img1.png\"\n",
        "\n",
        "image1 = load_image(image_path1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHGfyGhbPcqN"
      },
      "outputs": [],
      "source": [
        "# Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Describe the image in detail\"}\n",
        "        ]\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8ec1d2b",
        "papermill": {
          "duration": 0.007865,
          "end_time": "2024-10-20T13:44:04.681418",
          "exception": false,
          "start_time": "2024-10-20T13:44:04.673553",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
        "inputs = inputs.to(CFG.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeOqfzzqGF6l"
      },
      "source": [
        "W pierwszym kroku używamy `processor.apply_chat_template(messages, add_generation_prompt=True)`, który przekształca naszą strukturę wiadomości na format tekstowy zrozumiały dla modelu. Parametr `add_generation_prompt=True` dodaje specjalny znacznik, który sygnalizuje modelowi, że powinien zacząć generować odpowiedź. Jest to podobne do tego, jak w rozmowie możemy użyć pauzy lub tonu głosu, aby zasygnalizować rozmówcy, że oczekujemy jego odpowiedzi.\n",
        "\n",
        "Drugi krok to wywołanie procesora z argumentami `text=prompt, images=[image1], return_tensors=\"pt\"`. Procesor wykonuje tutaj kilka ważnych operacji. Tekst jest tokenizowany, czyli dzielony na mniejsze jednostki, które model potrafi przetwarzać. Obraz jest przekształcany do odpowiedniego formatu - jest skalowany do właściwego rozmiaru, normalizowany i przekształcany na tensor PyTorch (na co wskazuje parametr `return_tensors=\"pt\"`). Jest to jak przygotowanie składników przed gotowaniem - wszystko musi być we właściwej formie i proporcjach.\n",
        "\n",
        "Ostatni krok `inputs.to(CFG.device)` przenosi przygotowane dane na GPU. Jest to konieczne, ponieważ model znajduje się na GPU (pamiętamy o wcześniejszym `model.to('cuda')`), a dane wejściowe muszą być na tym samym urządzeniu co model. Możemy to porównać do przeniesienia składników na płytę kuchenną, gdzie będzie odbywało się gotowanie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRIoNZCHQpAr",
        "outputId": "2b6413d4-ce37-4fc3-9339-bf8b0394a8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User:<image>Describe the image in detail\n",
            "Assistant: The image depicts an office space with a large window looking out onto a marina and a cityscape. The window is framed by a brick wall on the left and a striped vertical blind on the right. The window is divided into three sections, with the middle section being the largest. The view outside the window shows a marina with several boats docked, and a cityscape in the distance. The cityscape includes a mix of modern and older buildings, with some trees visible in the background. The sky is overcast, with a mix of grey and light blue hues.\n",
            "\n",
            "To the right of the window, there is a large desk with a white top. On the desk, there is a computer monitor, a keyboard, a mouse, a lamp, a small table, and a few other items. The monitor is black and has a large screen. The keyboard is black and has a standard layout. The mouse is black and has a USB port. The lamp is white and has a large bulb. The small table is white and has a few items on it, including a book and a pen.\n",
            "\n",
            "To the left of the window, there is a chair. The chair is grey and has a modern design, with a curved back and a comfortable seat. The chair is on wheels and can be moved around easily.\n",
            "\n",
            "The floor of the office is grey and concrete, and the walls are white. The ceiling is white and has a modern design. There is a light fixture on the ceiling, which is white and has a large bulb.\n",
            "\n",
            "The image is well-lit, and the colors are muted. The overall atmosphere of the office is modern and professional.\n"
          ]
        }
      ],
      "source": [
        "# Generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(generated_texts[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4463935,
          "sourceId": 7665965,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 490.947482,
      "end_time": "2024-10-20T13:44:08.139865",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-20T13:35:57.192383",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
