{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwyBOOPeLh37"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "k5yT3UQvLVHb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install -qU openai\n",
        "# from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zzKr_FxPLVHl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X_pu8CG8N7OC"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    base_url = \"http://localhost:11434/v1\"\n",
        "    api_key = \"ollama\"  # required, but unused\n",
        "    model = \"gpt-4o-mini\"\n",
        "    num_tokens = 2000\n",
        "    temperature = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybmtPls6OSGi"
      },
      "source": [
        "Ta część kodu definiuje klasę konfiguracyjną `CFG`, która służy do przechowywania ustawień modelu językowego. Jest to przykład zastosowania wzorca programistycznego znanego jako klasa konfiguracyjna - centralne miejsce do zarządzania parametrami aplikacji.\n",
        "\n",
        "W tym konkretnym przypadku klasa zawiera tylko jedną zmienną klasową `model`, której przypisano wartość \"gpt-4o-mini\". Zmienna ta określa, który model OpenAI będzie używany do przetwarzania zapytań. Jest to szczególnie istotne, ponieważ różne modele OpenAI mają różne możliwości, koszty i limity.\n",
        "\n",
        "Używanie klasy konfiguracyjnej zamiast zwykłej zmiennej ma kilka zalet. Po pierwsze, wszystkie ustawienia są zgrupowane w jednym miejscu, co ułatwia ich zarządzanie i modyfikację. Po drugie, taka struktura pozwala na łatwe dodawanie kolejnych parametrów w przyszłości bez konieczności modyfikacji pozostałych części kodu. Po trzecie, klasa może być importowana i używana w różnych miejscach projektu, co zapewnia spójność konfiguracji w całej aplikacji.\n",
        "\n",
        "Jest to podobne do tablicy rozdzielczej w samochodzie - wszystkie ważne wskaźniki i ustawienia są zgrupowane w jednym miejscu, dzięki czemu kierowca ma do nich łatwy dostęp i może szybko sprawdzić lub zmienić potrzebne parametry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dRWiRWkLLVHn"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def assert_env_key(key: str | None, name: str) -> None:\n",
        "    if not key:\n",
        "        raise ValueError(f\"{name} is not set, please update .env file\")\n",
        "    \n",
        "OPENAI_API_KEY_NAME = \"OPENAI_API_KEY\"\n",
        "OPENAI_API_KEY = os.getenv(OPENAI_API_KEY_NAME)\n",
        "# OPENAI_API_KEY = userdata.get(OPENAI_API_KEY_NAME) # Google colab env retrieval option\n",
        "assert_env_key(OPENAI_API_KEY, OPENAI_API_KEY_NAME)\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "# client = OpenAI(api_key = userdata.get('openaivision'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWglV5eNNBWK"
      },
      "source": [
        "# Eksperymant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tworzenie nowego asystenta w OpenAI API\n",
        "\n",
        "Ten kod tworzy asystenta w OpenAI API z określonymi ustawieniami:  \n",
        "\n",
        "---\n",
        "\n",
        "##### 🔹 **Metoda tworzenia:**\n",
        "- **`client.beta.assistants.create()`** – Używana do utworzenia asystenta przez API w wersji beta.  \n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 **Parametry:**\n",
        "- **`name = \"helper2\"`** – Nazwa asystenta to \"helper2\". Ułatwia identyfikację.  \n",
        "\n",
        "- **`instructions`** – Wytyczne określające rolę i zachowanie asystenta jako **\"Business AI Guide\"**:  \n",
        "  - Upraszcza techniczne informacje na język biznesowy  \n",
        "  - Unika żargonu technicznego  \n",
        "  - Zachowuje formalny ton  \n",
        "  - Jest informacyjny i cierpliwy  \n",
        "  - Unika szczegółów technicznych, chyba że wymagane  \n",
        "  - Nie angażuje się w dyskusje polityczne  \n",
        "  - Nie spekuluje  \n",
        "  - **Odpowiada \"I do not know\"**, gdy nie jest pewny odpowiedzi  \n",
        "  - Prosi o wyjaśnienie **tylko raz na zapytanie**  \n",
        "\n",
        "- **`model = CFG.model`** – Określa model asystenta na podstawie konfiguracji w klasie `CFG`.  \n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 **Kontynuacja linii:**\n",
        "- **`\\`** na końcu linii pozwala na kontynuację długiego tekstu w kolejnych liniach, poprawiając czytelność kodu.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ASBxYV09LVHo"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name = \"helper2\",\n",
        "    instructions = \"Your role as 'Business AI Guide' is to assist users with questions \\\n",
        "                    and explanations about Artificial Intelligence (AI) and \\\n",
        "                    Machine Learning (ML), tailored for business professionals. \\\n",
        "                    You should simplify complex technical information into clear,\\\n",
        "                    business-friendly language, avoiding technical jargon. \\\n",
        "                    Maintain a formal tone in your communications, being \\\n",
        "                    informative and patient, ensuring clarity for users\\\n",
        "                    without a technical background. Avoid detailed \\\n",
        "                    technical explanations unless specifically requested. \\\n",
        "                    Refrain from discussing politics or current affairs, \\\n",
        "                    and avoid speculating. If uncertain about an answer,\\\n",
        "                    respond with 'I do not know.' Limit yourself to asking \\\n",
        "                    for clarification only once per query; if the query \\\n",
        "                    remains unclear after that, provide the best answer \\\n",
        "                    you can, filling in any missing details as needed.\",\n",
        "     model = CFG.model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tworzenie nowego wątku konwersacji w OpenAI API\n",
        "\n",
        "Ten kod tworzy nowy wątek konwersacji i wyświetla jego szczegóły:  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Tworzenie wątku:**\n",
        "- **`thread = client.beta.threads.create()`** – Tworzy nowy wątek konwersacji.  \n",
        "  - **Wątek** działa jak kontener dla wymiany wiadomości między użytkownikiem a asystentem.  \n",
        "  - Zapewnia **ciągłość i kontekst rozmowy**.  \n",
        "  - Po utworzeniu wątku API generuje unikalny identyfikator oraz inne metadane.  \n",
        "  - To jak zakładanie nowego zeszytu – każda wiadomość będzie przypisana do tego samego wątku, co pozwala zachować **spójność konwersacji**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Wyświetlanie szczegółów:**\n",
        "- **`print(thread)`** – Wyświetla szczegóły nowo utworzonego wątku:  \n",
        "  - Unikalny **identyfikator wątku**  \n",
        "  - **Czas utworzenia**  \n",
        "  - Inne **metadane** związane z wątkiem  \n",
        "  - Pomaga to w **debugowaniu** oraz weryfikacji poprawności utworzenia wątku.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Dlaczego to ważne?**\n",
        "- Mechanizm wątków umożliwia prowadzenie **wielu równoległych konwersacji**, każdej z **własnym kontekstem i historią**.  \n",
        "- To jak prowadzenie kilku rozmów w osobnych pokojach – każda ma **swój przebieg i kontekst**, nie mieszając się z innymi.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hNgJ-T3LVHp",
        "outputId": "979ee890-b099-4ea0-e802-f3f26fb1df1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thread(id='thread_u1zjmqIbjGfkVRDB1FY0MfYL', created_at=1739784403, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
          ]
        }
      ],
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWHGwAj8LVHq"
      },
      "source": [
        "## Tworzenie wiadomości w wątku konwersacji w OpenAI API\n",
        "\n",
        "Te linie kodu tworzą nową wiadomość w utworzonym wcześniej wątku konwersacji, wykorzystując API OpenAI:  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Metoda dodawania wiadomości:**\n",
        "- **`client.beta.threads.messages.create()`** – Metoda, która dodaje wiadomość do istniejącego wątku.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Parametry:**\n",
        "- **`thread_id = thread.id`** – Określa, do którego wątku ma zostać dodana wiadomość. Używamy identyfikatora wątku z poprzedniego kroku. To jak wskazanie zeszytu, do którego chcemy dodać notatkę.  \n",
        "\n",
        "- **`role = \"user\"`** – Określa, że wiadomość pochodzi od użytkownika. W OpenAI rozróżnianie ról ma kluczowe znaczenie, ponieważ wpływa na sposób interpretacji wiadomości przez asystenta. Rola działa jak etykieta, która mówi, kto wysłał wiadomość.  \n",
        "\n",
        "- **`content = \"What is generative AI?\"`** – Treść wiadomości. W tym przypadku pytanie o sztuczną inteligencję generatywną, na które asystent udzieli odpowiedzi.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Jak to działa?**\n",
        "System wiadomości działa jak zapisywanie w zeszycie – każda wiadomość to nowy wpis z informacjami o **roli** nadawcy i **treści**.  \n",
        "Wszystkie wiadomości są połączone poprzez identyfikator wątku, co pozwala asystentowi utrzymać **ciągłość rozmowy i kontekst**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ch44Zlt9LVHq"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role = \"user\",\n",
        "    content = \"What is generative AI?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inicjowanie wykonania wątku konwersacji w OpenAI API\n",
        "\n",
        "Ten fragment kodu inicjuje wykonanie (run) wątku konwersacji, łącząc go z wcześniej utworzonym asystentem. To moment, w którym system zaczyna przetwarzać wiadomość i generować odpowiedź.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Metoda inicjowania wątku:**\n",
        "- **`client.beta.threads.runs.create()`** – Inicjuje wykonanie wątku konwersacji i łączy go z asystentem.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Parametry:**\n",
        "- **`thread_id = thread.id`** – Określa, który wątek konwersacji ma zostać \"uruchomiony\". To jak przekazanie zeszytu z pytaniem do nauczyciela – musimy wskazać, którą rozmowę asystent ma przeanalizować i na co odpowiedzieć.  \n",
        "\n",
        "- **`assistant_id = assistant.id`** – Określa, który asystent ma zająć się tym wątkiem. Łączymy wątek z asystentem, który posiada odpowiednie instrukcje i specjalizację (np. asystent biznesowy).  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Jak to działa?**\n",
        "- **System OpenAI** analizuje historię wątku, bierze pod uwagę instrukcje asystenta i rozpoczyna proces generowania odpowiedzi.  \n",
        "- To jak moment, gdy nauczyciel czyta pytanie ucznia i zastanawia się nad odpowiedzią, uwzględniając wcześniejszą dyskusję i swoje wytyczne.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Ważna uwaga:**\n",
        "- Utworzenie \"run\" nie oznacza natychmiastowej odpowiedzi – to **inicjowanie procesu** jej tworzenia.  \n",
        "- To jak przekazanie pytania nauczycielowi – odpowiedź nadejdzie, ale potrzebny jest czas na jej przygotowanie.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8Huk8JwcLVHr"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id = thread.id,\n",
        "    assistant_id= assistant.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pętla oczekiwania na odpowiedź od asystenta w OpenAI API\n",
        "\n",
        "Ta pętla **`while True`** implementuje mechanizm oczekiwania i pobierania odpowiedzi od asystenta. Oto jak działa krok po kroku:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Sprawdzanie statusu wątku:**\n",
        "- **`client.beta.threads.runs.retrieve()`** – Sprawdza status wykonania wątku, używając identyfikatorów wątku (`thread_id`) i wykonania (`run_id`).  \n",
        "  - To jak zaglądanie nauczycielowi przez ramię, aby sprawdzić, czy odpowiedź jest gotowa.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Przerwa między sprawdzeniami:**\n",
        "- **`time.sleep(10)`** – Wprowadza 10-sekundową przerwę między kolejnymi zapytaniami do API.  \n",
        "  - Ważne, aby nie przeciążać API zbyt częstymi zapytaniami i dać systemowi czas na przetworzenie odpowiedzi.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Sprawdzanie zakończenia procesu:**\n",
        "- **`if run_status.status == 'completed'`** – Sprawdza, czy asystent zakończył już pracę nad odpowiedzią.  \n",
        "  - **`messages = client.beta.threads.messages.list(thread_id=thread.id)`** – Pobiera historię wiadomości z wątku, w tym odpowiedź asystenta.  \n",
        "  - **`break`** – Kończy pętlę, gdy odpowiedź jest gotowa.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Dodatkowe opóźnienie:**\n",
        "- **`else` z `time.sleep(2)`** – Jeśli odpowiedź nie jest jeszcze gotowa, dodaje 2-sekundową przerwę.  \n",
        "  - Pomaga to w zarządzaniu zasobami i ogranicza częstotliwość zapytań.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Jak to działa?**\n",
        "Cały ten mechanizm przypomina czekanie na odpowiedź od eksperta: regularnie sprawdzamy, czy skończył, ale robimy to w rozsądnych odstępach czasu, nie zaglądając co sekundę. Gdy odpowiedź jest gotowa, możemy ją odczytać i przetwarzać.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Przydatność:**\n",
        "- Tego typu mechanizm jest szczególnie użyteczny w **systemach asynchronicznych**, gdzie czas generowania odpowiedzi może być zmienny, zależny od złożoności pytania i obciążenia systemu.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "BDKppaXRLVHs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error details (if available): You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "No active exception to reraise",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m run_status\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError details (if available): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_status\u001b[38;5;241m.\u001b[39mlast_error\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
        "    if run_status.status == 'completed':\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "        break\n",
        "    elif run_status.status == 'failed':\n",
        "        print(f\"Error details (if available): {run_status.last_error.message}\")\n",
        "        raise\n",
        "    else:\n",
        "        time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7N8hID0LVHt",
        "outputId": "e1c2d96e-e9e9-4f2b-e3c3-08dd7adb01c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user:What is generative AI?\n"
          ]
        }
      ],
      "source": [
        "for message in reversed(messages.data):\n",
        "    print(message.role + \":\" + message.content[0].text.value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5601552,
          "sourceId": 9258072,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
